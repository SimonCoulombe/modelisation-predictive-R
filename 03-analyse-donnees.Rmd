```{r, setup, echo=FALSE}
library(sf)
library(lubridate)
library(data.table)
library(ggplot2)
library(rgdal)

source("src/collecte/load-historical-data.R")
source("src/collecte/load-merging-data.R")
source("src/collecte/merge-data.R")

historical_data <- load_historical_data("data/")
merging_data <- load_merging_data("data/")
data <- merge_data(historical_data, merging_data$data_stations, merging_data$points_stations)

data_bixi <- fread("data/data_bixi.csv")
data_stations <- fread("data/data_stations.csv")
data_quartiers <- readOGR("data/LIMADMIN.shp")

set.seed(20190513L)
```

# Exploration de données {#exploration}

Une étape souvent sous-estimée par les scientifiques de données amateurs est l'exploration initiale des données disponibles. Bien qu'il soit tout à fait possible de créer un modèle et de compléter le cycle complet de modélisation en négligeant cette étape, la garantie de qualité des résultats en serait alors fortement compromise.

L'expression populaire *Garbage-in, garbage-out* est généralement interprétée comme quoi un modèle prédicitif, même s'il utilise un algorithme à la fine pointe de la technologie, produira des mauvais résultats s'il est entraîné sur des données de mauvaise qualité. L'exploration initiale des données est la phase dans laquelle on évalue cette qualité. Il est beaucoup moins dommageable de reconnaître rapidement qu'on ne dispose pas des données nécessaires pour construire un modèle que de mettre un mauvais modèle en production.

Outre valider la qualité de nos données, l'analyse préliminaire permet d'acquérir des connaissances indispensables pour la suite du processus de modélisation. En effet, tel qu'on le verra dans le prochain chapitre, un élément clé pour obtenir un bon modèle prédictif est la création de nouvelles variables explicatives. Comme celle-ci seront basées sur des transformations des variables du jeu de données brut, il est impératif de bien en connaître les moindres détails.

Le but de ce chapitre est donc simple, on veut remplir les deux objectifs expliqués ci-haut :

1. Valider la quatlié des données brutes
1. Suggérer des transformations de variables potentielles

Le chapitre sera découpé en quatre parties, chacune d'entre elle étant un stade différent du processus d'analyse de données. Dans la première partie, on ira de l'analyse la plus simple en explorant des observations individuelles. Ensuite, on poursuivra l'analyse en agrégeant ces observations individuelles selon certains axes. L'avant-dernière étape consistera à définir et évaluer différentes métriques basées sur nos données. Enfin, pour terminer le chapitre, on créera des représentations graphiques de nos données.

Prendre note que le but de ce livre n'est pas de présenter une panoplie de solutions pour manipuler des données en R, mais bien d'avoir une vue d'ensemble sur le processus complet de modélisation. Pour des préférences personnelles, la minuplation des données sera effectuée en utilisant une combinaison des packages de base et du package `data.table` dans ce chapitre. Le lecteur est invité à consulter la documentation de base des packages pour mieux comprendre leur utilisation. Une autre solution populaire est l'utilisation de `dplyr` qui fait partie de la collection de packages du `tidyverse`.

## Observations individuelles

Pour commencer l'analyse des données, rien de trop compliqué. On se familiarise avec nos différents jeux de données en affichant les premières entrées qu'ils contiennent. Tel que vu dans le chapitre précédent, le jeu de données qu'on utilise pour le processus de modélisation contient deux tables, une contenant les données de trajet BIXI (`data_bixi`) et un contenant l'information géographique des différents stations (`data_stations`).

On commence par afficher les premières observations de `data_bixi` :

```{r, explore_head_bixi}
head(data_bixi)
```

Les variables présentes dans le jeu de données sont :

- `start_date` : jour et heure du départ;
- `start_station_code` : code de la station de départ;
- `end_date` : jour et heure de l'arrivée;
- `end_station_code` : dode de la station d'arrivée;
- `duration_sec` : durée du trajet (en secondes);
- `is_member` : indicateur de membre.

On affiche ensuite les premières observations de `data_stations` :

```{r, explore_head_stations}
head(data_stations)
```

Les variables présentes dans le jeu de données sont :

- `code` : code de la station;
- `name` : nom de la station;
- `latitude` : latitude de la station
- `longitude`: longitude de la station

Comme on peut facilement l'observer avec les premières observations de chaque jeu de données, les observations sont rarement ordonnées dans un ordre aléatoire. Quelqu'un de naïf pourrait légitimement croire, à première vue, que la majorité des trajets sont faits dans l'arrondissement de La Salle, les nuits de printemps par des membres réguliers. Bien que cela paraisse un peu ridicule dans ce contexte étant donné notre intuition face aux données, il faut garder en tête que le processus de modélisation doit être robuste peu importe le contexte dans lequel on l'utilise. Il n'est pas rare qu'un scientifique de données soit appelé à créer un modèle prédictif dans un contexte avec lequel il n'est pas familier.

La morale du dernier paragraphe est qu'il est donc préférable de regarder un échantillon aléatoire des données au lieu de regarder les premières observations pour éviter de se créer un biais mental non intentionnel. On répète l'exercice précédent avec un échantillon aléatoire.

```{r, explore_random}
data_bixi[sample(.N, 5L)]
data_stations[sample(.N, 5L)]
```

Comme on pouvait s'y en attendre, nos hypothèses précédentes, un peu loufoques avouons-le, sont majoritairement invalidées. Notre jeu de données semble maintenant contenir des trajets d'un peu partout sur l'île de Montréal effectués à tout moment au cours de la saison estivale 2017. Notre échantillon ne nous permet cependant pas d'infirmer notre hypothèse que tous les trajets sont effectués par des membres réguliers du programme BIXI. La prochaine section, le coeur du chapitre, nous permettra d'approfondir encore plus nos connaissances sur notre jeu de données.

## Observations agrégées

Dans la précédente section, on s'est familiarisé avec le format de nos données en regardant un échantillon aléatoire de nos observations. Bien que ce soit le point de départ, on est loin d'avoir suffisament apprivoisé notre jeu de données pour passer à la prochaine phase du cycle, le prétraitement des données.

La question qui reste en suspens pour le moment est de savoir si notre jeu de données ne contient que des observations des membres ou s'il contient aussi les observations des non-membres. Bien qu'on puisse augmenter le nombre d'observations aléatoires jusqu'à ce qu'on regarde tout le jeu de données, on conviendra que ce n'est pas la solution idéale pour connaître toutes les valeurs que peuvent prendre chacune des variables.

La première étape pour agréger des données est de se familiariser avec les distributions marginales de chacun des champs. On doit évidemment traîter différemment les variables continues des variables discrètes. Il y a différentes manières de procéder, on propose une solution parmi tant d'autres dans le livre.

```{r, spatial_exposure, warning=FALSE}
library(leaflet)
library(magrittr)

# Sommaire par station
data_stations_rides <- data[, .(
  code = start_station_code,
  quartier = start_quartier,
  nb = .N,
  mean = mean(duration_sec)
), .(start_station_code, start_quartier)][, `:=`(
  start_station_code = NULL,
  start_quartier = NULL
)]

# Agréger sommaire par station
setkey(data_stations_rides, code)
setkey(data_stations, code)
data_stations[data_stations_rides, `:=`(
  quartier = quartier,
  rides_nb = nb,
  rides_mean = mean
)]

# Sommaire par quartier
data_quartiers_rides <- data_stations[!is.na(quartier), .(
  nom = quartier,
  stations_nb = .N,
  rides_nb = sum(rides_nb),
  rides_mean = weighted.mean(rides_mean, rides_nb)
), quartier][, quartier:=NULL]

# Limiter moyenne des temps de trajet
limits <- c(700, 1000)
data_quartiers_rides[, rides_capped_mean:=rides_mean]
data_quartiers_rides[rides_mean<limits[1], rides_capped_mean:=limits[1]]
data_quartiers_rides[rides_mean>limits[2], rides_capped_mean:=limits[2]]

# Agréger sommaire par quartier
setkey(data_quartiers_rides, nom)
#data_quartiers <- readOGR("data/LIMADMIN.shp")
data_quartiers@data <- as.data.table(data_quartiers@data, keep.rownames="ID")
data_quartiers@data[, ID:=as.integer(ID)]
setkey(data_quartiers@data, NOM)
data_quartiers@data[data_quartiers_rides, `:=`(
  stations_nb = stations_nb,
  rides_nb = rides_nb,
  rides_mean = rides_mean
)]
setorder(data_quartiers@data, ID)[, ID:=NULL]

# Création de la palette de couleur des quartiers
categorical_pal <- colorFactor(
  palette = rainbow(data_quartiers@data[!is.na(stations_nb), .N]),
  domain = data_quartiers@data[!is.na(stations_nb), as.character(NOM)]
)

# Création de la carte
exposure_map <- leaflet() %>%
  addTiles() %>%
  addMarkers(
    data = data_stations,
    group = "Stations",
    lng = ~longitude,
    lat = ~latitude,
    icon = makeIcon(
      iconUrl = "static-files/bixi-logo.png",
      iconWidth = ~8+rides_nb/1500,
      iconHeight = ~8+rides_nb/1500
    ),
    popup = ~paste(
      paste0("<b>", name, "</b>"),
      paste0("Nombre de trajets : ", format(rides_nb, big.mark=" ")),
      #paste0("Durée moyenne : ", format(rides_mean, digits=0, big.mark=" "), " sec"),
      sep = "<br/>"
    ),
    label = ~name
  ) %>%
  addPolygons(
    data = data_quartiers,
    group = "Quartiers",
    color = "black",
    weight = 2,
    fillColor = ~categorical_pal(NOM),
    #fillOpacity = 0.5,
    dashArray = "2 4",
    popup = ~paste(
      paste0("<b>", NOM, "</b>"),
      paste0("Nombre de stations : ", format(stations_nb, big.mark=" ")),
      paste0("Nombre de trajets : ", format(rides_nb, big.mark=" ")),
      #paste0("Durée moyenne : ", format(rides_mean, digits=0, big.mark=" "), " sec"),
      sep = "<br/>"
    ),
    highlightOptions = highlightOptions(
      weight = 3,
      opacity = 1,
      dashArray = FALSE,
      bringToFront = TRUE
    )
  ) %>%
  # addLegend(
  #   data = data_quartiers@data[!is.na(stations_nb)],
  #   position = "bottomright",
  #   pal = categorical_pal,
  #   values = ~NOM,
  #   opacity = 1,
  #   title = "Quartiers",
  #   group = "Quartiers"
  # ) %>%
  addLayersControl(
    overlayGroups = c("Stations", "Quartiers"),
    options = layersControlOptions(collapsed = FALSE)
  )

# Affichage de la carte
exposure_map
```



```{r, spatial_one_way}





```


## Métriques des observations

## Visualisation graphique

