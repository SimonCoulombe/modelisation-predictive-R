
# Collecte de données

``` {r, echo = FALSE}
library(magrittr)
```

L'étape initiale de tout projet de modélisation prédictive est la collecte des données. Sans données, le modèle apprendra... rien!

Cette section sera découpée comme suit :

1. Description de l'approche
2. Liste potentielle de sources de données pour la tâche à accomplir
3. Considérations

## Description de l'approche

Les étapes sont : obtenir les données étiquettées, obtenir les données de [*jointure*](https://fr.wikipedia.org/wiki/Jointure_(informatique)) (traduction libre) et effectuer la jointure. Nous verrons également que la collecte se produit souvent différemment lors de la modélisation et lors de la prédiction en production.

### Obtenir les données étiquettées

Pour chaque observation, les données étiquettées contiennent au minimum l'étiquette, aussi appelée variable réponse. Autrement dit, c'est la valeur que nous voulons prédire. Dans notre cas, une observation sera un trajet. Pour chaque observation, nous devrons avoir 2 étiquettes : la durée du trajet et le statut de membre du client. Sans information supplémentaire pour chacune des observations, nous serions en présence d'un simple échantillon aléatoire grâce auquel nous pourrions simplement prédire un estimateur comme la moyenne ou le mode pour tous les trajets. C'est pourquoi nous nécessiterons également au moins une information supplémentaire nous permettant de segmenter nos prédictions d'une observation à l'autre. Nous appellerons ces informations supplémentaires les prédicteurs. Par exemple, le jour et l'heure de départ pourraient nous donner de l'information prédictive sur la durée du trajet et sur le statut de membre du client. Voici de quoi pourrait avoir l'air notre jeu de données étiquettées :

```{r, echo = FALSE}
etiquettees <- tibble::data_frame(temps_depart = c("2017-05-07 17:00:02", "2017-05-29 18:50:11", "2017-10-12 22:56:46"),
                                  `duree_trajet (sec)` = c(25*60 + 5, 2*60, 11 * 60 + 47))

etiquettees %>% 
  knitr::kable()
```


### Obtenir les données de *jointure*.

Après avoir trouvé nos données étiquettées contenant les étiquettes et les prédicteurs, nous sommes rarement satisfaits de la quantité de prédicteurs que nous pouvons y trouver. En effet, il arrive souvent dans les premiers instants d'un projet de prononcer une phrase du genre : "Imaginez si nous avions accès au prédicteur XYZ, qui malheureusement est absent des données étiquettées". Nous pouvons alors augmenter le jeu de données à l'aide de ce que nous appellerons les données de *jointure*. Nous les appellerons ainsi car nous devrons effectuer une opération de jointure entre notre jeu de données étiquettées et notre source de données additionnelle. Par exemple, si nous reprenons les données étiquettées précédentes, une source de données de jointure pourrait par exemple nous renseigner sur les heures de lever et de coucher du soleil pour chaque journée. En supposant que la noirceur décourage les cyclistes, cela pourrait aider à prédire la durée du trajet :

```{r, echo = FALSE}
jointure <- tibble::data_frame(date = c("2017-05-07", "2017-05-29", "2017-10-12"),
                               `heure lever` = c("06:35:00", "06:00:00", "07:30:00"),
                               `heure coucher` = c("20:10:00", "20:45:00", "17:55:00"))
jointure %>% 
  knitr::kable()
```

### Effectuer la *jointure*

Nous devrons ensuite effectuer une jointure à partir des informations présentes dans le(s) jeu(x) de données de *jointure* et dans le jeu de données étiquettées. Dans notre cas, la vie est belle : on joindrait les variables lever et coucher à l'aide de la date présente dans nos 2 jeux de données :

```{r}

etiquettees %>% 
    dplyr::mutate(date = stringr::str_extract(temps_depart, "\\d{4}-\\d{2}-\\d{2}")) %>% 
    dplyr::left_join(jointure, by = "date")

```

Par contre, dans la *vraie* vie, les opérations de jointure peuvent parfois s'avérer périlleuses. En effet, les informations de part et d'autres peuvent être manquantes ou insuffisantes, de sorte que la jointure sera effectuée avec un faible niveau de confiance dans sa qualité. Par exemple, si nos données de *jointure* avaient plutôt eu l'allure suivante, nos nouveaux prédicteurs auraient un pouvoir prédictif plus faible :

```{r}
jointure <- tibble::data_frame(mois = c("2017-05", "2017-10"),
                               `heure lever moyenne` = c("06:17:00", "07:25:00"),
                               `heure coucher moyenne` = c("20:27:00", "17:50:00"))
jointure %>% 
  knitr::kable()
```

```{r}
etiquettees %>% 
    dplyr::mutate(mois = stringr::str_extract(temps_depart, "\\d{4}-\\d{2}")) %>% 
    dplyr::left_join(jointure, by = "mois")
```


### Collecte historique vs. collecte en production

Dans les projets de modélisation prédictive, il existe généralement un fossé entre la modélisation, qui nécessite une collecte de données historiques, et la prédiction en production, qui nécessite une collecte de données en temps réel. Selon la configuration des systèmes en place et la nature de la problématique, le fossé peut être microscopique, et passer inaperçu, ou astronomique, et poser de sérieux ennuis pouvant même compromettre la faisabilité du projet. En effet, les natures différentes de ces 2 contextes peuvent parfois être irréconciliables : l'entraînement de modèles sur des données historiques exige un entreposage adéquat des données, mais les données historiques ont rarement été collectées dans l'optique de servire à entraîner des modèles. Il faudra donc distinguer la collecte des données selon si elle est faite dans l'optique de modélisation ou de prédiction.


## Liste des données potentielles pour notre atelier

Dans le but d'identifier les données étiquettées et de *jointure*, dressons d'abord une liste des données disponibles et potentiellement utiles.

Les plus expérimentés auront déjà en tête la majorité des considérations couvertes plus tard dans ce chapitre au moment de dresser leur liste. Dans notre cas, permettons-nous de lister naïvement un grand nombre de sources. Chaque considération à venir nous permettra ensuite d'invalider certaines sources et de distiller notre sélection.

### Données Étiquettées

Tout d'abord, une simple recherche des mots-clés *données historiques BIXI* dans votre moteur de recherche favori devrait vous mener à la page du site web de BIXI réservée à [l'historique des déplacements](https://bixi.com/fr/donnees-ouvertes).

Un coup d'oeil aux données disponibles permet de constater que, pour l'année 2017, nous avons accès, pour chacun des mois d'ouverture du service (avril à novembre), à des données tabulaires sous cette structure :

``` {r, echo = FALSE}
data_2017_04 <- data.table::fread("data/data_bixi.csv", nrows = 5)

data_2017_04 %>% 
  knitr::kable()
```

>[STOP] data_bixi n'existe pas nécessairement quand on load le projet sans appeler laod_historical_data(). Ajouter un paramètre nrows? Recoder?

Voilà qui devrait nous fournir les éléments les plus précieux : nos 2 étiquettes. En effet, pour chacun des trajets, nous pourrons déterminer la durée avec la colonne *duration_sec* et le statut de membre du client à l'aide de la colonne *is_member*. Ces données renferment aussi d'autres éléments intéressants pouvant servir de prédicteurs. Nous pourrons notamment utiliser le jour de la semaine, l'heure et la station de départ pour tenter de prédire les variables réponses.

Dans ce jeu de données, ```duration_sec``` et ```is_member``` sont les étiquettes. Les prédicteurs sont la station de départ, l'heure de la journée, le jour de la semaine, et le mois, qui peuvent tous fournir davantage de précision à nos prédictions.


### Données de *jointure*

La station de départ est représentée par un code unique représentant une entité unique/récurrente/identifiable (souvent appelé ID). Cela fait non seulement d'elle un bon prédicteur, mais également une bonne clé pour amener des données de *jointures*. Par exemple, dans notre cas, le code de station est une porte d'entrée vers l'information spécifique aux stations ou à leur emplacement géographique. Nous pouvons dès lors utiliser n'importe quelle information reliée à la station ou son emplacement et susceptible de raffiner les prédictions. Par exemple, si nous connaissions l'altitude de chaque station et croyions qu'une station de départ plus élevée produit en général des trajets plus longs, nous pourrions procéder ainsi :

```{r, echo = FALSE}
A <- data_2017_04 %>% 
  dplyr::select(start_station_code, duration_sec)

A %>% 
  knitr::kable() %>% 
  kableExtra::kable_styling(full_width = FALSE)
```

PLUS

```{r, echo = FALSE}
B <- tibble::data_frame(station_ID = c(6104, 6173, 6174, 6203, 7060), elevation = c(10, 20, 15, 5, 100))

B %>% 
  knitr::kable() %>% 
  kableExtra::kable_styling(full_width = FALSE)
```

EQUALS

```{r, echo = FALSE}
dplyr::left_join(A, B, by = c("start_station_code" = "station_ID")) %>% 
  dplyr::select(start_station_code, elevation, duration_sec) %>% 
  knitr::kable() %>% 
  kableExtra::kable_styling(full_width = FALSE, position = "float_right")
```

Le jour de la semaine pourrait aussi servir à faire des jointures, par exemple avec une liste des jours fériés ou avec une table des heures de lever et de coucher du soleil. La liste des jointures possibles ne s'arrête pas là; allons passer en revue toutes les données de jointure potentiellement à notre disposition.

Une recherche plus approfondie des mots-clés précédents devrait vous mener à la page du Portail données ouvertes de la Ville de Montréal réservée elle aussi à [l'historique des déplacements](http://donnees.ville.montreal.qc.ca/dataset/bixi-historique-des-deplacements). A priori, c'est simplement un lien vers les données précédentes. Un clic sur le bouton *Vélo* de la section Mots-clés nous ouvre toutefois une [boîte de Pandore](http://donnees.ville.montreal.qc.ca/dataset?q=velo) : L'[État en temps réel des stations BIXI](http://donnees.ville.montreal.qc.ca/dataset/bixi-etat-des-stations), la [Géolocalisation des arceaux à vélos](http://donnees.ville.montreal.qc.ca/dataset/arceaux-velos), la [cartographie du réseau cyclable](http://donnees.ville.montreal.qc.ca/dataset/pistes-cyclables) et le [nombre de passages quotidiens sur les pistes cyclables](http://donnees.ville.montreal.qc.ca/dataset/velos-comptage) sur le territoire de la Ville de Montréal.

> [STOP] premier exercice soft et pour s'assurer que tout le monde est bien set up : on load les données BIXI!

À cette liste, nous pourrions ajouter une tonne d'autres sources de données permettant de mieux contextualiser les observations et améliorer nos prédictions. Parmi celles-ci, notons les suivantes :

- Données météorologiques
    - [température](http://climate.weather.gc.ca/historical_data/search_historic_data_e.html)
    - précipitations
    - vent
- Données géographiques
    - [découpage des quartiers](http://donnees.ville.montreal.qc.ca/dataset/polygones-arrondissements/resource/bc6e94c7-9393-490d-899f-4296dd1e3dcf)
    - pentes
    - altitude
    - cours d'eau
    - pistes cyclables
- Données démographiques
    - densité
    - distribution du revenu
    - distribution de l'âge
- Données individuelles
    - adresse
    - statut résident/visiteur
    - lieu de travail
- Historique de trajets par individu


Tel que mentionné en introduction, notre ouvrage est linéaire, donc le processus de sélection de données se fera d'un seul trait. Bien entendu, dans un projet réel, il est presque certain que certains aspects nous échappent au début, et que nous devions réajuster le tir dans les phases subséquentes.

> [STOP] demander aux participants leurs idées de dataset intéressant

## Considérations {#considerations}

La collecte de données est le point de départ de la chaîne. Les données étant quelque peu la matière première du produit que nous nous apprêtons à bâtir, plusieurs considérations influenceront leur collecte.

Les premières considérations qui viennent à l'esprit dans le cas des matières premières sont évidemment la quantité, la qualité et le prix. Toutefois, d'autres facteurs influencent le choix de matière première comme la provenance, le processus de récolte ou de transformation, la constance de l'approvisionnement, l'entreposage et la sécurité. Les mêmes considérations peuvent s'appliquer dans notre contexte. Nous devrons donc être vigilents dans la sélection de sources de données. La section suivante aura donc pour objectif d'aborder ces considérations, tout en exposant à celles-ci une liste de données potentielles pour l'atelier.

Comme nous pouvons le constater, la liste de données potentielles est sans limites. Les considérations qui suivent auront pour objectif de nous exposer les limites de certaines sources, pour ensuite identifier celle(s) qui sont le plus adaptées à notre problématique.

### Volume
On parlera de volume plutôt que de quantité. Nous définissons le volume comme l'espace occupé par un objet sur disque ou en mémoire. Le volume d'une source de données sera donc le produit du nombre d'observations et de la richesse de chaque observation. Si notre source de données contient beaucoup d'observations ou des observations riches, nous aurons donc des contraintes de rapidité de calcul, d'espace disque ou de mémoire au moment de l'entraînement, et potentiellement même au moment de la prédiction selon l'algorithme choisi.

#### Nombre d'observations
Si nous avons le *malheur* d'être ensevelis sous un trop grand nombre d'observations, plusieurs options s'offrent à nous, qu'elles soient informatiques ou mathématiques/statistiques.

Du point de vue informatique, R est parfois réputé lent et capricieux en termes de mémoire vive. Or, [il n'en est rien](http://adv-r.had.co.nz/Performance.html). En effet, il existe plusieurs stratégies simples pour optimiser une expression R : [profiler le code](http://adv-r.had.co.nz/Profiling.html), [comprendre la gestion de la mémoire](http://adv-r.had.co.nz/memory.html) ou des interfaces [C++](http://adv-r.had.co.nz/Rcpp.html) et [C](http://adv-r.had.co.nz/C-interface.html).

Du point de vue mathématique, il serait étonnant que la quantité d'observations *étiquettées* soit trop grande. Comme a déjà admis le Scientifique en Chef de Google : «Nous n'avons pas de meilleurs algorithmes que n'importe qui d'autre; nous avons seulement plus de données»[https://www.forbes.com/sites/scottcleland/2011/10/03/googles-infringenovation-secrets]. Pour contourner les problèmes de mémoire ou d'espace disque, il existe des algorithmes qui taitent les données par *batches*, c'est-à-dire par tranches de $n$ observations à la  fois, où $n$ devra être assez petit pour contourner les contraintes d'espace, mais assez grand pour que l'algorithme puisse généraliser son apprentissage. [source algos à batches].

#### Format
La richesse d'une observation est définie par son format.

Nous définissons le format comme la façon dont est représentée une donnée. C'est entre autres ce qui va déterminer la richesse de chaque donnée. On peut alors placer les données sur un continuum de structuré à non-structuré. Certaines définitions considèrent que les données semi-structurées sont une classe à part... nous nous contenterons de dire qu'il existe un continuum de degrés d'organisation des données.

##### Données structurées
organisées en base de données relationnelles constituées de tables disposées en colonnes et rangées. On utilise un modèle de données  
SQL=Structured Query Language  
oracle

##### Données non-structurées
les données non-structurées sont celles qui ne sont pas organisées à l'aide d'un modèle de données. Une base de données non-structurée peut donc contenir à la fois du texte, des images, du son ou une combinaison (ex. vidéo).  
JSON  
XML

#### Aggrégation
À la section précédente, nous avons pris la peine de préciser que ce sont les données étiquettées qui ne peuvent pas être trop nombreuses. Par contre, pour toutes les données de jointure, il est possible que les clés de jointure ne soient pas tout à fait alignées. Par exemple, si le taux d'échantillonnage des données de jointure n'est pas aussi précis que le permet la clé dans les données étiquettées, nous devrons nous contenter d'une valeur aggrégée, par exemple la moyenne sur une plus longue période. Nous préférerons presque toujours une source la plus granulaire possible, avec comme possibilité de faire l'aggrégation nous-même. Nous évitons ainsi la perte potentielle d'information prédictive pour notre modèle

base / dplyr / data.table [autre section?](https://stackoverflow.com/questions/21435339/data-table-vs-dplyr-can-one-do-something-well-the-other-cant-or-does-poorly)

> [STOP] on va faire un merge des données ... Météo?

### Qualité
Plusieurs aspects peuvent influencer notre perception de qualité d'une source de donnée, tels que son organisation, sa documentation, son pouvoir prédictif, sa fréquence, la capacité de faire des jointures et le biais. Encore une fois, notons que les étapes futures peuvent invalider une source de données. Par exemple, le pouvoir prédictif est difficile à évaluer sans un premier modèle naïf au moins.

#### organisation et documentation des données
Ces 3 sources de données contiennent la même information. Avec laquelle préfériez-vous travailler *a priori*?

```{r, echo = FALSE}
maintenant <- lubridate::now()
kable_settings <- function(x) kableExtra::kable_styling(x, bootstrap_options = "striped", full_width = F, position = "left")

knitr::kable(tibble::data_frame(dt = maintenant, temps = 0.5/24)) %>%
  kable_settings
knitr::kable(tibble::data_frame(Date = maintenant, `Durée (minutes)` = 30)) %>%
  kable_settings
knitr::kable(tibble::data_frame(`Date début` = maintenant, `Date fin` = maintenant + 30 * 60)) %>%
  kable_settings
```

Évidemment, nous préférerons, dans l'ordre, 3, 2, 1. Bien que cet exemple soit simpliste et fictif, il illustre tout de même que des éléments tels que le nom, la structure ou la documentation des données influenceront notre capacité à maximiser notre utilisation de ces données.

#### complexité
Rapport coût-bénéfice
Bénéfice : A-t-on déjà des bons proxys?
Coût : complexité additionnelle

#### pouvoir prédictif
Garbage In Garbage Out

#### fréquence
À quelle fréquence la source de données est-elle mise à jour? À quelle fréquence l'information sous-jacente à la source de données change-t-elle? De quelle précision aurons-nous besoin? Est-ce que note prédiction dépend de données instantanées ou intemporelles?

```{r, warning = FALSE, echo = FALSE}
x <- tibble::data_frame(`Mise à jour fréquente` = c("Données météorologiques", "????"),
                        `Mise à jour rare` = c("????", "Données géographiques"))
rownames(x) <- c("Changements fréquents", "Changements rares")

knitr::kable(x) %>%
  kable_settings
```

météo demande d'avoir un live feed, alors que données géographiques peuvent être figées dans le temps

#### Merge
facilité/pertinence de la clé, etc. Exemple : historique des déplacements du client ... pas dispo. Données ouvertes Montréal : ok peut-être...

http://donnees.ville.montreal.qc.ca/dataset/pistes-cyclables
http://donnees.ville.montreal.qc.ca/dataset/geobase


#### Biais
socio-démographique, etc.
dès que des données personnelles sont en jeu
dès que des proxys de données personnelles sont en jeu
aggravé lorsque les prédictions ont un impact réel sur la vie des gens

### Accessibilité
Dans notre parallèle avec la table, le coût était la troisième considération principale. Dans le cas des données, nous allons plutôt parler d'accessibilité, dont le coût est une des composantes.

#### coût (gratuit?)
Si (la)[] (plupart)[] des (tribunes)[] s'entendent pour dire que le nouvel or noir est les données, nous aurons donc à considérer le coût pour accéder aux données. Or, plusieurs (#sources) sont gratuites.

##### Sources gratuites {#sources-gratuites}
- https://www.kaggle.com/datasets
- https://toolbox.google.com/datasetsearch
- https://www.reddit.com/r/datasets
- https://archive.ics.uci.edu/ml/index.php?fbclid=IwAR09F5grBOTCr1SS4v8gONEYeqk0DqqWpPdt1blmYF9ucZkhsQeM5T0E7ew
- http://donnees.ville.montreal.qc.ca/
- stats can
- ville Montréal
- gc.ca


##### Sources payantes {#sources-payantes}
- brokers
- ...

#### Lecture de fichiers
data.table (fread)
readr
readxl
xlsx

#### Connexion à une base de données
sqldf
dbplyr
rio
DBI
odbc
RMySQL, RPostgresSQL, RSQLite
foreign
haven
https://db.rstudio.com/

#### Web Scraping (légalité/api/service/timeouts)

http://www.omdbapi.com
&apikey=9927136f

https://api.citybik.es/v2/
https://api.citybik.es/v2/networks/bixi-montreal

XML
xml2
curl
httr
rvest
downloader
jsonlite
googlesheets

#### Accès aux données historiques et en temps réel
Exemple température

#### Stabilité
Pour que la stabilité des sources de données soit problématique, deux éléments doivent être réunis :
- le format des données change
- notre fréquence d'extraction est trop élevée pour nous donner le temps de réagir

Alors que nous n'avons peu de contrôle sur le premier élément, nous pouvons mitiger le deuxième. Nous pouvons travailler avec une *copie* des données, de sorte que nous ne serons pas affectés par un changement à la source. Évidemment, en contrepartie on sacrifie la réactivité... l'approche à prendre dépendra donc de la valeur d'avoir les données les plus à jour dans le modèle, de notre anticipation de la stabilité ainsi que de l'impact d'une panne du service.

Dans notre cas, les shapefiles...

- Évolution anticipée du format des données (pas de contrôle sur la source de données? → copier les données freezées. Le modèle a encore de la valeur)
- Pourquoi on va utiliser les shapefiles comme exemple de merge (statique, etc.)

#### sécurité et confidentialité
Comme dans tout projet informatique, nous devons nous assurer du niveau de sécurité approprié pour l'usage

## Références
