
# Collecte de données

``` {r, echo = FALSE}
library(sf)
library(magrittr)
```

La collecte de données est le point de départ de la chaîne. Les données étant quelque peu la matière première du produit que nous nous apprêtons à bâtir, plusieurs considérations influenceront leur collecte.

Les premières considérations qui viennent à l'esprit dans le cas des matières premières sont évidemment la quantité, la qualité et le prix. Toutefois, d'autres facteurs influencent le choix de matière première comme la provenance, le processus de récolte ou de transformation, la constance de l'approvisionnement, l'entreposage et la sécurité.

Les mêmes considérations peuvent s'appliquer dans notre contexte. La section suivante aura donc pour objectif d'aborder ces considérations, tout en exposant à celles-ci une liste de données potentielles pour l'atelier.

## Liste des données potentielles pour notre atelier

La première étape d'un déploiement de modèle prédictif est de dresser la liste des sources de données potentielles.

Les plus expérimentés auront déjà en tête la majorité des considérations couvertes dans ce chapitre au moment de dresser leur liste. Dans notre cas, permettons-nous de lister naïvement un grand nombre de sources. Chaque considération à venir nous permettra de distiller notre sélection.

Tout d'abord, une simple recherche des mots-clés *données historiques BIXI* dans votre moteur de recherche favori devrait vous mener à la page du site web de BIXI réservée à [l'historique des déplacements](https://bixi.com/fr/donnees-ouvertes).

Un coup d'oeil aux données disponibles permet de constater que, pour l'année 2017, nous avons accès, pour chacun des mois d'ouverture du service (avril à novembre), à des données tabulaires sous cette structure :

``` {r, echo = FALSE}
data_2017_04 <- data.table::fread("data/data_bixi.csv", nrows = 5)

data_2017_04 %>% 
  knitr::kable()
```

Voilà qui devrait nous fournir les éléments les plus précieux : nos 2 variables réponses, ou *étiquettes*. En effet, pour chacun des trajets, nous pourrons déterminer la durée avec la colonne *duration_sec* et si le cycliste était membre à l'aide de la colonne *is_member*. Ces données renferment aussi d'autres éléments intéressants. Nous pourrons notamment utiliser le jour de la semaine, l'heure et la station de départ pour tenter de prédire les variables réponses.

En général, dans un contexte de modélisation prédictive, nous avons sous la main une source de données *étiquettée* comme la nôtre,

```{r, echo = FALSE}
data_2017_04 %>% 
  dplyr::select(start_station_code, duration_sec) %>% 
  knitr::kable()
```

où ```start_station_code``` est un code unique représentant une entité unique/récurrente/identifiable (souvent appelé ID), et où ```duration_sec``` est l'étiquette, soit la valeur que nous voulons prédire. Souvent, quelques variables accompagnent ces champs et peuvent nous aider à prédire les étiquettes. Dans nos données historiques de BIXI, c'est le cas notamment de l'heure de la journée, du jour de la semaine, et du mois, qui peuvent tous fournir davantage de précision à nos prédictions.

Or, notre travail de collecte ne s'arrête pas là. Il est souvent possible de joindre des données supplémentaires, entre autres via les IDs. Par exemple, dans notre cas, le code de station est une porte d'entrée vers l'information spécifique aux stations ou à leur emplacement géographique. Nous pouvons dès lors utiliser n'importe quelle information reliée à la station ou son emplacement et susceptible de raffiner les prédictions. Par exemple, si nous connaissions l'altitude de chaque station et croyions qu'une station de départ plus élevée produit en général des trajets plus longs, nous pourrions procéder ainsi :

```{r, echo = FALSE}
A <- data_2017_04 %>% 
  dplyr::select(start_station_code, duration_sec)

A %>% 
  knitr::kable() %>% 
  kableExtra::kable_styling(full_width = FALSE)
```

PLUS

```{r, echo = FALSE}
B <- tibble::data_frame(station_ID = c(6104, 6173, 6174, 6203, 7060), elevation = c(10, 20, 15, 5, 100))

B %>% 
  knitr::kable() %>% 
  kableExtra::kable_styling(full_width = FALSE)
```

EQUALS

```{r, echo = FALSE}
dplyr::left_join(A, B, by = c("start_station_code" = "station_ID")) %>% 
  dplyr::select(start_station_code, elevation, duration_sec) %>% 
  knitr::kable() %>% 
  kableExtra::kable_styling(full_width = FALSE, position = "float_right")
```

Nous entreprenons donc une revue des sources de données additionnelles disponibles pour nous aider à prédire les étiquettes. Une recherche plus approfondie des mots-clés précédents devrait vous mener à la page du Portail données ouvertes de la Ville de Montréal réservée elle aussi à [l'historique des déplacements](http://donnees.ville.montreal.qc.ca/dataset/bixi-historique-des-deplacements). A priori, c'est simplement un lien vers les données précédentes. Un clic sur le bouton *Vélo* de la section Mots-clés nous ouvre toutefois une [boîte de Pandore](http://donnees.ville.montreal.qc.ca/dataset?q=velo) : L'[État en temps réel des stations BIXI](http://donnees.ville.montreal.qc.ca/dataset/bixi-etat-des-stations), la [Géolocalisation des arceaux à vélos](http://donnees.ville.montreal.qc.ca/dataset/arceaux-velos), la [cartographie du réseau cyclable](http://donnees.ville.montreal.qc.ca/dataset/pistes-cyclables) et le [nombre de passages quotidiens sur les pistes cyclables](http://donnees.ville.montreal.qc.ca/dataset/velos-comptage) sur le territoire de la Ville de Montréal.


> [STOP] premier exercice soft et pour s'assurer que tout le monde est bien set up : on load les données BIXI!

À cette liste, nous pourrions ajouter une tonne d'autres sources de données permettant de mieux contextualiser les observations et améliorer nos prédictions. Parmi celles-ci, notons les suivantes :

- Données météorologiques
    - [température](http://climate.weather.gc.ca/historical_data/search_historic_data_e.html)
    - précipitations
    - vent
- Données géographiques
    - [découpage des quartiers](http://donnees.ville.montreal.qc.ca/dataset/polygones-arrondissements/resource/bc6e94c7-9393-490d-899f-4296dd1e3dcf)
    - pentes
    - altitude
    - cours d'eau
    - pistes cyclables
- Données démographiques
    - densité
    - distribution du revenu
    - distribution de l'âge
- Données individuelles
    - adresse
    - statut résident/visiteur
    - lieu de travail
- Historique de trajets par individu


Tel que mentionné en introduction, notre ouvrage est linéaire, donc le processus de sélection de données se fera d'un seul trait. Bien entendu, dans un projet réel, il est presque certain que certains aspects nous échappent au début, et que nous devions réajuster le tir dans les phases subséquentes.

> [STOP] demander aux participants leurs idées de dataset intéressant

## Considérations {#considerations}

Comme nous pouvons le constater, la liste de données potentielles est sans limites. Les considérations qui suivent auront pour objectif de nous exposer les limites de certaines sources, pour ensuite identifier celle(s) qui sont le plus adaptées à notre problématique.

### Volume
On parlera de volume plutôt que de quantité. Nous définissons le volume comme l'espace occupé par un objet sur disque ou en mémoire. Le volume d'une source de données sera donc le produit du nombre d'observations et de la richesse de chaque observation. Si notre source de données contient beaucoup d'observations ou des observations riches, nous aurons donc des contraintes de rapidité de calcul, d'espace disque ou de mémoire au moment de l'entraînement, et potentiellement même au moment de la prédiction selon l'algorithme choisi.

#### Nombre d'observations
Si nous avons le *malheur* d'être ensevelis sous un trop grand nombre d'observations, plusieurs options s'offrent à nous, qu'elles soient informatiques ou mathématiques/statistiques.

Du point de vue informatique, R est parfois réputé lent et capricieux en termes de mémoire vive. Or, [il n'en est rien](http://adv-r.had.co.nz/Performance.html). En effet, il existe plusieurs stratégies simples pour optimiser une expression R : [profiler le code](http://adv-r.had.co.nz/Profiling.html), [comprendre la gestion de la mémoire](http://adv-r.had.co.nz/memory.html) ou des interfaces [C++](http://adv-r.had.co.nz/Rcpp.html) et [C](http://adv-r.had.co.nz/C-interface.html).

Du point de vue mathématique, il serait étonnant que la quantité d'observations *étiquettées* soit trop grande. Comme a déjà admis le Scientifique en Chef de Google : «Nous n'avons pas de meilleurs algorithmes que n'importe qui d'autre; nous avons seulement plus de données»[https://www.forbes.com/sites/scottcleland/2011/10/03/googles-infringenovation-secrets]. Pour contourner les problèmes de mémoire ou d'espace disque, il existe des algorithmes qui taitent les données par *batches*, c'est-à-dire par tranches de $n$ observations à la  fois, où $n$ devra être assez petit pour contourner les contraintes d'espace, mais assez grand pour que l'algorithme puisse généraliser son apprentissage. [source algos à batches].

#### Format
La richesse d'une observation est définie par son format.

Nous définissons le format comme la façon dont est représentée une donnée. C'est entre autres ce qui va déterminer la richesse de chaque donnée. On peut alors placer les données sur un continuum de structuré à non-structuré. Certaines définitions considèrent que les données semi-structurées sont une classe à part... nous nous contenterons de dire qu'il existe un continuum de degrés d'organisation des données.

##### Données structurées
organisées en base de données relationnelles constituées de tables disposées en colonnes et rangées. On utilise un modèle de données  
SQL=Structured Query Language  
oracle

##### Données non-structurées
les données non-structurées sont celles qui ne sont pas organisées à l'aide d'un modèle de données. Une base de données non-structurée peut donc contenir à la fois du texte, des images, du son ou une combinaison (ex. vidéo).  
JSON  
XML

#### Aggrégation
À la section précédente, nous avons pris la peine de préciser que ce sont les données étiquettées qui ne peuvent pas être trop nombreuses. Par contre, pour toutes les données additionnelles à joindre à nos données étiquettées, il est possible que le... Par exemple, si le taux d'échantillonnage est trop grand pour nos besoins ... Le choix d'aggrégation des données sera alors capital. Nous préférerons tout de même toujours une source la plus granulaire possible, car en aggrégant nous même, nous évitons la perte potentielle d'information prédictive pour notre modèle

base / dplyr / data.table [autre section?](https://stackoverflow.com/questions/21435339/data-table-vs-dplyr-can-one-do-something-well-the-other-cant-or-does-poorly)

> [STOP] on va faire un merge des données ... Météo?

### Qualité
Plusieurs aspects peuvent influencer notre perception de qualité d'une source de donnée, tels que son organisation, sa documentation, son pouvoir prédictif, sa fréquence, la capacité de faire des jointures et le biais. Encore une fois, notons que les étapes futures peuvent invalider une source de données. Par exemple, le pouvoir prédictif est difficile à évaluer sans un premier modèle naïf au moins.

#### organisation et documentation des données
Ces 3 sources de données contiennent la même information. Avec laquelle préfériez-vous travailler *a priori*?

```{r, echo = FALSE}
maintenant <- lubridate::now()
kable_settings <- function(x) kableExtra::kable_styling(x, bootstrap_options = "striped", full_width = F, position = "left")

knitr::kable(tibble::data_frame(dt = maintenant, temps = 0.5/24)) %>%
  kable_settings
knitr::kable(tibble::data_frame(Date = maintenant, `Durée (minutes)` = 30)) %>%
  kable_settings
knitr::kable(tibble::data_frame(`Date début` = maintenant, `Date fin` = maintenant + 30 * 60)) %>%
  kable_settings
```

Évidemment, nous préférerons, dans l'ordre, 3, 2, 1. Bien que cet exemple soit simpliste et fictif, il illustre tout de même que des éléments tels que le nom, la structure ou la documentation des données influenceront notre capacité à maximiser notre utilisation de ces données.

#### complexité
Rapport coût-bénéfice
Bénéfice : A-t-on déjà des bons proxys?
Coût : complexité additionnelle

#### pouvoir prédictif
Garbage In Garbage Out

#### fréquence
À quelle fréquence la source de données est-elle mise à jour? À quelle fréquence l'information sous-jacente à la source de données change-t-elle? De quelle précision aurons-nous besoin? Est-ce que note prédiction dépend de données instantanées ou intemporelles?

```{r, warning = FALSE, echo = FALSE}
x <- tibble::data_frame(`Mise à jour fréquente` = c("Données météorologiques", "????"),
                        `Mise à jour rare` = c("????", "Données géographiques"))
rownames(x) <- c("Changements fréquents", "Changements rares")

knitr::kable(x) %>%
  kable_settings
```

météo demande d'avoir un live feed, alors que données géographiques peuvent être figées dans le temps

#### Merge
facilité/pertinence de la clé, etc. Exemple : historique des déplacements du client ... pas dispo. Données ouvertes Montréal : ok peut-être...

http://donnees.ville.montreal.qc.ca/dataset/pistes-cyclables
http://donnees.ville.montreal.qc.ca/dataset/geobase


#### Biais
socio-démographique, etc.
dès que des données personnelles sont en jeu
dès que des proxys de données personnelles sont en jeu
aggravé lorsque les prédictions ont un impact réel sur la vie des gens

### Accessibilité
Dans notre parallèle avec la table, le coût était la troisième considération principale. Dans le cas des données, nous allons plutôt parler d'accessibilité, dont le coût est une des composantes.

#### coût (gratuit?)
Si (la)[] (plupart)[] des (tribunes)[] s'entendent pour dire que le nouvel or noir est les données, nous aurons donc à considérer le coût pour accéder aux données. Or, plusieurs (#sources) sont gratuites.

##### Sources gratuites {#sources-gratuites}
- https://www.kaggle.com/datasets
- https://toolbox.google.com/datasetsearch
- https://www.reddit.com/r/datasets
- https://archive.ics.uci.edu/ml/index.php?fbclid=IwAR09F5grBOTCr1SS4v8gONEYeqk0DqqWpPdt1blmYF9ucZkhsQeM5T0E7ew
- http://donnees.ville.montreal.qc.ca/
- stats can
- ville Montréal
- gc.ca


##### Sources payantes {#sources-payantes}
- brokers
- ...

#### Lecture de fichiers
data.table (fread)
readr
readxl
xlsx

#### Connexion à une base de données
sqldf
dbplyr
rio
DBI
odbc
RMySQL, RPostgresSQL, RSQLite
foreign
haven
https://db.rstudio.com/

#### Web Scraping (légalité/api/service/timeouts)
XML
xml2
curl
httr
rvest
downloader
jsonlite
googlesheets

#### Accès aux données historiques et en temps réel
Exemple température

#### Stabilité
Pour que la stabilité des sources de données soit problématique, deux éléments doivent être réunis :
- le format des données change
- notre fréquence d'extraction est trop élevée pour nous donner le temps de réagir

Alors que nous n'avons peu de contrôle sur le premier élément, nous pouvons mitiger le deuxième. Nous pouvons travailler avec une *copie* des données, de sorte que nous ne serons pas affectés par un changement à la source. Évidemment, en contrepartie on sacrifie la réactivité... l'approche à prendre dépendra donc de la valeur d'avoir les données les plus à jour dans le modèle, de notre anticipation de la stabilité ainsi que de l'impact d'une panne du service.

Dans notre cas, les shapefiles...

- Évolution anticipée du format des données (pas de contrôle sur la source de données? → copier les données freezées. Le modèle a encore de la valeur)
- Pourquoi on va utiliser les shapefiles comme exemple de merge (statique, etc.)

#### sécurité et confidentialité
Comme dans tout projet informatique, nous devons nous assurer du niveau de sécurité approprié pour l'usage

## Références
