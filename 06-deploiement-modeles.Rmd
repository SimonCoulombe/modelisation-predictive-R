# Déploiement de modèles

L'ultime étape pour donner vie à un modèle est de le mettre en production. 

Les exemples de déploiement de modèles sont omniprésents. Il peut s'agir de cas plus évidents comme l'obtention d'un score de crédit afin de déterminer l'éligibilité pour un prêt ou encore une soumission pour un produit d'assurance basé sur les caractéristiques de l'appliquant et du bien assurés. De manière plus large, ça inclut également des recommendations d'achat faites par un marchand en ligne, aux utilitaires un appareil photo permettant la reconnaissance des visages ou à un système de conduite autonome. 

Il existe différentes manières d'y arriver, dépendemment du cas d'utilisation du modèle. 

Ce qu'on chercher à accomplir: 

usager => requête => serveur

serveur => résultat => usager

Nous verrons ici deux outils permettant de servir les résultats d'un modèle: 

 - [OpenCPU](https://www.opencpu.org/)
 - [Plumber](https://www.rplumber.io/)

Chacun de ces outils permet de servir un modèle via un REST API, ce qui signifie que les échanges sont réalisés au travers du protocole http. Comme il s'agit d'un protocole omniprésent, il existe de nombreux outils disponibles pour en faciliter l'intégration dans différentes applications. Ça signifie que ces applications peuvent exploiter les capacités de modélisation de R sans qu'elles aient à intégrer R d'aucune façon. L'utilisation d'un modèle via un REST API devient donc agnostic du ou des langages utilisés à l'intérieur de cet API. 

## OpenCPU

L'unité de travail de OpenCPU repose sur la librairie. Pour pouvoir servir un modèle, on doit donc le structurer de manière à ce qu'il puisse être servi par une librairie. Si l'approche peut paraître contraignante, elle incite à l'adoption d'une discipline dans la structure du code et de ses dépendances qui peut être salutaires à sa maintenance. 

### Débuter avec OpenCPU

Le déploiement d'un modèle se fait normalement à partir d'un serveur. Pour se familiariser avec l'outil ainsi que faciliter le développement, il est possible de rouler le serveur OpenCPU localement.

- Installer la librairie: `install.packages("opencpu")`  
- La charger: `library(opencpu)`
- Démarrer le serveur: `opencpu::ocpu_start_server()`

Une fois le serveur démarré, il est désormais possible d'interagir avec celui-ci via le port local: `http://localhost:5656/ocpu/test/` (localhost est ici équivalent à 127.0.0.1). 

Une requête http peut alors être faite à ce serveur. La programme `curl` est communément utilisé pour ces requêtes et peut être utilisé à partir du Terminal dans RStudio (ou à partir de tout autre terminal) ou même à l'intérieur de R grâce à des librairies comme `curl` our `httr`: 

`curl http://localhost:5656/ocpu/library/stats/R/rnorm/json -d n=2`

La commande précédente devrait retourner un résultat similaire au suivant:
![](./deploiement/ocpu_rnorm_demo.PNG)

Que s'est-il passé? Le serveur a reçu une requête pour la fonction `rnorm` situé dans `stats/R/` avec `n=2` comme paramètre. La segment `/json` a servi à demander à ce que le résultat soit retourné dans le format json. De fait, l'appel à la fonction a retourné un json contenant deux observations simulées d'une Normale(0,1). 

Il a été possible de faire la précédente requête puisque par défaut, la librairie `stats` est accessible par OpenCPU, à l'instar de l'ensemble des librairies accessibles au niveau système. Pour servir un modèle prédictif, une librairie permettant de retourner les prédictions de ce modèle devra donc être développée. 

### Bâtir un squelette de librairie

Avant d'illustruer le déploiement d'un modèle, nous allons présenter comment utiliser OpenCPU à partir d'un exemple le plus minimal possible.

La première étape est d'avoir le code structuré en une librairie. Une librairie peut être initialisée à l'aide de la commande: 

```
usethis::create_package("~/nom_de_la_librairie/")
usethis::create_package("~/model.ocpu/")
```

Alternativement, Rstudio offre la fonctionnalité à partir du menu: `File -> New Project -> New Directory -> R Package`. 

Une librarie en R n'est qu'un projet qui contient un fichier `DESCRIPTION`, un fichier `NAMESPACE` ainsi qu'un dossier `R/` à l'intérieur duquel se trouve les codes R. Les fonctions d'initialisation ci-haut ne sont que des aides facilitant la création de cette structure. 

Une fois la création du squelette complétée, une fonction test peut être créée. Par exemple, avec le code suivant dans le fichier `./R/salut.R`:

```
#' Salut
#' Une fonction qui salue.
#' @export
salut <- function() {
  print("Salut tout le monde!")
}
```

La librairie mode.ocpu peut maintenant être bâtie. 
La documentation sera d'abord générée:

```
devtools::document()
```

Et l'installtion peut se faire avec le raccourci: `ctrl-shift-B`.

En démarrant à une nouvelle session du serveur OpenCPU, il sera désormais possible d'intéragir avec la librairie model.ocpu nouvellement installée. 

`curl http://localhost:5656/ocpu/library/model.ocpu/R/salut/json -d ""`


### Intégrer un modèle prédictif dans une librairie

Une librairie permettant de retourner des prédictions devra supporter les fonctionnalité suivante: 
 - Lire les informations relativement aux observations pour lesquelles une prédiction doit être retournée
 - Appliquer les possibles transformations utilisées dans la préparation des données sur lesquelle le modèle a été construit. 
 - Effectuer la prédiction sur ces donnée à partir du modèle sélectionné pour la déploiement. 

Comment rendre accessible le modèle entraîné à l'intérieur de la librairie? Un modèle peut être représenté comme la combinaison entre des paramètres et un algorithme décrivant comment de transformer les informations d'une observations en une prédiction. Lorsqu'un modèle est entraîné, l'objet résultant contient ces informations, de sorte que la fonction `predict` appliquée sur ce modèle permet d'obtenir les prédictions désirée. 

L'approche la plus naturelle sera donc de sauvegarder le modèle désiré à l'intérieur de la librairie et de le rendre accessible aux fonctions de la librairie d'inférence. 

La méthode recommandée consiste en la création d'un script générant un fichier `.Rda` qui contient les modèles et autres objets R nécessaires à l'inférence. Ce script sera localisé dans le dossier `data-raw`. 

Par exemple, ce script sera le suivant pour rendre disponible le modèle développé à la section précédente: 

```
model_iris <- xgboost::xgb.train(params = param, data = dtrain, nrounds = 400)
usethis::use_data(model_iris, internal = T, overwrite = T)
```

Une fonction d'inférence peut maintenant être construite. 

```
#' Classification from vector of features
#' @export
pred_xgboost_vector <- function(input = c(0,0,0,0)) {
  data_pred <- matrix(input, nrow = 1)
  prob <- predict(object = model_iris, newdata = data_pred)
  label = iris_levels[which.max(prob)]
  return(list(label = label, prob = prob))
}
```


 - Ajouter des fonctions et dépendances
 
 - Déploiement sur server

 - Intégration avec webhook
 
 - Déploiement sur Docker


## Plumber

L'approche prise par Plumber repose sur l'ajout d'annotations au code. La technologie sous-jacente est similaire à OpenCPU, l'idée étant de convertir un code R en des services accessibles via le protocole HTTP.


### Débuter avec Plumber
 
- Installer la librairie: `install.packages("plumber")`  
- La charger: `library(plumber)`

Une exemple minimaliste est founi dans le dossier `deploiement/model.plumber/plumber_ini.R`. 
Ce code peut être servi en un service plumber de la manière suivante: 

```
pr <- plumber::plumb("src/deploiement/model.plumber/plumber_ini.R")
pr$run()
```

Il est alors possible d'accéder au service: 

```
curl -X GET "http://127.0.0.1:5099/message?msg=Salut!" -H "accept: application/json"
```

```
curl -d "msg=Salut!" -G "http://127.0.0.1:5099/message"
```

### Inférence de modèle prédictif


```
pr <- plumber::plumb("src/deploiement/model.plumber/pred_xgboost.R")
pr$run()
```

```
curl -X GET --data '{"Sepal.Length":0,"Sepal.Width":0,"Petal.Length":0,"Petal.Width":0}' "http://localhost:5099/xgbkwargs"
curl -X GET --data '{"input":[1.1,2.2,3.3,4]}' "http://localhost:5099/xgbvector"
```

### OpenAPI/Swagger doc

### Déploiement docker

 