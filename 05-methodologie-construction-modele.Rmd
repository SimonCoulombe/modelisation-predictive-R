# Construction de modèles {#metho}


Une fois les données nettoyées convenablement, la variables réponses et les variables explicatives choisies, nous sommes prêts pour construire le modèle prédictif.
Bien sûr, le choix des variables explicatives restent préliminaires : on peut se rendre compte, après avoir tenté plusieurs modèles, qu'elles ne véhiculent pas assez d'information pour permettre des prédictions satisfaisantes.
Dans ce cas, il faudra alors considérer d'autres options; transformer nos variables, peut-être même en collecter de nouvelles.

Supposons pour l'instant que notre variable réponse $y$ est quantitative. En introduction du monographe, nous avons fait l'hypothèse qu'une fonction $f$ connecte nos variables explicatives $\mathbf{x}$ à $y$ de telle sorte que
$$
  \mathbf{y} \approx f(\mathbf{X}).
$$
L'objectif principal, dans ce chapitre, est d'apprendre (ou plutôt d'approximer) la fonction $f$ à l'aide de la théorie de l'apprentissage statistique. Plusieurs éléments sont tirés des livres *An Introduction to Statistical Learning: with Application in R* de Gareth James, Daniela Witten, Trevor Hastie et Robert Tibshirani; *The Elements of Statistical Learning* de Trevor Hastie, Robert Tibshirani et Jerome H. Friedman; et finalement *R for Data Science* de Hadley Wickham et Garrett Grolemund. L'expression *apprentissage statstique* a été grandement popularisé par les auteurs des deux premières références, qui donne la définition (traduction libre)

>> L'apprentissage statistique fait référence à un ensemble d'outils pour modéliser et comprendre des jeux de données complexes. C'est une sous-discipline récente de la statistique qui se développe en parallèle avec les avancées en informatique et, plus particulièrement, en apprentissage automatique. [@James:2014:ISL:2517747]

Nous supposons que le lecteur possède des connaissances de base en statistique (distribution, espérance, variance, etc).

Les sujets suivant sont traités dans le chapitre : la gestion du jeu de données; l'identification de modèles adéquats; l'estimation de modèles (fonction de perte, compromis biais-variance); la sélection d'un modèle (AIC,BIC,validation croisée); et l'évaluation de ce modèle (erreur de généralisation). 



## Gestion des données {#split}

Pour plusieurs raisons, il est conseillé, avant même le pré-traitement des données de la Section \@ref(preprop), de séparer aléatoirement son jeu de données en trois partie distinctes : les jeux de données d'entraînement, de validation et de test.
Chacune des trois parties est associées à une étape de la construction du modèle. Les données d'entrainement serviront à estimer nos modèles; les données de validation à sélectionner un modèle; les données de test à évaluer le modèle final.
Lorsque le nombre d'observation le permet, la règle du pouce généralement employée est d'utiliser la moitié(e?) des observations pour l'entrainement et le quart pour chacune des deux autres étapes.
$$
  {\Large \left(\mathbf{X}|\mathbf{y}\right)}
  \quad
  =
  \quad
  \left(\begin{array}{ccc|c}
    x_{11} & \dots & x_{1d}  & y_1\\
    x_{21} & \dots & x_{2d}  & y_2\\
    \vdots &  & \vdots & \vdots\\
    x_{n1} & \dots & x_{nd}  & y_n
  \end{array}\right)
  \begin{array}{ccc}
    \Bigg\} & \stackrel{\approx 1/2}{\longrightarrow} & (\mathbf{X}_{\rm train}|\mathbf{y}_{\rm train})\\
    \Big\} & \stackrel{\approx 1/4}{\longrightarrow} & (\mathbf{X}_{\rm val}|\mathbf{y}_{\rm val})\\
    \Big\} & \stackrel{\approx 1/4}{\longrightarrow} & (\mathbf{X}_{\rm test}|\mathbf{y}_{\rm test})
  \end{array}
$$

Il est important de garder en tête que ce n'est qu'une *règle du pouce*.
Si le jeu de données contient peu de signal (d'information) pour prédire $\mathbf{y}$, il se peut qu'en laissant de côté certaines observations, l'estimation des modèles soit trop déficiente pour être utile.
Les étapes des Chapitre **REF** permettent généralement de se faire une idée de la situation.
Il est aussi conseillé (à moins qu'on veuille prendre avantage d'un effet temporel dans nos données) d'effectuer une permutation aléatoire de nos données, pour éviter que notre division du jeu de données ne soit pollué par des effets indésirables de l'ordre de collecte.


On associe aux jeux de données d'entraînement l'*erreur d'entrainement*, qui est l'erreur qu'on minimise lors de l'estimation du modèle.
En contrepartie, les jeux de données de validation et de test servent tous deux à estimer l'*erreur de généralisation*, c'est-à-dire l'erreur faite sur de **nouvelles** données.
Cela explique d'ailleurs la confusion entourant ces derniers et leur utilité.
Les données de validation servent à choisir un modèle parmi tous ceux estimés.
À titre d'exemples concrets, ceci permet de choisir le nombre d'interactions croisées dans un modèle linéaire généralisé ou le nombre de couches cachées dans un réseau de neurones.
Malgré que ce ne soit pas particulièrement conseillé, il peut arriver que l'on veuille re-mélangé les jeux d'entrainement et de validation lors du processus.
Il est absolument impératif toutefois de garder les données de test dans un coffre-fort bien cellé.
Sinon, notre estimation finale de l'*erreur de généralisation* pourrait être induement optimiste. Il faut garder en tête que l'objectif est de prédire de **nouvelles** valeurs $y$ à l'aide de **nouvelles** valeurs $\mathbf{x}$; l'erreur de généralisation est donc au coeur de nos préoccupations.

Lorsque trop peu de données sont disponible, l'étape de test doit souvent être abandonnée et des techniques plus sophistiquées peuvent nous permettre d'estimer l'erreur de généralisation à partir des données d'entrainement.
Pour expliquer plus en détails les méthodes en question, mettons-nous dans cette situation et divisons notre jeu de données en deux : les données d'entraînement et de test.
```{r, echo=T}
# data_bixi <- data.table::fread("data/data_bixi.csv") # jeu de données original
# ind_test <- sample(nrow(data_bixi),nrow(data)*25/100) # 25% des données assignées au jeu test
# saveRDS(ind_test, "data/models/ind_test.rds")
#
# source("src/preprocessing/preprocessing.R")
# data_preprocess <- preprocessing(data_bixi[-ind_test,], path_objects = "data/models/", train = TRUE)

data_preprocess <- fst::read.fst("data/data_preprocess.fst", as.data.table = T)

y_duree <- data_preprocess$target_duree
y_meme <- data_preprocess$target_meme_station

X <- data_preprocess[,-c("target_duree","target_meme_station")]
rm(data_preprocess)
```
Pour une fonction qui automatise le processus, les fonctions `caret::createDataPartition` and `caret::createFolds` sont des options intéressantes.

Encore une fois, notez que **la séparation du jeu de données précède le prétraitement**.
L'étape de sélection de modèle, en fin de chapitre, sera effectuée à l'aide de la validation croisée.
D'ici là, nous utilisons $\mathbf{X}$ en tant que données d'entrainement.


## Description d'un modèle {#description}

Commençons d'abord en reformulant REF en tant qu'égalité stricte.
Pour ce faire, on introduit une quantité aléatoire $\varepsilon$ qui représente la variabilité non captée par notre modèle.
Cela donne ainsi l'équation
$$
  y = f(\mathbf{x}) + \varepsilon.
$$
Pour une variable réponse $y$ continue, il est naturel de faire les deux hypothèses suivantes à propos de $\varepsilon$:

- son espérance est nulle, c'est-à-dire $\mathbf{E}(\varepsilon) = 0$; et
- elle est indépendante de $\mathbf{x}$.

Ceci nous permet entre autre d'ignorer $\varepsilon$ lorsque vient le temps de faire une prédiction.
Étant donné $\mathbf{x}$, on s'attend à ce qu'en moyenne $y$ soit égale à $f(\mathbf{x})$, *i.e.* $\mathbf{E}(y) = f(\mathbf{x})$.

En introduisant $\varepsilon$, on admet l'existence d'une erreur *irréductible* : même si nous réussissions à estimer $f$ parfaitement, il faudrait s'attendre à ce que nos prédictions ne soient pas nécéssairement parfaite.
Par exemple, même en sachant que nos données sont telles que $y_i = 2x_i + \varepsilon_i$, l'erreur moyenne de prédiction résultante (avec le vrai modèle) sera d'environ $\mathbb{V}{\rm ar}(\varepsilon_1)$.
Avec $n=25$, on obtiendrait par exemple le graphique suivant, qui montre l'écart résiduel $y_i - f(x_i)$ entre la fonction de prédiction $f(x)$ et les observations $(y_i)_{i=1}^n$.
```{r, echo=FALSE}
dt <- data.table(x = runif(25), eps = rnorm(25,0,.1))
dt[, y := 2*x + eps]

ggplot(dt, aes(x=x)) +
  geom_point(aes(y=y)) +
  geom_line(aes(y=2*x)) +
  geom_text(x=.15, y=.4, label="f(x)") +
  theme_minimal()
```

Ceci s'explique par la présence de facteurs influençant $y$ auxquels nous n'avons pas accès (qui ne sont pas mesurés) ou qui ne sont simplement pas mesurables; mais aussi par un choix de modèle $f$ qui ne permet pas de capturer l'essentiel de la relation entre $\mathbf{x}$ et $y$.
Ainsi, pour minimiser l'erreur dite *réductible* autant que possible, il est nécessaire d'accéder à un maximum d'information (pertinente! et si possible non-redondante) et de choisir une famille de modèles appropriée pour le problème qui nous intéresse.

On divise généralement les problèmes en deux grandes catégories : la régression et la classification.
La régression sous-entend une variable réponse continue, *e.g.* la grandeur d'une individue ; la classification sous-entend une variable réponse catégorique (une classe), *e.g.* chat ou chien.
Plusieurs exemples de ces deux types de modèles sont présentés dans la dernière section du chapitre.
Nous continuons pour l'instant en supposant que notre variable $y$ est continue.
Toutefois, les concepts présentés sont aussi valides pour la classification; d'autant plus que pour classifier des observations/exemples, on modélise généralement la probabilité qu'une observation appartienne à certaine une classe, ce qui rend en quelque sorte notre variable réponse continue.
On assigne ensuite l'observation à la classe avec la plus forte probabilité.


## Choix d'un modèle {#choix}

Un modèle est en fait un ensemble de contraintes qu'on impose à la fonction $f$ de REF.
L'introduction de contraintes limite le type de relation entre $Y$ et $\mathbf{X}$ qu'il sera possible d'apprendre ; paradoxalement, c'est aussi ce qui permet l'apprentissage.
Par exemple, la (bien connue!) régression linéaire sous-entend une relation linéaire entre la variable réponse (continue) et les facteur explicatifs :
$$
  y = f(\mathbf{X}) + \varepsilon = \beta_0 + \beta_1 x_1 + \dots + \beta_1 x_d + \varepsilon
$$
Ici, les paramètres $\mathbf{\beta} = (\beta_0,\dots,\beta_d)$ détermine comment un changement porté aux variables explicatrices va influencer notre prédiction.

La plupart du temps, les modèles plus contraignants sont favorisés lorsque peu d'observations sont disponibles.
Lorsque justifiées, de telles contraintes permettent de prendre avantage d'une structure dans les données qui est connue (ou supposée) *à priori*.
Certains modèles comme les réseaux de neurones profonds sont reconnus pour être efficaces dans des cas ou la relation entre les variables peut être très complexe, mais requierent généralement une quantité astromnomique de données.
Certains modèles plus simples, comme la régression linéaire justement, sont parfois favorisés aussi pour leur interprétation plus facile.
Dans ce document, nous nous intéressons plusau pouvoir prédictif qu'à l'interprétabilité.

Certaines contraintes sont parfois plus intuitives lorsqu'introduites via notre variable réponse. Supposons par exemple que notre objectif soit de prédire une proportion $y \in (0,1)$.
La régression linéaire présentée ci-haut n'assure pas le respect de cette contrainte (que $y$ soit toujours entre zéro et un).
Pour l'intégrer au modèle, on peut modifier le côté gauche de l'équation de telle sorte qu'il puisse prendre toutes les valeurs réelles, en s'assurant néanmoins que notre variable $y$ reste entre 0 et 1.
On utilise ici la fonction *logit*, qui donne
$$
  \mathrm{ln}\left( \frac{y}{1-y} \right) = \beta_0 + \beta_1 x_1 + \dots + \beta_d x_d + \varepsilon.
$$
Cette modification porte le nom de régression logistique et est un cas particulier de modèle linéaire généralisé. Le cas général est présenté en exemple à la fin du chapitre.


## Estimation d'un modèle {#estimation}

Estimer un modèle consiste à déterminer les valeurs de ses paramètres; trouver les paramètres optimaux. 
Ils sont choisis de telle sorte que le modèle soit le plus précis possible dans ses prédictions (sur les données d'entraînement).
On cherche à minimiser l'*erreur de prédiction* du modèle, ce qui sous-entend donc qu'on puisse quantifier cette erreur.
Pour ce faire, on utilise une fonction de perte $L(y,\hat{y}) = L(y,f(x))$, qui détermine la pénalité associée à une prédiction.
Dans certains cas comme la détection de fraude, où une transaction identifiée comme frauduleuse sera vérifiée par une agente, il peut être souhaitable de minimiser le nombre de faux négatifs (les transactions frauduleuses qui nous glissent entre les doigts), quitte à introduire plus de faux positifs (des transactions identifiées frauduleuses qui ne le sont pas réellement).
Le choix de $L$ doit donc s'aligner avec nos attentes par rapport au modèle.

Concentrons-nous sur des cas plus classiques.
La fonction de perte la plus populaire est sans contredit l'erreur quadratique, $L(y,\hat{y}) = (\hat{y} - y)^2$.
Puisque nous avons à notre disposition plusieurs observations (supposées indépendantes), il s'agit de minimiser l'*erreur prédictive moyenne*, ou de façon équivalente la somme des erreur de prédiction.
Pour la régression linéaire (qui détermine $f(\mathbf{x})$) et l'erreur quadratique (qui détermine $L$), on obtient
$$
  \sum_{i=1}^n L(y_i,\hat{y}_i) = \sum_{i=1}^n (y_i - \hat{y}_i)^2 = \sum_{i=1}^n \Big(y_i - (\beta_0 + \beta_1 x_{i1} + \dots + \beta_1 x_{id})\Big)^2.
$$
En notation matricielle, c'est-à-dire lorsqu'on considère $\mathbf{y} \in \mathbb{R}^n$ et $\mathbf{X} \in \mathbb{R}^{n \times d}$, cela donne (en supposant ici $\beta_0 = 0$)
$$
  \mathrm{argmin}_{\boldsymbol{\beta}} \sum_{i=1}^n L(y_i,\hat{y}_i) = \mathrm{argmin}_{\mathbf{\beta}} \ (\mathbf{y} - \mathbf{X}^\top \boldsymbol{\beta})^{\top}(\mathbf{y} - \mathbf{X}^\top \boldsymbol{\beta}) = (\mathbf{X} \mathbf{X}^\top)^{-1} \mathbf{X} \mathbf{y},
$$
le bien connu estimateurs des moindres carrées.


- Un mot très rapide sur la régularisation (l2 (ridge), l1 (lasso), l1+l2 (elastic net))



## Sélection d'un modèle {#sélection}

Jusqu'à présent, nous avons traiter notre problème abstrait de prédiction comme si un seul modèle était choisi et estimé, mais rien ne nous empêche d'en essayer plusieurs à la fois; des modèles totalement différents et/ou des modèles semblables utilisant des variables explicatives différentes.
Dans ce cas, il devient nécéssaire d'utiliser une méthode adéquate pour choisir le modèle final, celui qui sera déployé.


### Validation directe

Notre objectif ultime concerne la prédiction **sur de nouvelles données**, on utilisera donc, si disponinble, le jeu de données de validation pour calculer l'*erreur de généralization*.
Cette dernière est en fait calculée de la même façon que l'*erreur de prédiction*, c'est-à-dire avec la fonction $L$.
Elle diffère toutefois du fait que les observations $(\mathbf{x},y)$ concernées n'ont pas été utilisé pour estimer le modèle; l'information qu'elles contiennent n'est donc pas "aggrégée" dans $f$.
Dans une telle situation, le choix est simple : on retient le modèle avec la plus petite *erreur de généralisation*.

Un mot sur neural nets.
- Exemple plot Keras.


### Validation croisée

Malheuresement, on n'a pas toujours un jeu de données de validation à disposition.

- estimation via CV (Jackknife et son évolution)
- mention du bootstrap...

Ces méthodes estiment l'*erreur de généralisation* moyenne conditionelle aux données $X$.






## Exemples additionels (bullshiat fo now)

**Exemples**

À des fins explicatives, nous focaliserons ici sur la méthode des $K$ plus proche voisins ($K$-ppv, notre traduction de $K$-nn, *K nearest neighbors*).
Elle peut servir par exemple pour prédire le type $y^*$ (membre vs non-memmbre) du prochain utilisateur BIXI, étant données l'heure $x^*$ de sa location.
Une façon naturelle de procéder consiste à trouver les $K$ usagers de notre jeux de données ayant utilisé le service aux heures les plus similaires et d'assigner à $y^*$ la catégorie la plus fréquente parmis les $K$ usagers retenus $y_{i_1},\dots,y_{i_K}$.
Dans cet exemple, la fonction $f$ implicite est
$$
  f(x^*) = \mathrm{mode}(x_{i_1},\dots,x_{i_K}), \qquad (x_{i_1},\dots,x_{i_K}) = \mathrm{argmin}_{(j_1,\dots,j_K)} \sum_{k=1}^K |x_{j_k} - x^*|.
$$
Malgré sa forme fonctionnelle quelque peu épeurante, la méthode est terriblement simple.
L'expression de droite consiste à trouver les $K$ usagers les plus similaires : on minimise la différence entre l'heure qui nous intéresse $x^*$ et celles des usagers choisis.
L'expression de gauche calcule la classe la plus fréquente.

Cette méthode peut aussi être utilisée pour prédire des variables continues comme celle qui nous intéresse, la proportion de membres parmi les usagers selon l'heure et l'arondissement. En nous restreignant encore à l'heure d'utilisation seulement, cela correspondrait à
$$
  f(x^*) = \frac{1}{K} \sum_{k=1}^K x_{i_k}, \qquad (x_{i_1},\dots,x_{i_K}) = \mathrm{argmin}_{(j_1,\dots,j_K)} \sum_{k=1}^K |x_{j_k} - x^*|.
$$


Un exemple pour chacune des trois méthodes.

  - régression linéaire (mention : bien comprendre les modèles linéaires est essentiel pour comprendre les modèles non-linéaire)
  - modèles lineaires généralisé (reg. logistique)
  - modèles additifs
  - arbres de décisions (boosted trees)
  - SVM (note : plus populaire en info)

**NOTE :** a-t-il été mention de...

- $g(Y) = f(X) + \epsilon$ -- modèle additif versus modèle multiplicatif?
