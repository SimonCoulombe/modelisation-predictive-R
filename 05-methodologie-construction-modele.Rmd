# Construction de modèles {#metho}

Une fois les données nettoyées convenablement, la variables réponse et les variables explicatives choisies, nous sommes prêts pour construire le modèle prédictif.
Bien sûr, le choix des variables explicatives restent préliminaires : on peut réaliser, après avoir tenté plusieurs modèles, qu'elles ne véhiculent pas assez d'information pour permettre des prédictions satisfaisantes.
Il faudra alors considérer d'autres options; transformer nos variables, ou en en collecter de nouvelles.

Supposons pour l'instant que notre variable réponse $y$ est quantitative. En introduction du document, nous avons fait l'hypothèse qu'il existe une fonction $f$ connectant nos variables explicatives $\mathbf{x}$ à $y$ de telle sorte que
\begin{equation} 
  \mathbf{y} \approx f(\mathbf{X}). (\#eq:approx)
\end{equation}
L'objectif principal, dans ce chapitre, est d'apprendre (ou plutôt d'approximer) la fonction $f$ à l'aide de la théorie de l'apprentissage statistique. Plusieurs éléments sont tirés des livres *An Introduction to Statistical Learning: with Application in R* de Gareth James, Daniela Witten, Trevor Hastie et Robert Tibshirani; *The Elements of Statistical Learning* de Trevor Hastie, Robert Tibshirani et Jerome H. Friedman; et finalement *R for Data Science* de Hadley Wickham et Garrett Grolemund. L'expression *apprentissage statstique* a été grandement popularisé par les auteurs des deux premières références, qui donne la définition (traduction libre)

>> L'apprentissage statistique fait référence à un ensemble d'outils pour modéliser et comprendre des jeux de données complexes. C'est une sous-discipline récente de la statistique qui se développe en parallèle avec les avancées en informatique et, plus particulièrement, en apprentissage automatique. [@James:2014:ISL:2517747]

Nous supposons que le lecteur possède des connaissances de base en statistique (espérance, variance, etc).

Les sujets suivants sont traités : la gestion du jeu de données (entraîenement/validation/test); l'identification de modèles adéquats; l'estimation de modèles (fonction de perte, compromis biais-variance); la sélection et l'évaluation d'un modèle (validation croisée, erreur de généralisation). 



## Gestion des données {#split}

Pour plusieurs raisons, il est conseillé, avant même le pré-traitement des données de la Section \@ref(preprop), de séparer aléatoirement son jeu de données en trois partie distinctes : les jeux de données d'entraînement, de validation et de test.
Chacune des trois parties est associées à une étape de la construction du modèle. Les données d'entrainement serviront à estimer différents modèles; les données de validation à sélectionner un modèle; les données de test à évaluer le modèle final.
Lorsque le nombre d'observation le permet, la règle du pouce généralement employée est d'utiliser la moitié des observations pour l'entrainement et le quart pour chacune des deux autres étapes.
$$
  {\Large \left(\mathbf{X}|\mathbf{y}\right)}
  \quad
  =
  \quad
  \left(\begin{array}{ccc|c}
    x_{11} & \dots & x_{1d}  & y_1\\
    x_{21} & \dots & x_{2d}  & y_2\\
    \vdots &  & \vdots & \vdots\\
    x_{n1} & \dots & x_{nd}  & y_n
  \end{array}\right)
  \begin{array}{ccc}
    \Bigg\} & \stackrel{\approx 1/2}{\longrightarrow} & (\mathbf{X}_{\rm train}|\mathbf{y}_{\rm train})\\
    \Big\} & \stackrel{\approx 1/4}{\longrightarrow} & (\mathbf{X}_{\rm val}|\mathbf{y}_{\rm val})\\
    \Big\} & \stackrel{\approx 1/4}{\longrightarrow} & (\mathbf{X}_{\rm test}|\mathbf{y}_{\rm test})
  \end{array}
$$

Il est important de garder en tête que ce n'est qu'une *règle du pouce*.
Si le jeu de données contient peu de signal (d'information) pour prédire $\mathbf{y}$, il se peut qu'en laissant de côté certaines observations, l'estimation des modèles soit trop déficiente pour être utile.
Les étapes des chapitres précédents permettent généralement de se faire une idée de la situation.
Il est aussi conseillé (à moins qu'on veuille prendre avantage d'un effet temporel dans nos données) d'effectuer une permutation aléatoire de nos données, pour éviter que notre division du jeu de données ne soit pollué par des effets indésirables de l'ordre de collecte.

On associe aux jeux de données d'entraînement l'*erreur d'entrainement*, qui est l'erreur qu'on minimise lors de l'estimation du modèle.
En contrepartie, les jeux de données de validation et de test servent tous deux à estimer l'*erreur de généralisation*, c'est-à-dire l'erreur faite sur de **nouvelles** données.
Cela explique d'ailleurs la confusion entourant ces derniers et leur utilité.
Les données de validation servent à choisir un modèle parmi tous ceux estimés.
À titre d'exemples concrets, ceci permet de choisir le nombre d'interactions croisées dans un modèle linéaire généralisé ou le nombre de couches cachées dans un réseau de neurones.
On appelle ces derniers, qui dictent la structure du modèle, des *hyper-paramètres*.

Malgré que ce ne soit pas particulièrement conseillé, il peut arriver que l'on veuille re-mélangé les jeux d'entrainement et de validation lors du processus de modélisation.
Il est absolument impératif toutefois de garder les données de test dans un coffre-fort bien cellé.
Sinon, notre estimation finale de l'*erreur de généralisation* pourrait être induement optimiste.
Il faut garder en tête que l'objectif est de prédire de **nouvelles** valeurs $y$ à l'aide de **nouvelles** valeurs $\mathbf{x}$; l'erreur de généralisation est donc au coeur de nos préoccupations.

Lorsque trop peu de données sont disponible, l'étape de test doit souvent être abandonnée et des techniques plus sophistiquées peuvent nous permettre d'estimer l'*erreur de généralisation* à partir des données d'entrainement.
C'est ce que allons faire dans ce chapitre.
Divisons donc notre jeu de données en deux : les données d'entraînement et de test.
```{r, echo=T}
# data_bixi <- data.table::fread("data/data_bixi.csv") # jeu de données original
# ind_test <- sample(nrow(data_bixi),nrow(data)*25/100) # 25% des données assignées au jeu test
# saveRDS(ind_test, "data/models/ind_test.rds") # on garde en mémoire la sélection
#
# source("src/preprocessing/preprocessing.R")
# data <- preprocessing(data_bixi[-ind_test,], path_objects = "data/models/", train = TRUE)

library(data.table)
library(ggplot2)
data <- fst::read.fst("data/data_preprocess.fst", as.data.table = T)[1:10000,]
data$moment_journee <- factor(data$moment_journee, labels=c("A","B","C","D"))
```
Pour automatiser le processus, les fonctions `caret::createDataPartition` and `caret::createFolds` sont des options intéressantes.

Encore une fois, notez que **la séparation du jeu de données précède le prétraitement**.
D'ici au moment ou nous nécessiterons le jeu de données test, nous utilisons $\mathbf{X}$ en référence aux données d'entrainement.


## Description d'un modèle {#description}

Commençons d'abord en reformulant l'équation \@ref(eq:approx) en tant qu'égalité stricte.
Pour ce faire, on introduit une quantité aléatoire $\varepsilon$ qui représente la variabilité non captée par notre modèle.
Cela donne l'équation
\begin{equation}
  y = f(\mathbf{x}) + \varepsilon.
  (\#eq:equal)
\end{equation}
Pour une variable réponse $y$ continue, il est naturel de faire les deux hypothèses suivantes à propos de $\varepsilon$:

- son espérance est nulle, c'est-à-dire $\mathbf{E}(\varepsilon) = 0$; et
- elle est indépendante de $\mathbf{x}$.

Ceci nous permet entre autre d'ignorer $\varepsilon$ lorsque vient le temps de faire une prédiction.
Étant donné $\mathbf{x}$, on s'attend à ce qu'en moyenne $y$ soit égale à $f(\mathbf{x})$, *i.e.* $\mathbf{E}(y) = f(\mathbf{x})$.

En introduisant $\varepsilon$, on admet l'existence d'une erreur *irréductible* : même si nous réussissions à estimer $f$ parfaitement, il faudrait s'attendre à ce que nos prédictions ne soient pas nécéssairement parfaites.
Par exemple, même en sachant que nos données sont telles que $y_i = 2x_i + \varepsilon_i$, l'erreur moyenne de prédiction résultante (avec le vrai modèle) sera d'environ $\mathbb{V}{\rm ar}(\varepsilon_1)$.
Générons $n=25$ observations à partir de ce modèle pour visualiser le phénomène.
```{r, echo=FALSE, out.width = "60%",fig.align='center'}
dt <- data.table(x = runif(25), eps = rnorm(25,0,.1))
dt[, y := 2*x + eps]

ggplot(dt, aes(x=x)) +
  geom_point(aes(y=y)) +
  geom_line(aes(y=2*x)) +
  geom_text(x=.15, y=.4, label="f(x)") +
  theme_minimal()
```
Aucune de nos prédictions (qui se trouvent toutes sur la droite) ne correspond à la vraie valeur $y$ observée.

L'intuition est qu'on admet la présence de facteurs influençant $y$ auxquels nous n'avons pas accès (qui ne sont pas mesurés) ou qui ne sont simplement pas mesurables.
L'utilisation d'un modèle $f$ qui ne permet pas de capturer l'essentiel de la relation entre $\mathbf{x}$ et $y$ peut aussi limiter notre potentiel de réduction de l'erreur.
Ce qui est en notre pouvoir (du moins, si on exclut la re-collecte de données) concerne la fonction $f$.
Il est donc important de choisir une famille de modèles appropriée pour le problème qui nous intéresse.

On divise généralement les problèmes en deux grandes catégories : la régression et la classification.
La régression sous-entend une variable réponse continue, *e.g.* la grandeur d'une individue ; la classification sous-entend une variable réponse catégorique (une classe), *e.g.* chat ou chien.
Avec nos données bixi, nous ferons une régression pour prédire la durée d'un trajet et une classification binaire pour prédire si un utilisateur terminera sa course dans le même arondissement ou non.
Des exemples de ces deux types de modèles sont présentés dans la dernière section du chapitre.
Notez que pour classifier des observations/exemples, on modélise généralement la probabilité qu'une observation appartienne à certaine une classe, ce qui revient en quelque à modéliser une variable réponse continue (une fréquence).
On assigne ensuite l'observation à la classe la plus probable.


## Choix d'un modèle {#choix}

Le choix d'une famille de modèles est intimement lié à la tâche que nous souhaitons résoudre et aux données dont nous disponsons.
En se restreignant à une certaines familles, on impose un ensemble de contraintes à la fonction $f$ de l'équation \@(eq:equal), ce qui limite le type de relation entre $Y$ et $\mathbf{X}$ qu'il sera possible d'apprendre ; paradoxalement, c'est aussi ce qui permet l'apprentissage.
Par exemple, la (bien connue!) régression linéaire sous-entend une relation linéaire entre la variable réponse (continue) et les facteur explicatifs :
\begin{equation}
  y = \beta_0 + \beta_1 x_1 + \dots + \beta_1 x_d + \varepsilon
  (\#eq:reg)
\end{equation}
Ici, les paramètres $\mathbf{\beta} = (\beta_0,\dots,\beta_d)$ déterminent comment un changement porté aux variables explicatrices va influencer notre prédiction.
Nous utiliserons ce modèle pour prédire la durée station à station d'un trajet en bixi.

La plupart du temps, les modèles plus contraignants sont favorisés lorsque peu d'observations sont disponibles pour prendre avantage d'une structure dans les données qui est connue (ou supposée) *à priori*.
Certains modèles comme les réseaux de neurones profonds sont reconnus pour être efficaces dans des cas ou la relation entre les variables est très complexe, mais requierent généralement une grande quantité de données.
Certains modèles plus simples, comme la régression linéaire, sont parfois choisient pour leur meilleure interprétabilité.

Pour une variable réponse catégorique (disons $K$ classes), la régression de \#(eq:reg) n'est pas conseillée [CITE].
On peut toutefois la modifier légèrement pour trouver un modèle de classification très répandu : la régression logistique.
Restons dans le cas binaire pour plus de clarté.
La clef consiste à considérer non pas notre réponse $Y$, mais $y^* = \mathbb{P}[Y = 1 | \mathbf{X}]$, la probabilité que $Y = 1$ conditionellement aux valeurs des variables explicatives $\mathbf{x}$.
Un problème majeur avec la régression en \@(eq:reg) est son incapacité à rester entre 0 et 1, l'espace naturel pour une probabilité.
Pour intégrer cette contrainte au modèle, on considère les *log-cotes* (*log-odds*) avec la fonction *logit* (*logistic unit*), ce qui donne
$$
  \mathrm{ln}\left( \frac{y^*}{1-y^*} \right) = \beta_0 + \beta_1 x_1 + \dots + \beta_d x_d + \varepsilon.
$$
Ce modèle est généralisable pour un problème de classification multi-classes.
La plupart du temps, la méthode du maximum de vraisemblance est utilisée pour ajuster ce modèle (voir *e.g.* [@Friedman:2001:ESL] pour plus de détails).



## Estimation d'un modèle {#estimation}

Estimer un modèle consiste à déterminer la valeur optimale de ses paramètres. 
Ils sont choisis de telle sorte que le modèle soit le plus précis possible dans ses prédictions (sur les données d'entraînement).
On cherche donc à minimiser l'*erreur de prédiction* du modèle, qu'on quantifie à l'aide d'une fonction de perte $L(y,\hat{y}) = L(y,f(x))$.
Elle détermine la pénalité associée à une mauvaise prédiction.
Dans certains cas comme la détection de fraude, où une transaction identifiée comme frauduleuse sera vérifiée par un agent, on peut vouloir minimiser le nombre de faux négatifs (les transactions frauduleuses qui nous glissent entre les doigts), quitte à introduire plus de faux positifs (des transactions identifiées frauduleuses qui ne le sont pas réellement).
En d'autres termes, le choix de $L$ doit s'aligner avec nos attentes par rapport au modèle.

La fonction de perte la plus populaire est sans contredit l'erreur quadratique, $L(y,\hat{y}) = (\hat{y} - y)^2$.
Puisque nous avons à notre disposition plusieurs observations (supposées indépendantes), il s'agit de minimiser la somme des erreur de prédiction.
Par exemple, combinée à la régression linéaire, l'erreur quadratique donne
$$
  \boldsymbol{L}(\boldsymbol{y},\boldsymbol{\hat{y}}) = \sum_{i=1}^n (y_i - \hat{y}_i)^2 = \sum_{i=1}^n \Big(y_i - (\beta_0 + \beta_1 x_{i1} + \dots + \beta_1 x_{id})\Big)^2.
$$
En notation matricielle, pour $\mathbf{y} \in \mathbb{R}^n$ et $\mathbf{X} \in \mathbb{R}^{n \times d}$, cela donne (en supposant ici $\beta_0 = 0$)
$$
  \boldsymbol{\hat\beta} = \mathrm{argmin}_{\mathbf{\beta}} \ (\mathbf{y} - \mathbf{X}^\top \boldsymbol{\beta})^{\top}(\mathbf{y} - \mathbf{X}^\top \boldsymbol{\beta}) = (\mathbf{X} \mathbf{X}^\top)^{-1} \mathbf{X} \mathbf{y},
$$
le fameux estimateurs des moindres carrées.

Plusieurs librairies R permettent l'ajustement de modèles linéaires généralisés.
Nous utilisons ici `glmnet` pour modéliser la durée d'un trajet. 
Tout d'abord, définissons une formule.
```{r, echo=T}
cols <- grep("start_q", colnames(data))
colnames(data)[cols] <- paste0("start_quartier_",1:length(cols))

rhs <- paste0(colnames(data)[-c(1,2)], collapse=" + ")
f <- as.formula(paste("target_duree", rhs, sep=" ~ "))
f
```
Pour introduire des effets croisée (des paramètres partagés par plusieurs variables, *e.g.* $\beta x_{i1} x_{i2}$), on utilise les deux-points `:` comme par exemple dans
```{r, echo=T}
paste0("moment_journee:", colnames(data)[cols], collapse=" + ")
```
Contentons-nous ici du modèle sans intéractions.
La fonction `model.matrix` permet de créer la matrice $\mathbf{X}$ qu'on doit fournir à la fonction.
```{r, echo=T}
glm00 <- glmnet::glmnet(x = model.matrix(f, data), y = as.matrix(data$target_duree,ncol=1), family = "gaussian", lambda=0) # On reviendra sur alpha et lambda...
```

Pour une petite idée de la fidélité des prédictions
```{r, echo=T, out.width = "50%",fig.align='center'}
data[,glm00_pred := predict(object = glm00, newx = model.matrix(f, data))]
ggplot(data) +
  geom_histogram(aes(x=target_duree), fill="green") +
  geom_histogram(aes(x=glm00_pred), fill="blue") + theme_minimal()
```
Évidemment, plus de variables explicatives sont nécessaires pour prédire les valeurs extrêmes.
Le modèle fait tout de même des prédictions personnalisées.
```{r, echo=T}
glm00$beta
```

### Un mot sur la régularisation

Utilisons maintenant des intéractions entre le moment de la journée et les quartiers de départ (que nous avons encodés dans `cols`).
```{r, echo=T}
rhs <- paste0(rhs, " + ",
              paste0("moment_journee:", colnames(data)[cols], collapse=" + "))
f <- as.formula(paste("target_duree", rhs, sep=" ~ "))
f
```
Lorsque beaucoup de variables explicatives (ou des fonctions de celles-ci) sont considérées simultanément, il est possible qu'un sous-ensemble d'entre elles ne soit pas pertinent pour la tâche à effectuer.
Plus généralement, lorsqu'un modèle est sur-paramétrisé par rapport à la quantité d'observations disponible, les techniques classiques d'estimation doivent être revues pour éviter le sur-apprentissage (*overfit*).
Le danger est que le modèle apprenne (en quelque sorte par coeur) le jeu de donnée d'entraînement, ce qui diminue son pouvoir de généralisation.

Les techniques de régularisation permettent de mitiger ces effets négatifs en modulant l'importance de certaines variables pour la prédiction. 
Nous l'expliquons ici dans le contexte de la régression linéaire, mais l'idée est valide ou généralisable pour plusieurs modèles.
L'approche consiste à ajouter une pénalité (appliquée aux paramètres $\boldsymbol{\beta}$) dans la fonction de perte $L$, c'est-à-dire
$$
  \sum_{i=1}^n L(y_i,f(\mathbf{x}_i | \boldsymbol\beta )) + \lambda P(\beta), \qquad \lambda \in \mathbb{R}.
$$
Le terme $\lambda$ est un *hyper-paramètre* controlant le degré de régularization que nous souhaitons appliquer.
La plupart du temps, la fonction $P$ pénalise davantage les vecteurs $\boldsymbol{\beta}$ avec de grandes valeurs.
Encore une fois, la norme euclidienne (carrée) qu'on utilise pour la fonction de perte est très populaire. 
$$
  P(\boldsymbol{\hat\beta}) = ||\boldsymbol{\hat\beta}||_2^2 = \sum_{j=1}^p \hat\beta_j^2.
$$
Sa combinaison avec la régression porte le nom de régression *ridge*. 
On l'utilise pour atténuer l'impact du bruit (la variance introduite par les variables non-pertinentes).
Intuitiviment, si certains coeficients $\beta_j$ sont artificiellement gonflés, alors on devrait obtenir une meilleure erreur de généralization lorsque ces derniers sont réduits.
La librairie `glmnet` permet la régression ridge avec le paramètre `lambda` avec *e.g.*
```
glmnet::glmnet(x = model.matrix(f, data), y = as.matrix(data$target_duree,ncol=1), family = "gaussian", lambda=1)
```
Pour des raisons computationelles (et de convergence), il n'est toutefois pas conseillé de fournir une valeur unique pour `lambda` à la fonction `glmnet`, mais plutôt un ensemble de valeurs pour chacunes desquelles l'algorithme ajustera un modèle.
À la prochaine section, nous verrons comment utiliser cette procédure conjointement avec la validation croisée pour déterminer parmi la multitude créée ici.

Une deuxième pénalité très populaire est la norme $\mathcal{L}^1$ des paramètres (la méthode *lasso*), donnée par
$$
  P(\boldsymbol{\hat\beta}) = \sum_{j=1}^p |\hat\beta_j|.
$$
Son grand avantage est qu'elle force, pour une intensité $\lambda$ assez forte, certains paramètres à zéro exactement (et non pas seulement à à être petits).
Dans ce cas, il est ensuite plus facile d'identifier les variables explicatives significatives et d'interprété le modèle.
La fonction `glmnet::glmnet` applique cette pénalité par défault avec `alpha = 1`.
En fait, `alpha` permet de pondérer les pénalités *ridge* et *lasso*, et donc d'utiliser les deux à la fois (la technique *elastic net*).

### Les hyper-paramètres et le compromis biais-variance

Pour comprendre pourquoi la régularisation fonctionne, il faut s'attarder au concept de *compromis biais-variance*.
Dans le cas de la régression linéaire, la fonction $f(\cdot| \boldsymbol{\hat\beta})$ estimée dépend des données par l'entremise de $\boldsymbol{\hat\beta}$, c'est donc dire qu'avec un autre jeu de données (provenant de la mème distribution) on obtiendrait un modèle différent.
La variance inhérente au processus d'estimation est une composante importante de l'*erreur de généralisation* (à un point $\mathbf{x}_0$ donné),
\begin{equation}
  \mathbb{E}[(Y - \hat{f}(\mathbf{x}_0))^2 | \mathbf{x}_0] = \mathbb{V}{\rm ar}(\hat{f}(\mathbf{X})) + {\rm Biais}
[\hat{f}(\mathbf{X})]^2 + \mathbb{V}{\rm ar}(\varepsilon).
\end{equation}
Le dernier terme est l'erreur irréductible, sur laquelle nous n'avons pas de contrôle.
Le deux premiers termes sont le biais (au carré) et la variance de l'estimateur $\hat{f}$ de $f$.
L'introduction d'une pénalité augmente le biais : certains paramètres se voient réduits injustement.
Par contre, l'effet sur la variance va dans l'autre sens : les modèles plus pénalisés auront tendance à moins changer lorsqu'entraînés sur de nouvelles données.

Pour illustrée l'idée, considérons des données qui proviennet d'un mélange de deux populations normales avec des moyennes différentes.
```{r, echo=T}
train_dummy <- data.table(y = sample(0:1, 50, replace = T)) # 100 obs of a bernoulli
train_dummy[, x := rnorm(n=50, mean=2*y, sd=1)]
```
On peut utiliser ces données pour prédire la classe associée à un nouveau point $x_0$ en fonction des (disons) 3 points $x_j$ dans `data_dummy` les plus près de $x_0$.
```{r, echo=T}
test_dummy <- data.table(y = sample(0:1, 50, replace = T))
test_dummy[, x := rnorm(n=50, mean=y, sd=1/2)]

new_y_pred <- class::knn(train = matrix(train_dummy$x,ncol=1),
                         test = matrix(test_dummy$x,ncol=1),
                         cl = train_dummy$y, # vraie classes du data d'ent.
                         k = 3) # nombre de voisins utilisé
head(cbind(as.numeric(as.vector(new_y_pred)), test_dummy$y))
```
Dans ce cas, `k` est l'hyper-paramètre.
Comme plusieurs hyper-paramètres, il sert à lisser nos prédictions.
Plus $k$ est grand, plus on aggrège d'information pour faire notre prédiction.
Par exemple lorsque $k = n$, le nombre d'observations dans le jeu d'entraînement, on obtient toujours la même prédiction (la classe avec le plus de représentants).
Approximons l'erreur de généralisation pour chaque valeur de $k$ au point $x_0 = .75$.
Notons que $\mathbb{E}[Y|X = .75]$ est donné par
```{r, echo=T}
x0 <- .75
Ex0 <- dnorm(x0,1,1/2)
```
On s'en sert ici pour calculer le biais
```{r, visible = F}
# createMseFig()

```
![Dummy MSE](dummy-mse.png)

Fait important, les hyper-paramètres comme $\alpha$, $\lambda$ et $k$ ne peuvent être estimés de façon traditionnelle.
On se contente souvent d'estimer un modèle pour chaque valeur de `alpha` et `lambda` préalablement choisies : ce qu'on appelle souvent un *grid search*.
On peut utiliser la fonction `expand.grid` pour créer la grille
```
hp_grid <- expand.grid(alpha = seq(0,1,.01), lambda = seq(0,10,.1)).
```
Pour la fonction `glmnet::glmnet`, on peut laisser l'argument `lambda` vide et spécifier `nlambda`, le nombre de valeurs à essayer (la fonction s'occupe du reste).
Par défaut `lambda = NULL` et `nlambda = 100`.
On a donc qu'à s'occuper de `alpha`
```{r, echo=T}
# glm_list <- lapply(seq(0,1,.25), function(alpha){
#   glmnet::glmnet(x = model.matrix(f, data), y = as.matrix(data$target_duree,ncol=1), family = "gaussian", alpha = alpha)
# })
```
Lorsque `alpha` est positif, les modèles associés aux plus fortes valeur de `lambda` feront usage de moins de variables pour expliquer la réponse; à l'opposé, `lambda = 0` produira un modèle les incluant toutes.
En particulier, le modèle *lasso* (`alpha = 1`) avec la plus forte intensité `lambda` utilise une seule variable.

Comme chaque combinaison d'hyper-paramètres produit un modèle, c'est à l'étape de sélection de modèle que leurs valeurs seront déterminées.


## Sélection d'un modèle {#sélection}

Une fois qu'on possède plusieurs candidats intéressants, il reste à choisir le modèle final parmi ceux-ci.
Pour rester fidèle à l'objectif ultime, on choisit le modèle avec la plus faible erreur de généralisation.
Voici quelques méthodes qui permettent de trouver ce modèle.

### Validation directe

Notre objectif ultime concerne la prédiction **sur de nouvelles données**, on utilisera donc, si disponible, le jeu de données de validation pour calculer l'*erreur de généralization*.
Cette dernière est en fait calculée de la même façon que l'*erreur de prédiction* sur le jeu d'entraînement, c'est-à-dire avec la fonction $\boldsymbol{L}$.
La fonction `predict` est l'outil R principal : on doit simplement lui fournir les données de validation, *e.g.* `predict(modele, newx = nouveau_data)`.
La différence majeure avec l'erreur de prédiction en entraînement vient du fait que les observations $(\mathbf{x},\mathbf{y})$ maintenant utilisées ne l'ont pas été durant l'estimation du modèle; l'information qu'elles contiennent n'est donc pas déjà "aggrégée" dans $f$.


### Critères classiques

Lorsqu'aucune donnée de validation n'est disponible, une alternative simple est d'utiliser des critères comme l'AIC (*Akaike Information Criterion*), le BIC (*Bayesian Information Criterion*), ou le $C_p de Mallows. 
Chacune de ses méthodes à ses particularités, mais elles s'opèrent similairement.
Penchons nous brièvement sur l'AIC par exemple.
Pour la régression linéaire,
$$
   AIC(f) = 2 k - 2 \boldsymbol{L}(\boldsymbol{y},f(\boldsymbol{\boldsymbol{x}}))
$$
où $k$ est le nombre de paramètres qu'implque la régression $f$ de [REF] et $\boldsymbol{L}$ est la perte quadratique.
Notez que ceci est valide justement parce que la perte quadratique, dans ce cas particulier, coincide avec la log-vraisemblance du modèle et $\boldsymbol{\hat\beta}$ est le vecteur qui minimise la perte.
L'utilisation de plus de paramètres permets un meilleur ajustement aux données d'entraînement (une valeur plus faible de $\boldsymbol{L}$), mais cette amélioration, pour être acceptée, doit compenser l'augmentation qu'induit le terme $2k$.

Ces méthodes, quoique très simples, reposent en général sur certaines hypothèses qui peuvent rendre leur utilisation douteuse.
Pour notre application, nous utilisons la validation croisée.


### Sélection par étapes (step-wise)

Backward - Forward - Stepwise

Lasso fait un peu ça!

### Validation croisée

- Bien important d'intégrer toute la procédure d'estimation! Et nous pas juste les modèles...
- $K$-fold
- glmnet à déjà une procédure pour `lambda` -- cv.glmnet(X, Y, parallel = TRUE)
- faisons de même pour `alpha` (on met en oeuvre le gris search)
- [REF] à un exemple sur additive models à la fin de la section.




## Exemples additionels (bullshiat fo now)

**Exemples**

À des fins explicatives, nous focaliserons ici sur la méthode des $K$ plus proche voisins ($K$-ppv, notre traduction de $K$-nn, *K nearest neighbors*).
Elle peut servir par exemple pour prédire le type $y^*$ (membre vs non-memmbre) du prochain utilisateur BIXI, étant données l'heure $x^*$ de sa location.
Une façon naturelle de procéder consiste à trouver les $K$ usagers de notre jeux de données ayant utilisé le service aux heures les plus similaires et d'assigner à $y^*$ la catégorie la plus fréquente parmis les $K$ usagers retenus $y_{i_1},\dots,y_{i_K}$.
Dans cet exemple, la fonction $f$ implicite est
$$
  f(x^*) = \mathrm{mode}(x_{i_1},\dots,x_{i_K}), \qquad (x_{i_1},\dots,x_{i_K}) = \mathrm{argmin}_{(j_1,\dots,j_K)} \sum_{k=1}^K |x_{j_k} - x^*|.
$$
Malgré sa forme fonctionnelle quelque peu épeurante, la méthode est terriblement simple.
L'expression de droite consiste à trouver les $K$ usagers les plus similaires : on minimise la différence entre l'heure qui nous intéresse $x^*$ et celles des usagers choisis.
L'expression de gauche calcule la classe la plus fréquente.

Cette méthode peut aussi être utilisée pour prédire des variables continues comme celle qui nous intéresse, la proportion de membres parmi les usagers selon l'heure et l'arondissement. En nous restreignant encore à l'heure d'utilisation seulement, cela correspondrait à
$$
  f(x^*) = \frac{1}{K} \sum_{k=1}^K x_{i_k}, \qquad (x_{i_1},\dots,x_{i_K}) = \mathrm{argmin}_{(j_1,\dots,j_K)} \sum_{k=1}^K |x_{j_k} - x^*|.
$$


Un exemple pour chacune des trois méthodes.

  - régression linéaire (mention : bien comprendre les modèles linéaires est essentiel pour comprendre les modèles non-linéaire)
  - modèles lineaires généralisé (reg. logistique)
  - modèles additifs
  - arbres de décisions (boosted trees)
  - SVM (note : plus populaire en info)

**NOTE :** a-t-il été mention de...

- $g(Y) = f(X) + \epsilon$ -- modèle additif versus modèle multiplicatif?
