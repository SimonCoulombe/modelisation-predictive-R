# Prétraitements des données

Pour faire de la modélisation prédictive, il est généralement inévitable de faire des prétraitements aux données pour exploiter autant que possible les différents algorithmes d'apprentissage. Ces prétraitements permettront de réaliser 2 choses:

1. Transformer les données dans un format compatible pour l'algorithme
2. Transformer les données de manière à faciliter l'apprentissage

À ce stade-ci, nous avons une certaine connaissance de ces données acquise grâce à l'analyse de données \@ref(exploration). Dans cette section, nous serons donc en mesure de produire des données propices à l'apprentissage qui sera faite dans la section \@ref(metho). Pour se faire, nous allons couvrir la théorie derrière le nettoyage, la réduction et la transformation de données. Ensuite, nous allons voir comment intégrer cette théorie dans le processus de modélisation. Finalement, nous allons présenter des packages en R qui permettent de faciliter ce pré-traitement de données.

## Théorie

Cette section a comme objectif de couvrir les concepts théoriques à la base du prétraitement de données. L'objectif est de présenter pourquoi il est primordial de considérer ces étapes avant de lancer des algorithmes d'apprentissage.

### Nettoyage de données

La première étape du prétraitement de données consite à nettoyer les données. Après cette étape, l'algorithme devrait au minimum être en mesure de fonctionner. Le nettoyage de données comprend plusieurs traitements propres à chaque jeu de données, voici ceux que nous avons identifiés et qui sont assez générales en pratique : 

1. Imputation de données manquantes
2. Traitement des données aberrantes
3. Encodage des données catégoriques

#### Imputation de données manquantes

La majorité des jeux de données réels sont témoins de données manquantes. Certains algorithmes ne sont pas en mesure de gérer les données manquantes par eux-mêmes. Cela nous oblige donc à traiter celles-ci et imputer le jeu de données. Les causes d'absence de données peuvent être relativement variées: bris de système, abstention, perte de données, etc. Il est généralement difficile de connaître la raison exacte, ce qui rend le traitement de données manquantes une étape importante. Avant même de trouver une méthode pour imputer ces données manquantes, il faut d'abord évaluer la quantité de données manquantes (% par variable) et tenter de comprendre le mécanisme expliquant la non-réponse d'une donnée. Il existe essentiellement 3 types de mécanismes de non-réponse:

1. Données manquantes complètement au hasard (MCAR)
2. Données manquantes au hasard (MAR)
3. Données manquantes pas au hasard (NMAR)

Dans le premier cas (MCAR), l'absence de donnée ne dépend pas de la vraie donnée. Par exemple, un système est défaillant dans la collecte de données et arrête de fonctionner en moyenne 5% du temps, et ce, de manière complètement aléatoire.

Dans le deuxième cas (MAR), l'absence de donnée dépend uniquement de la valeur des variables qui ont été observées. Par exemple, des données ont été récoltées à partir de 2 systèmes différents et il est toujours connu de quel système les données proviennent. Pour un des deux systèmes, les données ont une plus grande probabilité d'être manquante.

Dans le troisième cas (NMAR), l'absence de donnée dépend également des données manquantes. La probabilité de non-réponse peut dépendre de la variable elle-même ou d'une autre variable. Par exemple, c'est le cas si les personnes ayant un salaire faible ont une probabilité plus faible de répondre à la question d'un sondage en lien avec le salaire gagné.

[citer le cours d'anne-sophie]

Il existe différentes manières d'émettre une hypothèse quant au mécanisme de non-réponse. Certains tests [^tests] statistiques existent pour identifier un cas de MCAR. Cependant, il est difficile de différencier statistiquement MAR et NMAR. Pour se faire, on peut entres autres analyser le comportement des autres variables en fonction de l'absence ou la présence d'une donnée. Une connaissance du domaine d'affaire peut également être utile pour évaluer le mécanisme de non-réponse. Par la suite, il est possible de faire l'imputation selon différentes méthodes:

[^tests]: Voir le test de Welch et le test de Little. Les deux tests ne peuvent garantir l'hypothèse de MCAR. 

*Analyse des cas complets* <br>
Conserver uniquement les observations pour lesquelles toutes les variables sont présentes. Cette méthode est très simple, mais nécessite MCAR, sinon peut introduire un biais notable dans les estimateurs.
<br>

*Imputation par une mesure de centralité* <br>
Utiliser la moyenne, la médianne ou le mode pour remplacer les données manquantes. Cette méthode est relativement simple, mais peu recommandée pour des variables ayant un pourcentage élevé de données manquantes (même pour MCAR). Cela diminue la variabilité et la corrélation entre les variables. 

*Imputation par régression* <br>
Remplacer les données manquantes par la prévision de modèle de régression entraîné sur les observations pour lesquelles cette variable est présente. On doit traiter chaque variable séparémment. On sur-estimera la corrélation entre les variables et diminuera la variance des variables (moins que par l'imputation par mesure de centralité). <br>

*Imputation par régression stochastique* <br>
Même chose que la méthode par régression, mais on utilise un résidu aléatoire à la prévision. Cela permet d'augmenter la variance.

En bref, il est important d'avoir une bonne compréhension des données manquantes dans le jeu de donnée. Cela nous permet de prendre des décisions éclairées sur les techniques utilisées pour imputer ces valeurs manquantes. 

#### Traitement des données aberrantes

Une autre étape classique du nettoyage de données consiste à faire le traitement des données aberrantes. Ces données peuvent parfois avoir des effets importants sur l'estimation des paramètres du modèle. La première étape est d'abord d'identifier ces données et d'ensuite les traiter. Voici quelques méthodes traditionnelles pour faire la detection de celles-ci à l'intérieur d'une distribution quelconque:

1. $\pm$ 3 écarts-types de la moyenne
2. $\pm$ 1.5 EI (écarts inerquartile)
3. Partitionnement (*clustering*)

Une fois que les observations aberrantes ont été identifiées, il faut par la suite les traiter. Un expert du domaine peut être utile dans ce cas-ci pour analyser ces observations et confirmer leur validité. Si la validité d'une donnée est remise en question, il est coutume de retirer ces observations du jeu de données, car cela peut avoir un impact notoire sur le modèle. Il est important de bien documenter le retrait de ces observations.

#### Encodage des données catégoriques {#encodage}

Plusieurs algorithmes d'apprentissage ne peuvent pas traiter des données catégoriques directement. Il faut donc généralement encoder ces attributs en variables numériques pour pouvoir les utiliser dans le modèle. Pour se faire, il faut d'abord faire la distinction entre une donnée de nominale et une donnée ordinale. Dans le premier cas, il n'y a pas vraiment de notion d'ordre entre les différentes catégories. Dans le deuxième cas, on peut ordonner les catégories d'une certaine manière.

Voici un exemple de donnée ordinale (`condition_generale`) où il est possible d'ordonner les observations:

```{r, echo = FALSE}
data.table(Id = seq(1, 4), condition_generale = c('moyen', 'mauvais', 'excellent', 'bon'))
```

À l'inverse, voici un exemple où il est moins adéquat d'ordonner les observations selon la variable `quartier`:

```{r, echo = FALSE}
data.table(Id = seq(1, 4), quartier = c('centre-ville', 'plateau mont-royal', 'verdun', 'rosemont'))
```

Dans le cas d'une variable ordinale, une méthode d'encodage possible et adéquate (selon le contexte) serait d'assigner une valeur numérique pour chaque catégorie:

```{r, echo = FALSE}
data.table(Id = seq(1, 4), condition_generale = c(1, 0, 3, 2))
```

Dans l'exemple ci-dessus, il faut être **prudent**, car en encodant la variable de cette manière, on spécifie au modèle que la distance entre `'mauvais'` et `'moyen'` est la même qu'entre `'bon'` et `'excellent'`. Cette hypothèse n'est pas toujours valide selon le contexte. Les avantages avec cette méthode sont qu'elle est relativement simple, intuitive et qu'elle n'augmente pas le nombre de variables.

Dans le cas d'une variable nominale, il faut généralement opter pour une autre stratégie. Une méthode classique d'encodage dans ce cas-ci est d'utiliser la méthode *un-chaud*[^onehot] (*one-hot encoding*). Cette méthode consiste à traiter chaque catégorie comme une variable indicatrice qui indique la présence de la catégorie:

[^onehot]: Traduction libre, crédit à [Jean-Thomas Baillargeon](https://github.com/jtbai).

```{r, echo = FALSE}
data.table(Id = seq(1, 4), quartier_centre.ville = c(1, 0, 0, 0), quartier_plateau.mont.royal = c(0, 1, 0, 0), quartier_verdun = c(0, 0, 1, 0), quartier_rosemont = c(0, 0, 0, 1))
```

Le principal inconvéniant avec la méthode d'encodage *un-chaud* est qu'elle augmente le nombre de variables. Cela peut être significatif pour une variable ayant plusieurs catégories différentes. C'est d'ailleurs dans ce genre de situations qu'il peut devenir intéressant de regrouper certaines classes entres elles, ce que nous allons voir à la section \@ref(transformations).

Il existe d'autres méthode comme l'encodage binaire ou le *hasing*, mais les deux méthodes présentés plus en détails sont généralement les méthodes les plus utilisées en pratique.


### Réduction de données

À la section \@ref(encodage), nous avons vu que certaines méthodes d'encodages peuvent augmenter le nombres de variables. Dans le même ordre d'idées, certains domaines sont propices à récolter beaucoup d'attributs et ainsi complexifier l'espace de données. Lorsque cela survient, les algorithmes d'apprentissage peuvent souffrir d'un phénomène appelé le fléau de la dimensionnalité. Cela survient lorsque le nombre de dimensions est trop grand, ce qui crée une "distance" plus importante entre les certaines données et rend plus difficile la tâche d'apprentissage.

#### Analyse en composantes principales

#### Calcul de score

### Transformation de données {#transformations}

Avant de passer à l'entraînement d'un modèle, il est préférable pour la majorité des algorithmes d'utiliser les connaissances acquises lors de l'exporation de données (section \@ref(exploration)) pour transformer les données dans un format plus propice à l'apprentissage. Cette étape de transformation est souvent ce qu'on appelle le *features engineering*. Cette étape demande d'ailleurs beaucoup de va et vient avec la prochaine section \@ref(metho).

#### Normalisation

La normalisation des données permet de ramener les données autour d'une distribution plus "standard". Pour certains types de modèles, en particulier ceux qui sont fondés sur des calculs de distance comme les $k$-PPV ou le *clustering*, il est primordial de normaliser les données avant d'en faire l'apprentissage. En effet, cela permet de standardiser les distributions des différents attributs du jeu de données. Par exemple, si j'ai deux variables étant distribuées sur des domaines ayant des échelles complètement différents comme à la figure XX (la température (°C) et une indicatrice indiquant la présence de précipitations), il est préférable de plutôt comparer celles-ci sur un échelle commune comme à la figure YY. Voilà l'objectif fondamental de la normalisation.

Il existe différentes méthodes de normalisation. Il n'y a pas de "bonne méthode", mais certaines sont mieux adaptées à des contextes en particulier. Voici quelques exemples de méthodes traditionnelles où $x_{A}^{\prime(i)}$ représente la donnée $i$ normalisée pour l'attribut $A$ et $x_{A}^{(i)}$ représente la donnée originale.

1. Normalisation centrée-réduite

$$
x_{A}^{\prime(i)}=\frac{x_{A}^{(i)}-\bar{A}}{\sigma_A}
$$

où $\bar{A}$ est la moyenne et $\sigma_A$ la variance de l'attribut $A$.

2. Normalisation *min-max*

$$
x_{A}^{\prime(i)}=\frac{x_{A}^{(i)}-min_A}{max_A-min_A}\big(new\_max_A-new\_min_A\big)+new\_min_A
$$
où le $min_A$ et le $max_A$ sont calculés sur la distribution de l'attribut $A$ alors que $new\_max_A$ et $new\_min_A$ correspondent au nouvel interval désiré. 

3. Normalisation pas décimation

$$
x_{A}^{\prime(i)}=\frac{x_{A}^{(i)}}{10^j}
$$
où $j$ est la plus petite valeur entière où $max(|x_{A}^{(i)}|)<1$.

#### Discrétisation

La discrétisation consiste à prendre une donnée numérique $A$ et de la transformer en un ensemble de valeurs discrètes qu'on appelle souvent *buckets* ou *bins*. L'idée derrière ce genre de transformation est de simplifier la vie du modèle en lui "pré-mâchant" une donnée continue en certains groupes de valeurs plus faciles à apprendre.

Prenons l'exemple où on utilise la variable température directement pour prédire la durée du trajet. La figure XX montre les distributions de cette variable pour les membres et les non-membres.

À la figure YY, nous analysons encore une fois la variable température, mais cette fois en regroupant les données dans différents groupes basés sur la valeur de la température.

Dans la figure YY, on remarque qu'il semble est intuitivement plus facile de réaliser que ...

Il existe encore une fois plusieurs méthodes pour construire ces regroupements de valeurs. L'étape de l'exploration des données permet entres autres de bien comprendre celles-ci et de créer des groupes qui ajoutent une valeur au modèle. La présence d'un expert du domaine peut également être utile dans ce cas-ci.


#### Création de nouveaux attributs

Un autre type de transformation classique en modélisation consiste à créer de nouveaux attributs en utilisant les attributs originaux. Ces nouveaux attributs peuvent prendre la forme de scores ou tout simplement être une redéfinition d'un attribut en particulier.

C'est d'ailleurs le genre de transformation qui demande une certaine compréhension des données et de l'algorithme utilisé pour faire l'apprentissage. Par exemple, si on prend la date de départ d'un trajet, il est difficile d'intégrer directement cette variable dans un modèle prédictif. Toutefois, on peut utiliser cette variable pour créer un attribut qui indique si le moment de départ du trajet est un jour de semaine ou un jour de week-end. On pourrait également créer un attribut qui indique s'il y a eu des précipitations de pluie pendant la journée au lieu d'utiliser la quantité de pluie (mm) directement.

Ce type de transformation permet d'intégrer de l'ingénierie dans le jeu de données. Cela peut également permettre de réduire la dimensionnalité lorsqu'un nouvel attribut contient plusieurs autres attributs et que ceux-ci peuvent être retirés du jeu de données. Ultimement, toutes ces transformations de données ont pour objectif de faciliter l'apprentissage et ainsi obtenir un modèle ayant un plus grand pouvoir prédictif.

Les méthodes de transformation de données présentées dans cette section ont comme objectif de faciliter l'apprentissage. Cependant, étant donné que les données sont désormais transformées, il faut faire attention à l'interprétation des résultats. En effet, certaines transformations comme la normalisation ou le lissage font en sorte que les données ne sont plus dans leur format original.

## Intégration

- Outputer un json (ou des objets .rds) permettant de faire l'imputation en prod
- Méthode pour faire le one-hot encoding
- Avoir une fonction par feature et parler du naming pour les variables raw versus features
- Garder en mémoire les valeurs de normalisation du train 

## Packages suggérés

- `Hmisc` 
- `missForest`
- `MICE` (pas mon pref, difficile à reconduire en prod)

Je dois faire des tests et investiguer plus les deux premiers. 

## Exemple
