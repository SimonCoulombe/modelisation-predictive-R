[
["index.html", "Modélisation prédictive en R Chapitre 1 Préface", " Modélisation prédictive en R .Layer 2019-05-13 Chapitre 1 Préface En tant que scientifiques de données provenant des domaines des mathématiques, de la statistique ou de l’actuariat, nous avons acquis des bases relativement solides en modélisation prédictive. Ces notions faisaient parties de plusieurs de nos cours à l’université, ce qui fait en sorte que nous sommes bien outillés pour entraîner et évaluer des modèles. Par contre, en œuvrant sur le marché du travail, nous avons remarqué que certains concepts importants concernant l’application de ces concepts à des problèmes réels nous étaient plutôt inconnue. En effet, les modèles prédictifs produits dans un contexte professionnel requièrent généralement d’être facile à intégrer dans un système quelconque déjà en place. Nous avons donc décidé de joindre nos connaissances théoriques acquises à l’université aux considérations rencontrées sur le marché du travail pour proposer un processus de modélisation robuste qui permet de déployer des modèles, et ce, en utilisant le langage R. Ce livre regroupe plusieurs idées, discussions et concepts que nous avons partagés entres nous et que nous pensons utiles pour quiconque voulant déployer des modèles prédictifs via un interface robuste et facile d’ utilisation. Ce document n’est qu’une première itération, et ne constitue évidemment pas la vérité absolue ! Loin de nous la prétention de couvrir l’essentiel du sujet. Nous accueillerons vos critiques et commentaires avec joie si jamais vous êtes en désaccord avec certaines des idées ou des choix véhiculés dans ce livre. Nous avons rédigé ce livre en tant que membre de la communauté scientifique .Layer, qui a comme mission de promouvoir la collaboration et le partage de connaissances dans le domaine de la science des données. Nous espérons qu’il permettra à ses lecteurs et lectrices de peaufiner leurs connaissances et, surtout, de nourrir leur passion pour la science des données. "],
["introduction.html", "Chapitre 2 Introduction 2.1 Considérations 2.2 Jeu de données", " Chapitre 2 Introduction Modélisation prédictive en R est un livre proposant un proposant complet de modélisation, de l’extraction des données jusqu’au déploiement d’un modèle. Il comprend autant des notions théoriques que des considérations techniques nécessaires au déploiement d’un modèle dans un contexte de production. À des fins pédagogiques, nous avons séparé le processus en 5 grandes étapes. Tout d’abord, la collecte de données, souvent délaissée; ensuite les 3 étapes classiques d’exploration, prétraitement et modélisation, auxquelles nous ajoutons l’étape de déploiement. 2.1 Considérations Les données peuvent prendre toutes sortes de formes. C’est la beauté du travail du scientifique des données, mais aussi son plus grand défi. Dans ce livre, nous présentons certaines notions et procédures que nous croyons applicables dans plusieurs contextes, mais quiconque a œuvré dans un domaine impliquant de la modélisation le sait, l’aventure est rarement linéaire. On progresse, on revient, on réessaie … La présentation d’un processus linéaire ne fait donc évidemment pas honneur à la réalité. Une partie de ce va-et-vient est nécessaire et sain. Nous espérons que la méthodologie présentée permettra de minimiser le va-et-vient inutile autant que possible. De plus, en tant que document portant sur le langage R et la modélisation, nous utilisons, recommandons et citons plusieurs librairies et algorithmes. Le vaste étendu de méthodes disponibles en R rend impossible leur énumération. Nous avons choisit d’utiliser certaines librairies que nous recommandons, mais l’accent doit être mis sur le processus dans lequel elles sont insérées, et non sur les librairies elle-mêmes. 2.2 Jeu de données Pour illustrer différents exemples tout au long de ce document, nous avons utilisé un jeu de données ouvert et gratuit disponible sur le site de BIXI Montréal. BIXI Montréal est un organisme à but non lucratif créé en 2014 par la Ville de Montréal pour gérer le système de vélopartage à Montréal. Les données utilisées dans le cadre de ce livre sont en lien avec les trajets de bicyclette réalisés et enregistrés par le système de transport urbain. "],
["collecte.html", "Chapitre 3 Collecte de données 3.1 Description de l’approche 3.2 Liste des données potentielles 3.3 Considérations 3.4 Références", " Chapitre 3 Collecte de données ## Linking to GEOS 3.6.1, GDAL 2.2.3, PROJ 4.9.3 L’étape initiale de tout projet de modélisation prédictive est la collecte des données. Sans données, le modèle apprendra… rien! La présente section aura donc pour but de couvrir la collecte de données. Nous décrirons d’abord l’approche générale pour la collecte. Ensuite, nous dresserons une liste potentielle de sources de données pour la tâche à accomplir. Nous terminerons finalement en dressant une liste de considérations, tout en soumettant la liste de sources à ces considérations. 3.1 Description de l’approche L’étape de collecte peut être subdivisée à son tour en 3 sous-étapes : obtenir les données etiquetées, obtenir les données de jointure et effectuer la jointure. Nous verrons également que la collecte se produit souvent différemment lors de la modélisation et lors de la prédiction en production. 3.1.1 Obtenir les données etiquetées Dans tout projet de modélisation prédictive, il faut obtenir des données etiquetées, de façon à ce que le modèle puisse apprendre à prédire une étiquette. Ces données etiquetées sont des exemples réels tirés du passé et pour lesquels nous connaissons le résultat, et donc duquel le modèle pourra apprendre. Nous appellerons ces exemples des observations. Pour chaque observation, les données etiquetées contiennent au minimum l’étiquette, aussi appelée variable réponse. Autrement dit, c’est la valeur que nous voulons prédire. Dans notre cas, une observation sera un trajet. Pour chaque observation, nous aurons 2 étiquettes : la durée du trajet et le statut de membre du client. Sans information supplémentaire pour chacune des observations, nous serions en présence d’un simple échantillon aléatoire grâce auquel nous pourrions simplement prédire un estimateur comme la moyenne ou le mode pour tous les trajets. C’est pourquoi nous nécessiterons également au moins une information supplémentaire nous permettant de segmenter nos prédictions d’une observation à l’autre. Nous appellerons ces informations supplémentaires les prédicteurs. Par exemple, le jour et l’heure de départ pourraient nous donner de l’information prédictive sur la durée du trajet et sur le statut de membre du client. Voici de quoi pourrait avoir l’air notre jeu de données etiquetées : temps_depart duree_trajet membre 2017-05-07 17:00:02 1505 1 2017-05-29 18:50:11 120 0 2017-10-12 22:56:46 707 0 Ici, une ligne représente une observation ou un trajet, temps_depart est le prédicteur, et duree_trajet et membre sont les étiquettes. 3.1.2 Obtenir les données de jointure Après avoir trouvé nos données etiquetées contenant les étiquettes et les prédicteurs, nous sommes rarement satisfaits de la quantité de prédicteurs que nous pouvons y trouver. En effet, il arrive souvent dans les premiers instants d’un projet de prononcer une phrase du genre : “Imaginez si nous avions accès à telle ou telle donnée (qui malheureusement est absente des données etiquetées)”. Nous pouvons alors augmenter le jeu de données à l’aide de ce que nous appellerons les données de jointure. Nous les appellerons ainsi car nous devrons effectuer une opération de jointure entre notre jeu de données etiquetées et notre source de données additionnelle. Par exemple, si nous reprenons les données etiquetées précédentes, une source de données de jointure pourrait nous renseigner sur les heures de lever et de coucher du soleil pour chaque journée. En supposant que la noirceur décourage les cyclistes, cela pourrait aider à prédire la durée du trajet : date heure lever heure coucher 2017-05-07 06:35:00 20:10:00 2017-05-29 06:00:00 20:45:00 2017-10-12 07:30:00 17:55:00 Ici, la variable date nous permettrait d’effectuer une jointure entre ces données et les données etiquetées. 3.1.3 Effectuer la jointure Maintenant que nous avons les données de jointure et les données etiquetées, nous pouvons effectuer une jointure à partir des informations présentes dans les 2 jeux de données, soit la variable date : temps_depart duree_trajet membre date heure lever heure coucher 2017-05-07 17:00:02 1505 1 2017-05-07 06:35:00 20:10:00 2017-05-29 18:50:11 120 0 2017-05-29 06:00:00 20:45:00 2017-10-12 22:56:46 707 0 2017-10-12 07:30:00 17:55:00 Le jeu de données serait alors prêt à passer à l’étape de prétraitement 5, où nous pourrions par exemple créer une variable indicatrice noirceur qui indiquerait s’il fait noir ou non au début du trajet. 3.1.4 Collecte historique vs. collecte en production Dans les projets de modélisation prédictive, il existe généralement un fossé entre la modélisation, qui nécessite une collecte de données historiques, et la prédiction en production, qui nécessite une collecte de données en temps réel. Selon la configuration des systèmes en place et la nature de la problématique, le fossé peut être microscopique, et passer inaperçu, ou astronomique, et poser de sérieux ennuis pouvant même compromettre la faisabilité du projet. En effet, les natures différentes de ces 2 contextes peuvent parfois être irréconciliables : l’entraînement de modèles sur des données historiques exige un entreposage adéquat des données, mais les données historiques ont rarement été collectées dans l’optique de servire à entraîner des modèles. Ainsi, nous pouvons généralement reconnaître 3 situations : une source de données disponible à l’entraînement mais indisponible en production en temps réel est inutile. Par exemple, disposer d’instruments météorologiques qui nécessitent une récolte manuelle mensuelle des données serait bénéfique à l’entraînement, mais inutilisable en production. une source de données disponible en production en temps réel mais indisponible à l’entraînement peut être utile à condition de faire certaines hypothèses au jugement avant, pendant ou après l’entraînement. Par exemple, si nous pouvons effectuer une lecture en temps réel de nos instruments météorologiques mais que ces données ne sont pas déversées dans une base de données, nous devrons poser une hypothèse : introduire l’opinion d’experts ou le résultat d’analyses simplifiées dans le modèle de façon manuelle. Attention toutefois : le modèle devient sensible à notre hypothèse, et nous introduisons potentiellement du biais ou de l’incertitude indésirables dans notre modèle. entre les 2 situations précédentes se trouve le cas où les données d’entraînement sont différentes de celles de production. C’est notamment le cas quand une source de données fournit les données historiques, et une autre fournit les données en temps réel. L’hypothèse par défaut serait que les sources produisent la même distribution de données. Une autre hypothèse pourrait être par exemple que la source de production sous-estime systématiquement de 2 unités, et nous apporterions des corrections manuelles à la source de production. Peu importe la complexité de l’hypothèse, encore une fois le modèle devient sensible à cette hypothèse et nous introduisons potentiellement du biais et de l’incertitude. Alors que la première situation est difficile à corriger tant que les données en temps réel sont indisponibles, les 2 suivantes permettent de continuer d’avancer, puis de commencer à entreposer les données de la source de production afin de fournir au modèle les données de production historiques lors de son prochain réentrainement. Également, les données de jointure peuvent représenter des concepts connus d’avance (ex. les heures de lever et coucher du soleil pour une journée donnée), ou d’autres devant être recalculés ou récupérés d’une source externe à chaque prédiction (ex. la température ambiante au moment de la prédiction). Alors que le premier type peut déjà être calculé et entreposé d’avance, puis récupéré rapidement lors de la prédiction, le deuxième doit nécessairement être calculé ou récupéré en temps réel à chaque prédiction, ce qui peut augmenter le temps de réponse. Pour des raisons de différence entre les sources et de rapidité des calculs, il faudra donc distinguer la collecte des données selon si elle est faite dans l’optique de modélisation, de prédiction, ou des 2. 3.2 Liste des données potentielles Dans le but d’identifier les données étiquetées et de jointure, dressons d’abord une liste des données disponibles et potentiellement utiles. Les plus expérimentés auront déjà en tête la majorité des considérations couvertes plus tard dans ce chapitre au moment de dresser leur liste. Dans notre cas, permettons-nous de lister naïvement un grand nombre de sources. Chaque considération à venir nous permettra ensuite d’invalider certaines sources et de distiller notre sélection. 3.2.1 Données étiquetées Tout d’abord, une simple recherche des mots-clés données historiques BIXI dans votre moteur de recherche favori devrait vous mener à la page du site web de BIXI réservée à l’historique des déplacements. Un coup d’oeil aux données disponibles permet de constater qu’il y a 2 types de fichiers : des données historiques et de l’information sur les stations. Concentrons-nous pour commencer avec les données historiques. Pour l’année 2017, nous avons accès, pour chacun des mois d’ouverture du service (avril à novembre), à des données tabulaires sous cette structure : if(file.exists(&quot;data/data_bixi.csv&quot;)){ data_2017_04 &lt;- data.table::fread(&quot;data/data_bixi.csv&quot;, nrows = 5) } else { data_2017_04 &lt;- data.table::fread(&quot;https://s3.ca-central-1.amazonaws.com/jeremiedb/share/dot-layer/R-Quebec/BixiMontrealRentals2017/OD_2017-04.csv&quot;, nrows = 5)} data_2017_04 %&gt;% knitr::kable() %&gt;% kableExtra::kable_styling(full_width = FALSE, position = &quot;left&quot;) start_date start_station_code end_date end_station_code duration_sec is_member 2017-04-15 00:00 7060 2017-04-15 00:31 7060 1841 1 2017-04-15 00:01 6173 2017-04-15 00:10 6173 553 1 2017-04-15 00:01 6203 2017-04-15 00:04 6204 195 1 2017-04-15 00:01 6104 2017-04-15 00:06 6114 285 1 2017-04-15 00:01 6174 2017-04-15 00:11 6174 569 1 Voilà qui devrait nous fournir les éléments les plus précieux : nos 2 étiquettes. En effet, pour chacun des trajets, nous pourrons déterminer la durée avec la colonne duration_sec et le statut de membre du client à l’aide de la colonne is_member. Ces données renferment aussi d’autres éléments intéressants pouvant servir de prédicteurs. Nous pourrons notamment utiliser le jour de la semaine, l’heure et la station de départ pour tenter de prédire les variables réponses. Dans ce jeu de données, duration_sec et is_member sont les étiquettes. Les prédicteurs sont la station de départ, l’heure de la journée, le jour de la semaine, et le mois, qui peuvent tous fournir davantage de précision à nos prédictions. 3.2.2 Données de jointure La station de départ est représentée par un code unique représentant une entité unique et identifiable (souvent appelé ID). Cela fait non seulement d’elle un bon prédicteur, mais également une bonne clé pour amener des données de jointures. Par exemple, dans notre cas, le code de station est une porte d’entrée vers l’information spécifique aux stations ou à leur emplacement géographique. Nous pouvons dès lors utiliser n’importe quelle information reliée à la station ou son emplacement et susceptible de raffiner les prédictions. Or, il s’avère que le deuxième type de fichier fourni par BIXI contient la position GPS de chaque station : code name latitude longitude 7015 LaSalle / 4e avenue 45.43074 -73.59191 6714 LaSalle / SÃ©nÃ©cal 45.43443 -73.58669 6712 LaSalle / Crawford 45.43791 -73.58274 6715 Natatorium (LaSalle / Rolland) 45.44441 -73.57557 7048 MÃ©tro Angrignon 45.44653 -73.60354 Le jour de la semaine pourrait aussi servir à faire des jointures, par exemple avec une liste des jours fériés ou avec une table des heures de lever et de coucher du soleil. La liste des jointures possibles ne s’arrête pas là; allons passer en revue toutes les données de jointure potentiellement à notre disposition. Une recherche plus approfondie des mots-clés précédents (données historiques BIXI) devrait vous mener à la page du Portail données ouvertes de la Ville de Montréal réservée elle aussi à l’historique des déplacements. A priori, c’est simplement un lien vers les données précédentes. Un clic sur le bouton Vélo de la section Mots-clés nous ouvre toutefois une boîte de Pandore : L’état en temps réel des stations BIXI, la géolocalisation des arceaux à vélos, la cartographie du réseau cyclable et le nombre de passages quotidiens sur les pistes cyclables sur le territoire de la Ville de Montréal. À cette liste, nous pourrions ajouter une tonne d’autres sources de données permettant de mieux contextualiser les observations et améliorer nos prédictions. Parmi celles-ci, notons les suivantes : Autres données BIXI disponibilité des vélos aux différentes stations historique de trajets par individu Données météorologiques température historique température en temps réel précipitations vent Données géographiques découpage des quartiers altitude des stations pentes cours d’eau pistes cyclables Données démographiques densité distribution du revenu distribution de l’âge Données individuelles adresse statut résident/visiteur lieu de travail Tel que mentionné en introduction, notre ouvrage est linéaire, donc le processus de sélection de données se fera d’un seul trait. Bien entendu, dans un projet réel, il est presque certain que certains aspects nous échappent au début, et que nous devions réajuster le tir dans les phases subséquentes. Comme nous pouvons le constater, la liste de données potentielles est sans limites. Pour les besoins de l’atelier, les sources de données qui seront réellement considérées à partir de maintenant seront les suivantes : la géolocalisation des arceaux la cartographie du réseau cyclable le nombre de passages quotidiens sur les pistes cyclables la disponibilité des vélos aux différentes stations la température historique la température en temps réel le découpage des quartiers l’altitude des stations 3.3 Considérations La collecte de données est le point de départ de la chaîne. Les données étant quelque peu la matière première du produit que nous nous apprêtons à bâtir, plusieurs considérations influenceront leur collecte. Les premières considérations qui viennent à l’esprit dans le cas des matières premières sont évidemment le prix, la qualité et la quantité. Toutefois, d’autres facteurs influencent le choix de matière première comme la provenance, le processus de récolte ou de transformation, la constance de l’approvisionnement, l’entreposage et la sécurité. Les mêmes considérations peuvent s’appliquer dans notre contexte et viendront influencer la collecte des données. La présente section aura pour objectif d’aborder ces considérations, tout en les appliquant sur les sources mentionnées et retenues précédemment. 3.3.1 Accessibilité Pour une matière première, la première considération principale serait le coût. Dans le cas des données, nous allons plutôt parler d’accessibilité, dont le coût est une des composantes. 3.3.1.1 Coût Si plusieurs tribunes s’entendent pour dire que le nouvel or noir est les données, cet engouement vient nécessairement avec un coût, dicté par l’offre et la demande. Certaines sources sont gratuites, alors que d’autres sont payantes. Plusieurs approches sont donc possibles pour maximiser les bénéfices d’un projet tout en limitant ses coûts. 3.3.1.1.1 Sources gratuites Outre l’approche par moteur de recherche effectuée ci-haut, un balayage des différentes sources gratuites peut révéler des trésors cachés. Parmi ces sources gratuites, notons les suivantes : Kaggle Google Dataset Search Données ouvertes de la Ville de montréal Données ouvertes du Gouvernement du Canada Statistiques Canada UC Irvine et même Reddit! 3.3.1.1.2 Sources payantes Les sources payantes sont aussi vastes. Parfois elles sont accessibles en ligne, mais bien souvent des tarifs sont offerts à la pièce. Les fournisseurs principaux de ce type de données sont les courtiers, les aggrégateurs, les entreprise technologiques et les entreprises qui ont beaucoup de données. 3.3.1.2 Lecture et écriture Les sources de données peuvent se présenter sous plusieurs formes : un fichier, une base de données ou un service. Dans tous les cas, R gère facilement la lecture et l’écriture des données. 3.3.1.2.1 Lecture et écriture de fichiers Les sources de données se présentent parfois sous la forme de fichier. Aussi peu dynamique cela soit-il, il est fréquent d’avoir recours à des fichiers aux premiers stades de développement d’une idée. En effet, un fichier est souvent plus simple à obtenir, et plus rapide à configurer pour des besoins ponctuels qu’une connexion à une base de données ou à une API. Plusieurs packages R facilitent et accélèrent cette étape du processus. Parmi ceux-ci, notons data.table et ses fonctions fread et fwrite, readr, readxl et xlsx. 3.3.1.2.2 Connexion à une base de données L’importation de fichiers peut fonctionner à petite échelle. Pour éliminer des étapes manuelles, il convient de configurer l’environnement de modélisation pour qu’il puisse se connecter à des bases de données. Parmi les types de bases de données, on peut les placer sur un continuum allant de structuré à non-structuré. Certaines définitions considèrent que les données semi-structurées sont une classe à part… nous nous contenterons de dire qu’il existe un continuum de degrés d’organisation des données, avec à un extrême les données structurées, et à l’autre les données non-structurées. Sans entrer dans les détails, notons que les packages permettant à R de se connecter à des bases de données sont multiples : odbc, DBI, dbplyr, sparklyr, ROracle, RMySQL, RPostgresSQL, RSQLite, sqldf, rio, foreign et haven sont du nombre. Notons également que l’éditeur RStudio facilite l’interaction avec plusieurs de ces packages. 3.3.1.2.2.1 Données structurées Les données structurées sont caractérisées par un modèle de données qui garanti une uniformité entre toutes les données, de façon à ce que la lecture et l’écriture de ces données soient facilitées et accélérées. L’implantation la plus connue de données structurées est probablement les bases de données relationnelles constituées de tables disposées en colonnes et en rangées. L’utilisation de jointures est omniprésente et à la base de ce type de bases de données. Le langage par excellence pour ce type de bases de données est le SQL (Structured Query Language). 3.3.1.2.2.2 Données non-structurées Les données non-structurées sont celles qui ne sont pas organisées à l’aide d’un modèle de données. Une base de données non-structurée peut donc contenir à la fois du texte, des images, du son ou une combinaison (ex. vidéo). R est également équipé pour traiter ce genre de données. Par exemple, pour le traitement d’images, les packages imager, magick et png sont disponibles. 3.3.1.2.3 Service Outre la collecte par fichiers ou par connexion à des bases de données, il est possible de procéder en faisant appel à un service. Un service est accessible via une adresse précise, faisant référence à un port sur un serveur. On y envoie des instructions, et on reçoit une réponse, souvent en XML ou en JSON. En fait, le produit fini du présent ouvrage sera un service déployé sur le web. Voyez par vous-même : r &lt;- httr::GET(&quot;35.203.45.227:8080/bixikwargs?start_date=2017-04-15%2000:48&amp;start_station_code=6079&amp;is_member=1&quot;) httr::content(r) ## $duree ## $duree[[1]] ## $duree[[1]][[1]] ## [1] 620.488 ## ## ## ## $meme_station ## $meme_station[[1]] ## [1] FALSE Des APIs du genre sont souvent utilisées pour rendre des données disponibles aux développeurs. C’est de cette façon que des développeurs peuvent créer des applications web ou en connecter entre elles. Avec votre accord si on utilise vos données bien sûr! Vous êtes-vous déjà demandés comment les développeurs pouvaient bâtir une application aussi facilement par-dessus une autre? Par exemple, comment BIXI peut afficher les disponibilités de ses vélos sur une carte aussi facilement que ça? En suivant ce lien, on apprend que BIXI rend ses données en temps réel sur l’état des stations publiques. La documentation est ici, le statut des stations est ici et les autres endpoints disponibles sont ici. Il suffit alors de connecter ce flux à un autre service ouvert : OpenStreetMap. D’autres services de vélo-partage à travers le monde rendent leurs données publiques. Puisque chaque service est doté d’un modèle de données, des aggrégateurs comme celui-ci sont possibles. Pour la suite de l’atelier, nous discarterons ces source de données car, bien que nous ayions accès aux données en temps réel, nous ne disposons pas de l’historique pour entraîner notre modèle. Bien que la collecte par service soit commode, ce n’est pas la panacée. Les sources sont rarement gratuites, et quand elles le sont elles limitent souvent l’utilisation avec des timeouts ou des limites de requêtes par unité de temps. Notons pour terminer que les packages suivants facilitent la communication avec des serveurs : curl, httr, jsonlite, xml2, XML, downloader 3.3.1.3 Web Scraping Le principal obstacle du web scraping est l’aspect légal. Néanmoins, les packages suivants facilitent la collecte d’information par web scraping : rvest, googlesheets. 3.3.1.4 Accès aux données historiques et en temps réel Nous avons discarté le service de disponibilité des vélos par absence d’historique. Voyons maintenant comment pourrait s’orchestrer l’utilisation de sources différentes en entraînement et en production. Le site weatherstats.ca permet de télécharger un historique des températures à Montréal. Le site OpenWeatherMap.org, lui, permet d’obtenir la température en temps réel à l’aide d’une requête à un service : r &lt;- httr::GET(&quot;http://api.openweathermap.org/data/2.5/weather?q=montreal&amp;APPID=06284235673deed0ce24aeaaa1e8f296&quot;) httr::content(r) ## $coord ## $coord$lon ## [1] -73.61 ## ## $coord$lat ## [1] 45.5 ## ## ## $weather ## $weather[[1]] ## $weather[[1]]$id ## [1] 803 ## ## $weather[[1]]$main ## [1] &quot;Clouds&quot; ## ## $weather[[1]]$description ## [1] &quot;broken clouds&quot; ## ## $weather[[1]]$icon ## [1] &quot;04n&quot; ## ## ## ## $base ## [1] &quot;stations&quot; ## ## $main ## $main$temp ## [1] 284.04 ## ## $main$pressure ## [1] 1014 ## ## $main$humidity ## [1] 37 ## ## $main$temp_min ## [1] 282.04 ## ## $main$temp_max ## [1] 286.15 ## ## ## $visibility ## [1] 24140 ## ## $wind ## $wind$speed ## [1] 4.1 ## ## $wind$deg ## [1] 80 ## ## ## $clouds ## $clouds$all ## [1] 75 ## ## ## $dt ## [1] 1557720128 ## ## $sys ## $sys$type ## [1] 1 ## ## $sys$id ## [1] 943 ## ## $sys$message ## [1] 0.0073 ## ## $sys$country ## [1] &quot;CA&quot; ## ## $sys$sunrise ## [1] 1557739592 ## ## $sys$sunset ## [1] 1557792909 ## ## ## $id ## [1] 6077243 ## ## $name ## [1] &quot;Montreal&quot; ## ## $cod ## [1] 200 Les lectures proviennent-elles des mêmes stations météo? Les distributions ont-elles des biais systématiques, ou des variabilités imprévisibles entre elles? La tâche de réconcilier ces sources de données et de les intégrer au modèle sera laissée en exercice au lecteur. 3.3.1.5 Stabilité Pour que la stabilité des sources de données soit problématique, deux éléments doivent être réunis : - le format des données change - notre fréquence de collecte est trop élevée pour nous donner le temps de réagir Alors que nous n’avons que peu de contrôle sur le premier élément, nous pouvons mitiger le deuxième. Nous pouvons travailler avec une copie des données, de sorte que nous ne serons pas affectés par un changement à la source. Évidemment, en contrepartie on sacrifie la réactivité… l’approche à prendre dépendra donc de la valeur ajoutée d’avoir les données les plus à jour dans le modèle, de notre anticipation de la stabilité ainsi que de l’impact d’une panne du service. Dans notre cas, les limites administratives des quartiers de la Ville de Montréal changent très peu souvent. Nous pouvons donc nous permettre de travailler avec une copie des données figée dans le temps, autant pour l’entraînement que pour la prédiction. Étant donné ces propriétés désirables, nous retenons cette source de données pour la suite. Voyons maintenant comment nous pouvons déterminer le quartier pour chaque observation, à partir des coordonnées des stations et des limites administratives des arrondissements. Premièrement, nous chargeons les polygones , aussi connus sous le nom de shapefiles : extension_list &lt;- c(&quot;.shx&quot;, &quot;.shp&quot;, &quot;.prj&quot;, &quot;.dbf&quot;) AWS_path &lt;- &quot;https://s3.ca-central-1.amazonaws.com/jeremiedb/share/dot-layer/R-Quebec/LIMADMIN/LIMADMIN&quot; if (! all(purrr::map_lgl(extension_list, ~ file.exists(paste0(&quot;data/LIMADMIN&quot;, .x))))){ purrr::walk(extension_list, ~ download.file(url = paste0(AWS_path, .x), destfile = paste0(&quot;data/LIMADMIN&quot;, .x), method = &quot;auto&quot;, mode = &quot;wb&quot;)) } shape_file &lt;- sf::read_sf(dsn=&quot;data/LIMADMIN.shp&quot;) Prenons maintenant la première station, et utilisons le package sf pour déterminer à quel arrondissement elle appartient : point_station_sf &lt;- sf::st_sfc(sf::st_point(c(data_stations[[1, &quot;longitude&quot;]], data_stations[[1, &quot;latitude&quot;]])), crs = 4326) pnts_trans &lt;- sf::st_transform(point_station_sf, 2163) shape_file_trans &lt;- sf::st_transform(shape_file, 2163) vecteur_intersection &lt;- sf::st_intersects(shape_file_trans, pnts_trans, sparse = FALSE) shape_file_trans[which(vecteur_intersection), ]$NOM ## [1] &quot;LaSalle&quot; Nous pourrons ainsi rattacher le prédicteur de l’arrondissement à chacune des observations. 3.3.1.6 sécurité et confidentialité Simple note : comme dans tout projet informatique, nous devons nous assurer des niveaux de sécurité et de confidentialité appropriés pour l’usage. 3.3.2 Qualité Plusieurs aspects peuvent influencer notre perception de qualité d’une source de données, tels que sa complexité, son pouvoir prédictif, la capacité de faire des jointures et le biais. Encore une fois, notons que les étapes futures peuvent invalider une source de données. Par exemple, le pouvoir prédictif est difficile à évaluer sans un premier modèle naïf au moins. 3.3.2.1 complexité Nous pouvons décrire la complexité à utiliser une source de données de 2 façons : - Par la qualité de l’organisation et de la documentation de la source - Par la simplicité à construire une clé de jointure ou des prédicteurs 3.3.2.1.1 organisation et documentation des données Ces 3 sources de données contiennent la même information. Avec laquelle préfériez-vous travailler a priori? dt temps 2019-05-13 00:05:28 0.0208333 Date Durée (minutes) 2019-05-13 00:05:28 30 Date début Date fin 2019-05-13 00:05:28 2019-05-13 00:35:28 Évidemment, nous préférerons, dans l’ordre, 3, 2, 1. Bien que cet exemple soit simpliste et fictif, il illustre tout de même que des éléments tels que le nom, la structure ou la documentation des données influenceront notre capacité à maximiser notre utilisation de ces données. 3.3.2.1.2 construire une clé de jointure ou des prédicteurs Prenons l’exemple des sources contenant la cartographie du réseau cyclable, le nombre de passages quotidiens et la position des arceaux. Dans les 3 cas cela nécessiterait dans un premier temps de sommariser les données géographiques complexes, pour ensuite devoir les rattacher aux données étiquetées par les seules clés de jointure disponibles : les points GPS reliés aux stations. Nous devrions donc grandement compresser de l’information riche à cause de la clé de jointure limitée. 3.3.2.2 pouvoir prédictif Comme l’énonce le fameux dicton : Garbage In, Garbage Out. Autrement dit, si les données que nous fournissons à notre modèle sont de piètre qualité ou peu corrélées avec les étiquettes à prédire, l’apprentissage machine n’aura jamais l’effet d’une baguette magique. Également, notons que nous devons considérer le pouvoir prédictif en termes relatif, et non en termes absolu. En effet, si nous avons déjà des bons proxys pour des prédicteurs, il peut s’avérer futile de vouloir amener ces prédicteurs à tout prix dans nos données d’entraînement, au bénéfice de quelques poussières de précision additionnelle. Par exemple, dans le cas des données géographiques mentionnée dans la sous-section précédente, nous considérons que les limites administratives captureront déjà une bonne partie des effets géographiques concentrés autour des stations. Puisque la clé de jointure aurait été la station de toute façon, nous jugeons donc qu’en plus d’être compliqué à joindre à notre jeu de données, le pouvoir prédictif serait limité. 3.3.2.3 complexité-bénéfice Comme dans n’importe quel projet, nous devrons faire des analyses complexité-bénéfice afin de déterminer à quel point le bénéfice espéré justifie le coût associé à la complexité additionnelle. Revenons aux données géographiques mentionnées aux 2 sous-sections précédentes. Non seulement la complexité nous apparaît élevée, les bénéfices marginaux nous apparaissent également limités. Nous discartons donc ces 3 sources de données. 3.3.2.4 jointure Dans la section approche @, nous avons grandement simplifié le processus de jointure de sources de données. Or, dans la vraie vie, les opérations de jointure peuvent parfois s’avérer périlleuses. En effet, les informations de part et d’autres peuvent être manquantes ou insuffisantes, de sorte que la jointure produira un prédicteur avec une faible qualité ou un faible niveau de confiance. La quantité de jointures à effectuer peut aussi être supérieure à 1, ce qui augmente l’incertitude par rapport à la qualité des données jointes. 3.3.2.4.1 Faible qualité À la section 3.1.3, si au lieu d’avoir sous la main l’heure moyenne de lever et de coucher du soleil dans un mois, plutôt que la donnée exacte pour chacune des journées, nos prédicteurs auraient un pouvoir prédictif plus faible étant donné que l’heure moyenne dans un mois est seulement un proxy pour le vrai prédicteur, qui est l’heure exacte : mois heure lever moy heure coucher moy 2017-05 06:17:00 20:27:00 2017-10 07:25:00 17:50:00 La jointure aurait donné ceci : start_date start_station_code end_date end_station_code duration_sec is_member mois heure lever moy heure coucher moy 2017-04-15 00:00 7060 2017-04-15 00:31 7060 1841 1 2017-04 NA NA 2017-04-15 00:01 6173 2017-04-15 00:10 6173 553 1 2017-04 NA NA 2017-04-15 00:01 6203 2017-04-15 00:04 6204 195 1 2017-04 NA NA 2017-04-15 00:01 6104 2017-04-15 00:06 6114 285 1 2017-04 NA NA 2017-04-15 00:01 6174 2017-04-15 00:11 6174 569 1 2017-04 NA NA Nous constatons que les 2 premières observations ont le même prédicteur, alors que dans les faits les heures de lever et de coucher diffèrent à l’intérieur de chaque mois. Nous utiliserions donc une source de données en jointure, sachant que son pouvoir prédictif n’est pas aussi élevé qu’il pourrait l’être. 3.3.2.4.2 Faible niveau de confiance Un faible niveau de confiance dans la jointure est présent lorsque la clé de jointure comporte de l’incertitude. Contrairement au cas précédent où nous compressions volontairement l’information de la clé de jointure pour joindre des données aggrégées, ce cas-ci se produit lorsque la clé produit des jointures irrégulières ou imprévisibles. Par exemple, si nous avions accès au nom du client dans les données étiquetées, et que nous avions accès à des données de jointure triées par nom, il y aurait certainement des clients pour lesquels la jointure des 2 sources retournerait la mauvaise personne ou plusieurs personnes. 3.3.2.4.3 Plusieurs jointures Supposons un instant que nous disposons d’une source de données avec l’altitude pour toute position GPS (nous reviendrons à la connection à des APIs [API]). Supposons également que nous croyons qu’une station de départ plus élevée produit en général des trajets plus longs. Nous pourrions alors joindre successivement plusieurs sources de données jusqu’à temps que le prédicteur (altitude) soit vis-à-vis l’étiquette. En effet, en joignant les données etiquetées, la position GPS de chaque station et l’altitude par position GPS : start_station_code duration_sec 7060 1841 6173 553 6203 195 6104 285 6174 569 code name latitude longitude 7060 de l’Ã‰glise / de Verdun 45.463 -73.57157 6203 Hutchison / Sherbrooke 45.507 -73.57208 6104 Wolfe / RenÃ©-LÃ©vesque 45.516 -73.55419 6174 Roy / St-Denis 45.519 -73.57270 6173 Berri / Cherrier 45.519 -73.56951 longitude latitude elevation -73.57157 45.463 10 -73.57208 45.507 20 -73.55419 45.516 15 -73.57270 45.519 5 -73.56951 45.519 100 Nous obtiendrions le résultat suivant, avec lequel nous pourrions continuer la modélisation prédictive avec un nouveau prédicteur : l’altitude : start_station_code elevation duration_sec 7060 10 1841 6173 100 553 6203 20 195 6104 15 285 6174 5 569 En soi, la multitude de jointure n’est pas un problème. Or, elle amplifie les problèmes précédents (perte de confiance ou de qualité) en les multipliant entre eux. Nous discartons maintenant l’élévation parce que le rapport complexité-bénéfice nous semble très élevé. 3.3.2.5 Biais Plusieurs biais peuvent se glisser dans la collecte de données. Les plus vicieux sont sans doute les biais socio-démographiques, qui sont présents dès que des données personnelles (ou leurs proxys) sont en jeu, et sont aggravés lorsque les prédictions ont un impact réel sur la vie des gens. Nous devons donc rester vigilents à cette réalité durant la collecte. 3.3.3 Volume Dans le cas des données, on parlera de volume plutôt que de quantité. Nous définissons le volume comme l’espace occupé par un objet sur disque ou en mémoire. Le volume d’une source de données sera donc le produit du nombre d’observations et de la richesse de chaque observation. Si notre source de données contient beaucoup d’observations ou des observations riches, nous aurons des contraintes de rapidité de calcul, d’espace disque ou de mémoire au moment de l’entraînement, et potentiellement même au moment de la prédiction selon l’algorithme choisi. Si nous avons le malheur d’être ensevelis sous des observations en trop grand nombre ou d’une trop grande richesse, plusieurs options s’offrent à nous, qu’elles soient informatiques ou mathématiques/statistiques. Du point de vue informatique, R est parfois réputé lent et capricieux en termes de mémoire vive. Or, il n’en est rien. En effet, il existe plusieurs stratégies simples pour optimiser une expression R : profiler le code, comprendre la gestion de la mémoire ou des interfaces C++ et C. Du point de vue mathématique, il serait étonnant que la quantité d’observations etiquetées soit trop grande. Comme a déjà admis le Scientifique en Chef de Google : «Nous n’avons pas de meilleurs algorithmes que n’importe qui d’autre; nous avons seulement plus de données». Pour contourner les problèmes de mémoire ou d’espace disque, il existe des algorithmes qui taitent les données par batches, c’est-à-dire par tranches de \\(n\\) observations à la fois, où \\(n\\) devra être assez petit pour contourner les contraintes d’espace, mais assez grand pour que l’algorithme puisse généraliser son apprentissage. C’est le cas notamment des algorithmes d’apprentissage profond. Pour les besoins de l’atelier, nous utiliserons seulement les données de 2017 pour entraîner le modèle, mais vous pouvez vous amuser à modéliser avec autant de données que vous le désirez! 3.3.3.1 Aggrégation À la section précédente, nous avons pris la peine de préciser que ce sont les données etiquetées qui ne peuvent pas être trop nombreuses. Par contre, pour toutes les données de jointure, il est possible que les clés de jointure ne soient pas tout à fait alignées avec leur correspondance dans les données etiquetées. Par exemple, si le taux d’échantillonnage des données de jointure n’est pas aussi précis que le permet la clé dans les données etiquetées, nous devrons nous contenter d’une valeur aggrégée, par exemple la moyenne sur une plus longue période. D’un point de vue mathématique, nous préférerons une source la plus granulaire possible, car nous aurions toujours la possibilité de faire l’aggrégation nous-même si nécessaire. Nous évitons ainsi la perte potentielle d’information prédictive pour notre modèle. Par contre, des contraintes de coûts ou d’entreposage pourraient nous forcer à entreposer des données aggrégées. Pour effectuer ces aggrégations, il existe plusieurs écoles de pensées en R. Les packages base, dplyr et data.table sont tous capables d’effectuer la majorité des opérations, mais avec des vitesses et des convivialités variables d’une tâche à l’autre. Plutôt que choisir un camp, nous nous contenterons de vous orienter vers cette discussion, qui dresse les avantages et les désavantages de ces méthodes 3.4 Références ## ## Attaching package: &#39;lubridate&#39; ## The following object is masked from &#39;package:base&#39;: ## ## date ## ## Attaching package: &#39;data.table&#39; ## The following objects are masked from &#39;package:lubridate&#39;: ## ## hour, isoweek, mday, minute, month, quarter, second, wday, ## week, yday, year ## Loading required package: sp ## rgdal: version: 1.4-3, (SVN revision 828) ## Geospatial Data Abstraction Library extensions to R successfully loaded ## Loaded GDAL runtime: GDAL 2.2.3, released 2017/11/20 ## Path to GDAL shared files: C:/Users/Poste/Documents/R/win-library/3.5/rgdal/gdal ## GDAL binary built with GEOS: TRUE ## Loaded PROJ.4 runtime: Rel. 4.9.3, 15 August 2016, [PJ_VERSION: 493] ## Path to PROJ.4 shared files: C:/Users/Poste/Documents/R/win-library/3.5/rgdal/proj ## Linking to sp version: 1.3-1 "],
["exploration.html", "Chapitre 4 Exploration de données 4.1 Distribution des variables d’entrée 4.2 Corrélation entre les variables d’entrée 4.3 Effets one-way 4.4 Effet multi-ways 4.5 Proposition de transformations 4.6 Sommaire des transformations retenues", " Chapitre 4 Exploration de données Une étape souvent sous-estimée par les scientifiques de données amateurs est l’exploration initiale des données disponibles. Bien qu’il soit tout à fait possible de créer un modèle et de compléter le cycle complet de modélisation en négligeant cette étape, la garantie de qualité des résultats en serait alors fortement compromise. L’expression populaire Garbage-in, garbage-out est généralement interprétée comme quoi un modèle prédicitif, même s’il utilise un algorithme à la fine pointe de la technologie, produira des mauvais résultats s’il est entraîné sur des données de mauvaise qualité. L’analyse préliminaire permet d’acquérir des connaissances indispensables pour la suite du processus de modélisation. En effet, tel qu’on le verra dans le prochain chapitre, un élément clé pour obtenir un bon modèle prédictif est la création de nouvelles variables explicatives. Comme celle-ci seront basées sur des transformations des variables du jeu de données brut, il est impératif de bien en connaître les moindres détails. Le but de ce chapitre est donc simple, on veut remplir les deux objectifs expliqués ci-haut : S’approprier le jeu de données sur lequel on travaille; Suggérer des transformations de variables pertinentes pour le prétraitement des données. Structure du chapitre : Distribution de chaque variable d’entrée; Corrélation entre les variables d’entrée; Effet one-way des variables d’entrée sur la variable réponse; Effet multi-ways des variables d’entrée sur la variable réponse (interactions); Propositions de transformations. Prendre note que le but de ce livre n’est pas de présenter une panoplie de solutions pour manipuler des données en R, mais bien d’avoir une vue d’ensemble sur le processus complet de modélisation. Pour des préférences personnelles, la minuplation des données sera effectuée en utilisant une combinaison des packages de base et du package data.table dans ce chapitre. Le lecteur est invité à consulter la documentation de base des packages pour mieux comprendre leur utilisation. Une autre solution populaire est l’utilisation de dplyr qui fait partie de la collection de packages du tidyverse. Avant de commencer, on observe le jeu de données; Échantillon aléatoire pour éviter les biais Premières observations de data : data[sample(.N, 5L)] ## start_date start_station_code end_date end_station_code ## 1: 2017-10-25 6072 2017-10-25 16:09 6748 ## 2: 2017-07-01 7017 2017-07-01 16:10 7019 ## 3: 2017-06-24 6730 2017-06-24 14:14 6358 ## 4: 2017-07-10 6904 2017-07-10 08:49 6190 ## 5: 2017-06-07 7012 2017-06-07 12:27 6928 ## duration_sec is_member start_date_time ## 1: 464 1 2017-10-25 16:01:00 ## 2: 554 0 2017-07-01 16:00:00 ## 3: 1683 0 2017-06-24 13:45:00 ## 4: 938 1 2017-07-10 08:33:00 ## 5: 184 1 2017-06-07 12:24:00 ## start_quartier ## 1: Ville-Marie ## 2: Mercier-Hochelaga-Maisonneuve ## 3: Rosemont-La Petite-Patrie ## 4: Rosemont-La Petite-Patrie ## 5: Côte-des-Neiges-Notre-Dame-de-Grâce Les variables présentes dans le jeu de données sont : start_date : jour du trajet; (ne devrait pas se retrouver ici, transformation de start_date_time) start_station_code : code de la station de départ; end_date : jour et heure de l’arrivée (post-fact, à enlever); end_station_code : dode de la station d’arrivée (post-fact, à enlever); duration_sec : durée du trajet en secondes (variable réponse); is_member : indicateur de membre; start_date_time : jour et heure du départ; start_quartier : quartier dans lequel se trouve la station de départ. (Normalement, la collecte de données devrait devrait ramener l’ensemble des variables (sauf les post-fact), on aurait alors toute l’information qu’on a de besoin pour comprendre nos données. Comme certaines variables ont été enlevées, on explore les jeux de données bruts.) On affiche les premières observations de data_bixi : data_bixi[sample(.N, 5L)] ## start_date start_station_code end_date end_station_code ## 1: 2017-09-24 11:25 6216 2017-09-24 11:36 6094 ## 2: 2017-10-28 08:44 6196 2017-10-28 09:01 6361 ## 3: 2017-05-18 12:22 6043 2017-05-18 12:49 6041 ## 4: 2017-09-28 08:03 6132 2017-09-28 08:04 6132 ## 5: 2017-08-15 12:49 6737 2017-08-15 13:19 6158 ## duration_sec is_member ## 1: 628 0 ## 2: 1070 1 ## 3: 1626 1 ## 4: 86 1 ## 5: 1802 1 Les variables présentes dans le jeu de données sont : start_date : jour et heure du départ; start_station_code : code de la station de départ; end_date : jour et heure de l’arrivée; end_station_code : dode de la station d’arrivée; duration_sec : durée du trajet (en secondes); is_member : indicateur de membre. On affiche ensuite les premières observations de data_stations : data_stations[sample(.N, 5L)] ## code name latitude longitude ## 1: 6912 de Chateaubriand / Beaubien 45.53551 -73.60388 ## 2: 6418 de VendÃ´me / de Maisonneuve 45.47442 -73.60406 ## 3: 6386 MÃ©tro PrÃ©fontaine (Moreau / Hochelaga) 45.54144 -73.55431 ## 4: 6139 des Ã‰rables / Gauthier 45.53241 -73.56478 ## 5: 6072 Metcalfe / de Maisonneuve 45.50171 -73.57413 Les variables présentes dans le jeu de données sont : code : code de la station; name : nom de la station; latitude : latitude de la station longitude: longitude de la station 4.1 Distribution des variables d’entrée Dans la précédente section, on s’est familiarisé avec le format de nos données en regardant un échantillon aléatoire de nos observations. Bien que ce soit le point de départ, on est loin d’avoir suffisament apprivoisé notre jeu de données pour passer à la prochaine phase du cycle, le prétraitement des données. On pourrait toujours augmenter le nombre d’observations aléatoires jusqu’à ce qu’on regarde tout le jeu de données, mais on conviendra que ce n’est pas la solution idéale pour connaître toutes les valeurs que peuvent prendre chacune des variables. La première étape pour agréger des données est de se familiariser avec les distributions marginales de chacun des champs. On doit évidemment traîter différemment les variables selon leur type. Les variables peuvent être de type : Numérique; Catégorique; Temporelle (donc fort probablement cyclique); Spatiale. Il y a différentes manières de procéder, on propose une solution parmi tant d’autres dans le livre. Préférence : plotly (puisque le livre est en HTML) Alternatives : base, ggplot2, lattice 4.1.1 Status membre/non-membre (booléen/catégorique) library(plotly) ## ## Attaching package: &#39;plotly&#39; ## The following object is masked from &#39;package:ggplot2&#39;: ## ## last_plot ## The following object is masked from &#39;package:stats&#39;: ## ## filter ## The following object is masked from &#39;package:graphics&#39;: ## ## layout data_member &lt;- data[, .( nb = .N, mean = mean(duration_sec) ), is_member][ , status:=as.factor(ifelse(is_member, &quot;Membre&quot;, &quot;Non-Membre&quot;)) ] plot_ly( data = data_member ) %&gt;% add_bars( name = &quot;Nombre de trajets&quot;, hoverinfo = &quot;y&quot;, showlegend = FALSE, x = ~status, y = ~nb, xaxis = &quot;x&quot;, yaxis = &quot;y&quot; ) %&gt;% layout( xaxis = list( title = list( text = &quot;Status&quot; ), fixedrange = TRUE ), yaxis = list( title = list( text = &quot;Nombre de trajets&quot; ), fixedrange = TRUE, exponentformat = &quot;none&quot; ) ) 4.1.2 Quartier de la station de départ (catégorique) data_quartiers &lt;- data[, .( nb = .N, mean = mean(duration_sec) ), start_quartier] data_quartiers[is.na(start_quartier), start_quartier:=&quot;Hors MTL&quot;] plot_ly( data = data_quartiers ) %&gt;% add_bars( name = &quot;Nombre de trajets&quot;, hoverinfo = &quot;y&quot;, showlegend = FALSE, x = ~start_quartier, y = ~nb, xaxis = &quot;x&quot;, yaxis = &quot;y&quot; ) %&gt;% layout( xaxis = list( title = list( text = &quot;Status&quot; ), fixedrange = TRUE ), yaxis = list( title = list( text = &quot;Nombre de trajets&quot; ), fixedrange = TRUE, exponentformat = &quot;none&quot; ) ) 4.1.3 Date du trajet (temporelle) Observer l’effet de la tampérature sur la distribution en comparant avec une source externe; Suggère d’ajouter des données externes sur la température; On doit être prudent lors de la modélisation puisque le poids des données est distortionné par la température… # Sommaire par date data_date &lt;- data[, .( nb = .N, mean = mean(duration_sec) ), start_date][order(start_date)] plot_ly( data = data_date ) %&gt;% add_bars( name = &quot;Nombre de trajets&quot;, hoverinfo = &quot;x+y&quot;, showlegend = FALSE, x = ~start_date, y = ~nb, xaxis = &quot;x&quot;, yaxis = &quot;y&quot; ) %&gt;% layout( xaxis = list( title = list( text = &quot;Date du trajet&quot; ), rangeslider = TRUE, fixedrange = TRUE ), yaxis = list( title = list( text = &quot;Nombre de trajets&quot; ), fixedrange = TRUE, exponentformat = &quot;none&quot; ), hovermode = &quot;x&quot; ) 4.1.4 Heure du trajet (temporelle) data_time &lt;- copy(data) data_time[, start_hour:=hour(start_date_time)] data_time &lt;- data_time[, time_min:=strftime(start_date_time, format=&quot;%H:%M&quot;, tz=&quot;UTC&quot;)][, .( nb = .N, mean = mean(duration_sec) ), .(time_min, start_hour)][order(time_min)] plot_ly( data = data_time ) %&gt;% add_bars( name = &quot;Nombre de trajets&quot;, hoverinfo = &quot;x+y&quot;, showlegend = FALSE, x = ~time_min, y = ~nb, xaxis = &quot;x&quot;, yaxis = &quot;y&quot; ) %&gt;% layout( xaxis = list( title = list( text = &quot;Heure du trajet&quot; ), rangeslider = TRUE, fixedrange = TRUE ), yaxis = list( title = list( text = &quot;Nombre de trajets&quot; ), fixedrange = TRUE, exponentformat = &quot;none&quot; ), hovermode = &quot;x&quot; ) 4.1.5 Variables spatiales (stations + quartiers) library(leaflet) library(magrittr) data_quartiers_geo &lt;- readOGR(&quot;data/LIMADMIN.shp&quot;, verbose=FALSE) # Sommaire par station data_stations_rides &lt;- data[, .( code = start_station_code, quartier = start_quartier, nb = .N, mean = mean(duration_sec) ), .(start_station_code, start_quartier)][, `:=`( start_station_code = NULL, start_quartier = NULL )] # Limiter moyenne des temps de trajet limits &lt;- c(650, 1150) data_stations_rides[, capped_mean:=mean] data_stations_rides[mean&lt;limits[1], capped_mean:=limits[1]] data_stations_rides[mean&gt;limits[2], capped_mean:=limits[2]] # Agréger sommaire par station setkey(data_stations_rides, code) setkey(data_stations, code) data_stations[data_stations_rides, `:=`( quartier = quartier, rides_nb = nb, rides_mean = mean, rides_capped_mean = capped_mean )] # Sommaire par quartier data_quartiers_rides &lt;- data_stations[!is.na(quartier), .( nom = quartier, stations_nb = .N, rides_nb = sum(rides_nb), rides_mean = weighted.mean(rides_mean, rides_nb) ), quartier][, quartier:=NULL] # Limiter moyenne des temps de trajet data_quartiers_rides[, rides_capped_mean:=rides_mean] data_quartiers_rides[rides_mean&lt;limits[1], rides_capped_mean:=limits[1]] data_quartiers_rides[rides_mean&gt;limits[2], rides_capped_mean:=limits[2]] # Agréger sommaire par quartier setkey(data_quartiers_rides, nom) #data_quartiers &lt;- readOGR(&quot;data/LIMADMIN.shp&quot;) data_quartiers_geo@data &lt;- as.data.table(data_quartiers_geo@data, keep.rownames=&quot;ID&quot;) data_quartiers_geo@data[, ID:=as.integer(ID)] setkey(data_quartiers_geo@data, NOM) data_quartiers_geo@data[data_quartiers_rides, `:=`( stations_nb = stations_nb, rides_nb = rides_nb, rides_mean = rides_mean, rides_capped_mean = rides_capped_mean )] setorder(data_quartiers_geo@data, ID)[, ID:=NULL] # Création de la palette de couleur des quartiers categorical_pal &lt;- colorFactor( palette = rainbow(data_quartiers_geo@data[!is.na(stations_nb), .N]), domain = data_quartiers_geo@data[!is.na(stations_nb), as.character(NOM)] ) # Création de la carte exposure_map &lt;- leaflet() %&gt;% addTiles() %&gt;% addMarkers( data = data_stations, group = &quot;Stations&quot;, lng = ~longitude, lat = ~latitude, icon = makeIcon( iconUrl = &quot;static-files/bixi-logo.png&quot;, iconWidth = ~8+rides_nb/1500, iconHeight = ~8+rides_nb/1500 ), popup = ~paste( paste0(&quot;&lt;b&gt;&quot;, name, &quot;&lt;/b&gt;&quot;), paste0(&quot;Nombre de trajets : &quot;, format(rides_nb, big.mark=&quot; &quot;)), sep = &quot;&lt;br/&gt;&quot; ), label = ~name ) %&gt;% addPolygons( data = data_quartiers_geo, group = &quot;Quartiers&quot;, color = &quot;black&quot;, weight = 2, fillColor = ~categorical_pal(NOM), fillOpacity = 0.35, dashArray = &quot;2 4&quot;, popup = ~paste( paste0(&quot;&lt;b&gt;&quot;, NOM, &quot;&lt;/b&gt;&quot;), paste0(&quot;Nombre de stations : &quot;, format(stations_nb, big.mark=&quot; &quot;)), paste0(&quot;Nombre de trajets : &quot;, format(rides_nb, big.mark=&quot; &quot;)), sep = &quot;&lt;br/&gt;&quot; ), highlightOptions = highlightOptions( weight = 3, opacity = 1, dashArray = FALSE ) ) %&gt;% addLegend( data = data_quartiers_geo@data[!is.na(stations_nb)], position = &quot;bottomright&quot;, pal = categorical_pal, values = ~NOM, opacity = 1, labFormat = labelFormat( transform = function(values){lapply(values, function(value){ data_quartiers_geo@data[NOM==value, as.character(ABREV)] })} ), title = &quot;Quartiers&quot;, group = &quot;Quartiers&quot; ) %&gt;% addLayersControl( overlayGroups = c(&quot;Stations&quot;, &quot;Quartiers&quot;), options = layersControlOptions( collapsed = FALSE ) ) %&gt;% hideGroup(&quot;Quartiers&quot;) # Affichage de la carte exposure_map 4.2 Corrélation entre les variables d’entrée 4.2.1 Status (catégorique) vs Quartier de la station de départ (catégorique) data_quartiers_member &lt;- data[, .( nb = .N, mean = mean(duration_sec) ), .(is_member, start_quartier)] data_quartiers_member[is.na(start_quartier), start_quartier:=&quot;Hors MTL&quot;] data_quartiers_member[, status:=as.factor(ifelse(is_member, &quot;Membre&quot;, &quot;Non-Membre&quot;))] plot_ly( data = data_quartiers_member ) %&gt;% add_bars( name = &quot;Membres&quot;, hoverinfo = &quot;y&quot;, showlegend = TRUE, x = ~start_quartier[status==&quot;Membre&quot;], y = ~nb[status==&quot;Membre&quot;], colors = &quot;#1f77b4&quot;, xaxis = &quot;x&quot;, yaxis = &quot;y&quot; ) %&gt;% add_bars( name = &quot;Non-membres&quot;, hoverinfo = &quot;y&quot;, showlegend = TRUE, x = ~start_quartier[status==&quot;Non-Membre&quot;], y = ~nb[status==&quot;Non-Membre&quot;], colors = &quot;#ff7f0e&quot; ) %&gt;% layout( xaxis = list( title = list( text = &quot;Quartier&quot; ), fixedrange = TRUE ), yaxis = list( title = list( text = &quot;Nombre de trajets&quot; ), fixedrange = TRUE, exponentformat = &quot;none&quot; ), legend = list( y = 0 ) ) 4.3 Effets one-way 4.3.1 Status membre/non-membre (booléen/catégoriqe) data_member_quantiles &lt;- data[, .(quantiles=quantile(duration_sec, seq(0, 1, 0.25))), is_member] data_member_quantiles[, status:=as.factor(ifelse(is_member, &quot;Membre&quot;, &quot;Non-Membre&quot;))] plot_ly( data = data_member ) %&gt;% add_bars( name = &quot;Nombre de trajets&quot;, hoverinfo = &quot;y&quot;, showlegend = FALSE, x = ~status, y = ~nb, xaxis = &quot;x&quot;, yaxis = &quot;y2&quot; ) %&gt;% add_trace( data = data_member_quantiles, type = &quot;box&quot;, name = &quot;Quantiles&quot;, hoverinfo = &quot;y&quot;, boxpoints = FALSE, x = ~status, y = ~quantiles, xaxis = &quot;x&quot;, yaxis = &quot;y&quot; ) %&gt;% add_trace( data = data_member, type = &quot;scatter&quot;, mode = &quot;markers&quot;, name = &quot;Moyenne&quot;, hoverinfo = &quot;y&quot;, x = ~status, y = ~mean, xaxis = &quot;x&quot;, yaxis = &quot;y&quot; ) %&gt;% layout( xaxis = list( title = list( text = &quot;Status&quot; ), fixedrange = TRUE ), yaxis = list( title = list( text = &quot;Durée moyenne (sec)&quot; ), side = &quot;left&quot;, fixedrange = TRUE, exponentformat = &quot;none&quot;, range = list(-1000, 3000) ), yaxis2 = list( title = list( text = &quot;Nombre de trajets&quot; ), overlaying = &quot;y&quot;, side = &quot;right&quot;, showgrid = FALSE, fixedrange = TRUE, exponentformat = &quot;none&quot;, range = ~list(0, 4*max(nb)) ) ) 4.3.2 Quartier de la station de départ (catégorique) data_quartiers_quantiles &lt;- data[, .(quantiles=quantile(duration_sec, seq(0, 1, 0.25))), start_quartier] data_quartiers_quantiles[is.na(start_quartier), start_quartier:=&quot;Hors MTL&quot;] plot_ly( data = data_quartiers ) %&gt;% add_bars( name = &quot;Nombre de trajets&quot;, hoverinfo = &quot;y&quot;, showlegend = FALSE, x = ~start_quartier, y = ~nb, xaxis = &quot;x&quot;, yaxis = &quot;y2&quot; ) %&gt;% add_trace( data = data_quartiers_quantiles, type = &quot;box&quot;, name = &quot;Quantiles&quot;, hoverinfo = &quot;y&quot;, boxpoints = FALSE, x = ~start_quartier, y = ~quantiles, xaxis = &quot;x&quot;, yaxis = &quot;y&quot; ) %&gt;% add_trace( data = data_quartiers, type = &quot;scatter&quot;, mode = &quot;markers&quot;, name = &quot;Moyenne&quot;, hoverinfo = &quot;y&quot;, x = ~start_quartier, y = ~mean, xaxis = &quot;x&quot;, yaxis = &quot;y&quot; ) %&gt;% layout( xaxis = list( title = list( text = &quot;Status&quot; ), fixedrange = TRUE ), yaxis = list( title = list( text = &quot;Durée moyenne (sec)&quot; ), side = &quot;left&quot;, fixedrange = TRUE, exponentformat = &quot;none&quot;, range = list(-1000, 3000) ), yaxis2 = list( title = list( text = &quot;Nombre de trajets&quot; ), overlaying = &quot;y&quot;, side = &quot;right&quot;, showgrid = FALSE, fixedrange = TRUE, exponentformat = &quot;none&quot;, range = ~list(0, 4*max(nb)) ) ) 4.3.3 Date du trajet (temporelle) Observer l’effet de la tampérature sur la variable réponse en comparant avec une source externe; Suggère d’ajouter des transformations basées sur la température; TO DO : Déplacer la transformation semaine/fds dans la section Transformations. data_date[, weekend:=lubridate::wday(start_date, week_start = 7)%in%c(1, 7)] plot_ly( data = data_date ) %&gt;% add_bars( name = &quot;Nombre de trajets&quot;, hoverinfo = &quot;x+y&quot;, showlegend = FALSE, x = ~start_date, y = ~nb, xaxis = &quot;x&quot;, yaxis = &quot;y2&quot; ) %&gt;% add_trace( type = &quot;scatter&quot;, mode = &quot;lines&quot;, name = &quot;Durée moyenne&quot;, hoverinfo = &quot;y&quot;, x = ~start_date, y = ~mean, xaxis = &quot;x&quot;, yaxis = &quot;y&quot; ) %&gt;% add_trace( type = &quot;scatter&quot;, mode = &quot;lines&quot;, visible = &quot;legendonly&quot;, name = &quot;Fin de semaine&quot;, hoverinfo = &quot;y&quot;, legendgroup = &quot;weekend&quot;, x = ~start_date[weekend==TRUE], y = ~mean[weekend==TRUE], xaxis = &quot;x&quot;, yaxis = &quot;y&quot; ) %&gt;% add_trace( type = &quot;scatter&quot;, mode = &quot;lines&quot;, visible = &quot;legendonly&quot;, name = &quot;Semaine&quot;, hoverinfo = &quot;y&quot;, legendgroup = &quot;weekend&quot;, x = ~start_date[weekend==FALSE], y = ~mean[weekend==FALSE], xaxis = &quot;x&quot;, yaxis = &quot;y&quot; ) %&gt;% layout( xaxis = list( title = list( text = &quot;Date du trajet&quot; ), rangeslider = TRUE, fixedrange = TRUE ), yaxis = list( title = list( text = &quot;Durée moyenne (sec)&quot; ), side = &quot;left&quot;, fixedrange = TRUE, exponentformat = &quot;none&quot; ), yaxis2 = list( title = list( text = &quot;Nombre de trajets&quot; ), overlaying = &quot;y&quot;, side = &quot;right&quot;, showgrid = FALSE, fixedrange = TRUE, exponentformat = &quot;none&quot;, range = ~list(0, 4*max(nb)) ), hovermode = &quot;x&quot;, legend = list( y = -0.3 ) ) 4.3.4 Heure du trajet (temporelle) TO DO : Bouger la partie sur les moments de la journée dans la section Transformations. data_time[, moment_journee := &quot;nuit&quot;] data_time[start_hour &gt;= 6 &amp; start_hour &lt; 11, moment_journee := &quot;matin&quot;] data_time[start_hour &gt;= 11 &amp; start_hour &lt; 16, moment_journee := &quot;journee&quot;] data_time[start_hour &gt;= 16 &amp; start_hour &lt; 23, moment_journee := &quot;soir&quot;] data_time[, moment_journee:=as.factor(moment_journee)] plot_ly( data = data_time ) %&gt;% add_bars( name = &quot;Nombre de trajets&quot;, hoverinfo = &quot;x+y&quot;, showlegend = FALSE, x = ~time_min, y = ~nb, xaxis = &quot;x&quot;, yaxis = &quot;y2&quot; ) %&gt;% add_trace( type = &quot;scatter&quot;, mode = &quot;lines&quot;, name = &quot;Durée moyenne&quot;, hoverinfo = &quot;y&quot;, showlegend = FALSE, x = ~time_min, y = ~mean, xaxis = &quot;x&quot;, yaxis = &quot;y&quot; ) %&gt;% add_trace( type = &quot;scatter&quot;, mode = &quot;lines&quot;, visible = &quot;legendonly&quot;, name = &quot;Matin&quot;, hoverinfo = &quot;none&quot;, legendgroup = &quot;moment&quot;, x = ~time_min, y = ~ifelse(moment_journee==&quot;matin&quot;, mean, NA), xaxis = &quot;x&quot;, yaxis = &quot;y&quot; ) %&gt;% add_trace( type = &quot;scatter&quot;, mode = &quot;lines&quot;, visible = &quot;legendonly&quot;, name = &quot;Journée&quot;, hoverinfo = &quot;none&quot;, legendgroup = &quot;moment&quot;, x = ~time_min, y = ~ifelse(moment_journee==&quot;journee&quot;, mean, NA), xaxis = &quot;x&quot;, yaxis = &quot;y&quot; ) %&gt;% add_trace( type = &quot;scatter&quot;, mode = &quot;lines&quot;, visible = &quot;legendonly&quot;, name = &quot;Soir&quot;, hoverinfo = &quot;none&quot;, legendgroup = &quot;moment&quot;, x = ~time_min, y = ~ifelse(moment_journee==&quot;soir&quot;, mean, NA), xaxis = &quot;x&quot;, yaxis = &quot;y&quot; ) %&gt;% add_trace( type = &quot;scatter&quot;, mode = &quot;lines&quot;, visible = &quot;legendonly&quot;, name = &quot;Nuit&quot;, hoverinfo = &quot;none&quot;, legendgroup = &quot;moment&quot;, x = ~time_min, y = ~ifelse(moment_journee==&quot;nuit&quot;, mean, NA), xaxis = &quot;x&quot;, yaxis = &quot;y&quot; ) %&gt;% layout( xaxis = list( title = list( text = &quot;Heure du trajet&quot; ), rangeslider = TRUE, fixedrange = TRUE ), yaxis = list( title = list( text = &quot;Durée moyenne (sec)&quot; ), side = &quot;left&quot;, fixedrange = TRUE, exponentformat = &quot;none&quot; ), yaxis2 = list( title = list( text = &quot;Nombre de trajets&quot; ), overlaying = &quot;y&quot;, side = &quot;right&quot;, showgrid = FALSE, fixedrange = TRUE, exponentformat = &quot;none&quot;, range = ~list(0, 4*max(nb)) ), hovermode = &quot;x&quot;, legend = list( y = -0.4 ) ) 4.3.5 Variables spatiales (stations + quartiers) # Création de la palette de couleur pour les one way oneway_pal &lt;- colorNumeric( palette = &quot;YlGnBu&quot;, domain = limits ) # Création de la carte oneway_map &lt;- leaflet() %&gt;% addTiles() %&gt;% addCircleMarkers( data = data_stations, group = &quot;Stations&quot;, lng = ~longitude, lat = ~latitude, radius = ~4+rides_nb/3000, color = &quot;black&quot;, weight = 1, fillColor = ~oneway_pal(rides_capped_mean), fillOpacity = 0.5, popup = ~paste( paste0(&quot;&lt;b&gt;&quot;, name, &quot;&lt;/b&gt;&quot;), paste0(&quot;Nombre de trajets : &quot;, format(rides_nb, big.mark=&quot; &quot;)), paste0(&quot;Durée moyenne : &quot;, format(rides_mean, digits=0, big.mark=&quot; &quot;), &quot; sec&quot;), sep = &quot;&lt;br/&gt;&quot; ), label = ~name ) %&gt;% addPolygons( data = data_quartiers_geo, group = &quot;Quartiers&quot;, color = &quot;black&quot;, weight = 2, fillColor = ~oneway_pal(rides_capped_mean), fillOpacity = 0.5, dashArray = &quot;2 4&quot;, popup = ~paste( paste0(&quot;&lt;b&gt;&quot;, NOM, &quot;&lt;/b&gt;&quot;), paste0(&quot;Nombre de stations : &quot;, format(stations_nb, big.mark=&quot; &quot;)), paste0(&quot;Nombre de trajets : &quot;, format(rides_nb, big.mark=&quot; &quot;)), paste0(&quot;Durée moyenne : &quot;, format(rides_mean, digits=0, big.mark=&quot; &quot;), &quot; sec&quot;), sep = &quot;&lt;br/&gt;&quot; ), highlightOptions = highlightOptions( weight = 3, opacity = 1, dashArray = FALSE ) ) %&gt;% addLegend( data = data_stations, position = &quot;bottomright&quot;, pal = oneway_pal, values = ~rides_capped_mean, opacity = 1, labFormat = labelFormat( suffix = &quot; sec&quot;, big.mark = &quot; &quot; ), title = &quot;Durée moyenne&quot; ) %&gt;% addLayersControl( overlayGroups = c(&quot;Stations&quot;, &quot;Quartiers&quot;), options = layersControlOptions( collapsed = FALSE ) ) %&gt;% hideGroup(&quot;Quartiers&quot;) # Affichage de la carte oneway_map 4.4 Effet multi-ways 4.4.1 Status (catégorique) vs Quartier de la station de départ (catégorique) data_quartiers_member_quantiles &lt;- data[, .(quantiles=quantile(duration_sec, seq(0, 1, 0.25))), .(is_member, start_quartier)] data_quartiers_member_quantiles[is.na(start_quartier), start_quartier:=&quot;Hors MTL&quot;] data_quartiers_member_quantiles[, status:=as.factor(ifelse(is_member, &quot;Membre&quot;, &quot;Non-Membre&quot;))] plot_ly( data = data_quartiers_member ) %&gt;% add_bars( name = &quot;Membres&quot;, hoverinfo = &quot;y&quot;, showlegend = TRUE, legendgroup = &quot;Membre&quot;, x = ~start_quartier[status==&quot;Membre&quot;], y = ~nb[status==&quot;Membre&quot;], colors = &quot;#1f77b4&quot;, xaxis = &quot;x&quot;, yaxis = &quot;y2&quot; ) %&gt;% add_bars( name = &quot;Non-membres&quot;, hoverinfo = &quot;y&quot;, showlegend = TRUE, legendgroup = &quot;Non-Membre&quot;, x = ~start_quartier[status==&quot;Non-Membre&quot;], y = ~nb[status==&quot;Non-Membre&quot;], colors = &quot;#ff7f0e&quot;, xaxis = &quot;x&quot;, yaxis = &quot;y2&quot; ) %&gt;% add_trace( data = data_quartiers_member_quantiles, type = &quot;box&quot;, name = &quot;Quantiles&quot;, hoverinfo = &quot;y&quot;, showlegend = FALSE, legendgroup = &quot;Membre&quot;, boxpoints = FALSE, x = ~start_quartier[status==&quot;Membre&quot;], y = ~quantiles[status==&quot;Membre&quot;], line = list( color = &quot;#1f77b4&quot; ), fillcolor = &quot;#7cbce9&quot;, xaxis = &quot;x&quot;, yaxis = &quot;y&quot; ) %&gt;% add_trace( data = data_quartiers_member_quantiles, type = &quot;box&quot;, name = &quot;Quantiles&quot;, hoverinfo = &quot;y&quot;, showlegend = FALSE, legendgroup = &quot;Non-Membre&quot;, boxpoints = FALSE, x = ~start_quartier[status==&quot;Non-Membre&quot;], y = ~quantiles[status==&quot;Non-Membre&quot;], line = list( color = &quot;#ff7f0e&quot; ), fillcolor = &quot;#ffbb80&quot;, xaxis = &quot;x&quot;, yaxis = &quot;y&quot; ) %&gt;% add_trace( data = data_quartiers_member, type = &quot;scatter&quot;, mode = &quot;markers&quot;, name = &quot;Moyenne&quot;, hoverinfo = &quot;y&quot;, showlegend = FALSE, legendgroup = &quot;Membre&quot;, x = ~start_quartier[status==&quot;Membre&quot;], y = ~mean[status==&quot;Membre&quot;], marker = list( color = &quot;#1f77b4&quot; ), xaxis = &quot;x&quot;, yaxis = &quot;y&quot; ) %&gt;% add_trace( data = data_quartiers_member, type = &quot;scatter&quot;, mode = &quot;markers&quot;, name = &quot;Moyenne&quot;, hoverinfo = &quot;y&quot;, showlegend = FALSE, legendgroup = &quot;Non-Membre&quot;, x = ~start_quartier[status==&quot;Non-Membre&quot;], y = ~mean[status==&quot;Non-Membre&quot;], marker = list( color = &quot;#ff7f0e&quot; ), xaxis = &quot;x&quot;, yaxis = &quot;y&quot; ) %&gt;% layout( xaxis = list( title = list( text = &quot;Quartier&quot; ), fixedrange = TRUE ), yaxis = list( title = list( text = &quot;Durée moyenne (sec)&quot; ), side = &quot;left&quot;, fixedrange = TRUE, exponentformat = &quot;none&quot;, range = list(-1000, 3500) ), yaxis2 = list( title = list( text = &quot;Nombre de trajets&quot; ), overlaying = &quot;y&quot;, side = &quot;right&quot;, showgrid = FALSE, fixedrange = TRUE, exponentformat = &quot;none&quot;, range = ~list(0, 5*max(nb)) ), boxmode = &quot;group&quot;, legend = list( y = 0 ) ) 4.5 Proposition de transformations 4.5.1 Moment de la journée 4.5.1.1 Distribution data_moment &lt;- copy(data) data_moment[, start_hour:=hour(start_date_time)] data_moment[, moment_journee := &quot;Nuit&quot;] data_moment[start_hour &gt;= 6 &amp; start_hour &lt; 11, moment_journee := &quot;Matin&quot;] data_moment[start_hour &gt;= 11 &amp; start_hour &lt; 16, moment_journee := &quot;Journée&quot;] data_moment[start_hour &gt;= 16 &amp; start_hour &lt; 23, moment_journee := &quot;Soir&quot;] data_moment[, moment_journee:=factor(moment_journee, c(&quot;Matin&quot;, &quot;Journée&quot;, &quot;Soir&quot;, &quot;Nuit&quot;))] data_moment_quantiles &lt;- data_moment[, .(quantiles=quantile(duration_sec, seq(0, 1, 0.25))), moment_journee] data_moment &lt;- data_moment[, .( nb = .N, mean = mean(duration_sec) ), moment_journee] plot_ly( data = data_moment ) %&gt;% add_bars( name = &quot;Nombre de trajets&quot;, hoverinfo = &quot;y&quot;, showlegend = FALSE, x = ~moment_journee, y = ~nb ) %&gt;% layout( xaxis = list( title = list( text = &quot;Status&quot; ), fixedrange = TRUE ), yaxis = list( title = list( text = &quot;Nombre de trajets&quot; ), fixedrange = TRUE, exponentformat = &quot;none&quot; ) ) 4.5.1.2 Corrélation avec les autres variables TO DO 4.5.1.3 Effet one-way plot_ly( data = data_moment ) %&gt;% add_bars( name = &quot;Nombre de trajets&quot;, hoverinfo = &quot;y&quot;, showlegend = FALSE, x = ~moment_journee, y = ~nb, xaxis = &quot;x&quot;, yaxis = &quot;y2&quot; ) %&gt;% add_trace( data = data_moment_quantiles, type = &quot;box&quot;, name = &quot;Quantiles&quot;, hoverinfo = &quot;y&quot;, boxpoints = FALSE, x = ~moment_journee, y = ~quantiles, xaxis = &quot;x&quot;, yaxis = &quot;y&quot; ) %&gt;% add_trace( data = data_moment, type = &quot;scatter&quot;, mode = &quot;markers&quot;, name = &quot;Moyenne&quot;, hoverinfo = &quot;y&quot;, x = ~moment_journee, y = ~mean, xaxis = &quot;x&quot;, yaxis = &quot;y&quot; ) %&gt;% layout( xaxis = list( title = list( text = &quot;Status&quot; ), fixedrange = TRUE ), yaxis = list( title = list( text = &quot;Durée moyenne (sec)&quot; ), side = &quot;left&quot;, fixedrange = TRUE, exponentformat = &quot;none&quot;, range = list(-1000, 3000) ), yaxis2 = list( title = list( text = &quot;Nombre de trajets&quot; ), overlaying = &quot;y&quot;, side = &quot;right&quot;, showgrid = FALSE, fixedrange = TRUE, exponentformat = &quot;none&quot;, range = ~list(0, 4*max(nb)) ) ) 4.5.1.4 Effets multi-ways TO DO 4.6 Sommaire des transformations retenues Moment de la journée Matin : 6h à 11h Journée : 11h à 16h Soir : 16h à 23h Nuit : 23h à 6h Semaine/Fin de semaine Semaine : Lundi, Mardi, Mercredi, Jeudi, Vendredi Fin de semaine : Samedi, Dimanche Regroupement des quartiers Groupe 1 : Plateau-Mont-Royal; Groupe 2 : Ville-Marie; Groupe 3 : Ahuntsic-Cartierville, Villeray-Saint-Michel-Parc-Extension, Rosemont-La Petite-Patrie, Mercier-Hochelaga-Maisonneuve; Groupe 4 : Outremont, Côte-des-Neiges-Notre-Dame-de-Grâce, Westmount, Le Sud-Ouest, Verdun, LaSalle; Groupe 5 : Autre. 4.6.1 Données externes à explorer Météo FSAs au lieu de quartiers Réseau de métro Jours fériés Dates de festivals 4.6.2 Transformations à explorer (sur données déjà disponibles) Meilleur regroupement des moments de la journée Distance avec les autres quartiers Distance avec le fleuve (~bordures de la carte des quartiers) "],
["preprop.html", "Chapitre 5 Prétraitements des données 5.1 Nettoyage de données 5.2 Réduction de données 5.3 Transformation de données 5.4 Prétraitements de données en R 5.5 Conclusion 5.6 Références", " Chapitre 5 Prétraitements des données Pour faire de la modélisation prédictive, il est généralement inévitable de faire un prétraitement aux données pour exploiter autant que possible les différents algorithmes d’apprentissage. Ce prétraitement permettra de réaliser 2 choses: Transformer les données dans un format compatible pour l’algorithme Transformer les données de manière à faciliter l’apprentissage À ce stade-ci, nous avons acquis une certaine connaissance de ces données grâce à l’analyse de données 4. Dans cette section, nous serons donc en mesure de produire des données propices à l’apprentissage qui sera faite dans la section 6. Pour se faire, nous allons couvrir certains concepts théoriques derrière le nettoyage, la réduction et la transformation de données. Ensuite, nous allons voir comment appliquer ces concepts théoriques dans le processus de modélisation en tirant profit du langage R et de certains packages. 5.1 Nettoyage de données La première étape du prétraitement des données consiste à les nettoyer. Après cette étape, l’algorithme devrait au minimum être en mesure de fonctionner et de faire un apprentissage de base. Le nettoyage de données comprend plusieurs traitements propres à chaque jeu de données. Voici ceux que nous avons identifiés et qui sont assez générales en pratique : Imputation de données manquantes Traitement des données aberrantes Encodage des données catégoriques 5.1.1 Imputation de données manquantes La majorité des jeux de données rencontrés en pratique contient des données manquantes. Certains algorithmes ne sont pas en mesure de gérer les données manquantes par eux-mêmes. Dans certains cas, simplement effacer du jeu de données les observations contenant des données manquantes peut avoir un impact négatif sur la performance des algorithmes entraînés. Une alternative est d’imputer les données manquantes. Les causes d’absence de données peuvent être relativement variées: bris de système, abstention, perte de données, etc. Il est généralement difficile de connaître la raison exacte, ce qui rend le traitement de données manquantes une étape importante. Avant même de trouver une méthode pour imputer ces données manquantes, il faut d’abord évaluer la quantité de données manquantes et tenter de comprendre le mécanisme expliquant la non-réponse d’une donnée. Il existe essentiellement 3 types de mécanismes de non-réponse (Charest 2018b): Données manquantes complètement au hasard (MCAR) Données manquantes au hasard (MAR) Données manquantes pas au hasard (NMAR) Dans le premier cas (MCAR), l’absence d’une donnée ne dépend pas de sa vraie valeur (non-observée). Par exemple, un système est défaillant dans la collecte de données et arrête de fonctionner en moyenne 5% du temps, et ce, de manière complètement aléatoire. Dans le deuxième cas (MAR), l’absence d’une donnée dépend uniquement de la valeur des variables qui ont été observées. Par exemple, des données ont été récoltées à partir de 2 systèmes différents et il est toujours connu de quel système les données proviennent. Pour un des deux systèmes, les données ont une plus grande probabilité d’être manquantes. Dans le troisième cas (NMAR), l’absence d’une donnée dépend également des données manquantes. La probabilité de non-réponse peut dépendre de la variable elle-même ou d’une autre variable. Par exemple, c’est le cas si les personnes ayant un salaire faible ont une probabilité plus faible de répondre à la question d’un sondage en lien avec le salaire gagné. Il existe différentes manières d’émettre une hypothèse quant au mécanisme de non-réponse. Certains tests1 statistiques existent pour identifier un cas de MCAR. Cependant, il est difficile de différencier statistiquement MAR et NMAR. Pour se faire, on peut entres autres analyser le comportement des autres variables en fonction de l’absence ou la présence d’une donnée. Une connaissance du domaine d’affaire peut également être utile pour évaluer le mécanisme de non-réponse. Par la suite, il est possible de faire l’imputation selon différentes méthodes: Analyse des cas complets: Conserver uniquement les observations pour lesquelles toutes les variables sont présentes. Cette méthode est très simple, mais nécessite MCAR, sinon peut introduire un biais notable dans les estimateurs. Imputation par une mesure de centralité: Utiliser la moyenne, la médiane ou le mode pour remplacer les données manquantes. Cette méthode est relativement simple, mais peu recommandée pour des variables ayant un pourcentage élevé de données manquantes (même pour MCAR). Cela diminue la variabilité et la corrélation entre les variables. Imputation par régression: Remplacer les données manquantes par la prévision de modèle de régression entraîné sur les observations pour lesquelles cette variable est présente. On sur-estimera la corrélation entre les variables et diminuera la variance des variables (moins que par l’imputation par mesure de centralité). Imputation par régression stochastique: Même chose que la méthode par régression, mais on ajoute un résidu aléatoire à la prévision. Cela permet d’augmenter la variance. En bref, il est important d’avoir une bonne compréhension des données manquantes dans le jeu de données. Cela nous permet de prendre des décisions éclairées sur les techniques utilisées pour imputer ces valeurs manquantes. 5.1.2 Traitement des données aberrantes Une autre étape classique du nettoyage de données consiste à faire le traitement des données aberrantes. Ces données peuvent parfois avoir des effets importants sur l’estimation des paramètres du modèle. La première étape est d’abord d’identifier ces données et d’ensuite de les traiter. Voici quelques méthodes traditionnelles pour faire la détection de celles-ci à l’intérieur d’une distribution quelconque: \\(\\pm\\) 3 écarts-types de la moyenne \\(\\pm\\) 1.5 EI (écarts interquartile) Partitionnement (clustering) Une fois que les observations aberrantes ont été identifiées, il faut par la suite les traiter. Un expert du domaine peut être utile dans ce cas-ci pour analyser ces observations et confirmer leur validité. Si la validité d’une donnée est remise en question, il est coutume de retirer ces observations du jeu de données, car cela peut avoir un impact notoire sur le modèle. Il est important de bien documenter le retrait de ces observations. 5.1.3 Encodage des données catégoriques Plusieurs algorithmes d’apprentissage ne peuvent pas traiter des données catégoriques directement. Il faut donc généralement encoder ces attributs en variables numériques pour pouvoir les utiliser dans le modèle. Pour se faire, il faut d’abord faire la distinction entre une donnée de nominale et une donnée ordinale. Dans le premier cas, il n’y a pas vraiment de notion d’ordre entre les différentes catégories. Dans le deuxième cas, il existe un ordonnancement naturel des catégories. Voici un exemple de donnée ordinale (condition_generale) où il est possible d’ordonner les observations: ## Id condition_station ## 1: 1 moyen ## 2: 2 mauvais ## 3: 3 excellent ## 4: 4 bon À l’inverse, voici un exemple où il est moins adéquat d’ordonner les observations selon la variable quartier: ## Id start_quartier ## 1: 1 Ville-Marie ## 2: 2 Verdun ## 3: 3 Westmount ## 4: 4 LaSalle Dans le cas d’une variable ordinale, une méthode d’encodage possible et adéquate (selon le contexte) serait d’assigner une valeur numérique pour chaque catégorie: ## Id condition_station ## 1: 1 1 ## 2: 2 0 ## 3: 3 3 ## 4: 4 2 Dans l’exemple ci-dessus, il faut être prudent, car en encodant la variable de cette manière, on spécifie au modèle que la distance entre 'mauvais' et 'moyen' est la même qu’entre 'bon' et 'excellent'. Cette hypothèse n’est pas toujours valide selon le contexte. Les avantages avec cette méthode sont qu’elle est relativement simple, intuitive et qu’elle n’augmente pas le nombre de variables. Dans le cas d’une variable nominale, il faut généralement opter pour une autre stratégie. Une méthode classique d’encodage dans ce cas-ci est d’utiliser la méthode un-chaud2 (one-hot encoding). Cette méthode consiste à traiter chaque catégorie comme une variable indicatrice qui indique la présence de la catégorie: ## Id quartier_centre.ville quartier_plateau.mont.royal quartier_verdun ## 1: 1 1 0 0 ## 2: 2 0 1 0 ## 3: 3 0 0 1 ## 4: 4 0 0 0 ## quartier_rosemont ## 1: 0 ## 2: 0 ## 3: 0 ## 4: 1 Le principal inconvénient avec la méthode d’encodage un-chaud est qu’elle augmente le nombre de variables. Cela peut être significatif pour une variable ayant plusieurs catégories différentes. C’est d’ailleurs dans ce genre de situations qu’il peut devenir intéressant de regrouper certaines classes entres elles, ce que nous allons voir à la section 5.3. Il existe d’autres méthode comme l’encodage binaire ou le hasing, mais les deux méthodes présentés plus en détails sont généralement les méthodes les plus utilisées en pratique. 5.2 Réduction de données À la section 5.1.3, nous avons vu que certaines méthodes d’encodages peuvent augmenter le nombres de variables. Dans le même ordre d’idées, certains domaines (par exemple la génétique) sont propices à avoir beaucoup d’attributs et ainsi avoir des espaces de données très complexe. Lorsque cela survient, les algorithmes d’apprentissage peuvent souffrir d’un phénomène appelé le fléau de la dimensionnalité. Cela survient lorsque le nombre de dimensions est trop grand, ce qui crée une “distance” plus importante entre les certaines données et rend plus difficile la tâche d’apprentissage. Pour illustrer ce concept, considérons l’exemple suivant: On suppose qu’on a un jeu de données avec \\(p=1\\) attribut de \\(n\\) observations où \\(x_1,...,x_n \\stackrel{iid}{\\sim} U(0,1)\\). Combien d’observations en moyenne se trouveront dans l’intervalle \\([0;0.1]\\)? La réponse : \\(\\frac{n}{10}\\) observations. Maintenant, supposons que notre jeu de données est plus complexe et possède \\(p=10\\) attributs au lieu d’un seul attribut. Les observations suivent toujours une loi uniforme où \\(x_1,...,x_n \\stackrel{iid}{\\sim} U([0,1]^{10})\\). Combien d’observations en moyenne se trouveront dans l’intervalle \\([0;0.1]^{10}\\)? La réponse : \\(n(\\frac{1}{10})^{10}\\) observations. En d’autres mots, cet exemple (Charest 2018a) permet d’illustrer le fait que plus la complexité de l’espace des données est importante, plus on doit couvrir un étendu important pour capturer la même proportion d’observations. 5.2.1 Analyse en composantes principales Une méthode traditionnelle et utilisée en pratique pour réduire la dimensionnalité est l’analyse en composantes principales (Hotelling 1933). C’est une méthode non-supervisée (n’utilise pas la variable réponse) qui permet d’obtenir une représentation à plus petites dimensions d’un jeu de données tout en conservant le maximum d’information possible. En termes plus simple, on cherche à trouver une combinaison linéaire des \\(p\\) attributs qui permettra de maximiser la variance (information). Cette combinaison linéaire constituera notre première composante principale. Par la suite, on cherche une deuxième combinaison linéaire, qui aura comme contrainte d’être orthogonale à la première, qui maximise encore une fois la variance. On procède de cette manière jusqu’à temps qu’on juge avoir un nombre de composantes principales qui contient suffisamment d’information et réduit la dimensionnalité du jeu de données. À la figure 5.1 (Khrunin 2013), on peut voir un exemple où avec seulement les 2 premières composantes principales, il est possible de reconstruire les données de manière intéressante avec 166 000 attributs génétiques différents. Figure 5.1: Exemple où l’analyse en composantes principales permet de passer d’une dimension de 166 000 attributs vers une dimension de 2 attributs. On peut également voir que ce genre de méthode permet de rendre possible la visualisation de données à haute dimension. Voici quelques notes importantes à considérer lorsqu’on utilise cette méthode: Il est important de normaliser (voir section 5.3.1) les données au préalable. Dans le cas contraire, certaines variables ayant des échelles importantes pourraient avoir l’impression d’apporter beaucoup de variance. L’analyse en composantes principales permet de représenter les données dans une toute autre dimension (on l’espère plus petite), ce qui vient rendre difficile l’interprétation des données dans cette nouvelle dimension. Il peut donc être important de re-transformer les données dans leur format original à la fin. Il n’y a pas de méthode statistique pour choisir le bon nombre de composantes principales. On peut se baser sur le pourcentage de variance expliquée ou tout simplement considérer la transformation comme un hyperparamètre du modèle est conserver le nombre de composantes qui donne les meilleures performances. Pour plus d’informations sur le développement mathématique de la méthode, vous pouvez vous référer au chapitre 10.2 du livre (James et al. 2014). 5.2.2 Autres méthodes de réduction Il existe d’autres méthodes pour réduire la dimensionnalité d’un jeu de données lorsque celle-ci cause un problème lors de l’apprentissage. Voici quelques exemples fréquemment utilisées en pratique: Positionnement multidimensionnel: Trouver une représentation (avec une dimension inférieure à la dimension initiale) qui représente le mieux possible les distances entre les observations du jeu de données initial. Voir cette page web pour plus de détails. Analyse factorielle: Réduire la dimensionnalité en modélisant la structure expliquant la relation entre les variables du jeu de données. Voir cette page web pour plus de détails. Calcul de scores: On peut également réduire la dimensionnalité d’un jeu de données en combinant plusieurs attributs de base pour calculer un nouvel attribut (voir section 5.3.3) comme un score par exemple. Cela peut également permettre d’augmenter le pouvoir prédictif d’attributs éparses desquels il est difficile de tirer de l’information lorsque considérés individuellement. 5.3 Transformation de données Avant de passer à l’entraînement d’un modèle, il est préférable pour la majorité des algorithmes d’utiliser les connaissances acquises lors de l’exploration de données (section 4) pour transformer les données dans un format plus propice à l’apprentissage. Cette étape de transformation est souvent ce qu’on appelle le feature engineering. Cette étape demande d’ailleurs beaucoup de va-et-vient avec la prochaine section 6. 5.3.1 Normalisation La normalisation des données permet de ramener les données autour d’une distribution plus “standard”. Pour certains types de modèles, en particulier ceux qui sont basés sur des calculs de distances comme les \\(k\\)-PPV ou le clustering, il est primordial de normaliser les données avant d’en faire l’apprentissage. En effet, cela permet de standardiser les distributions des différents attributs du jeu de données. Par exemple, si deux variables sont distribuées sur des domaines ayant des échelles complètement différentes, il est préférable de plutôt comparer celles-ci sur un échelle commune. Voilà l’objectif fondamental de la normalisation. Il existe différentes méthodes de normalisation. Il n’y a pas de “bonne méthode”, mais certaines sont mieux adaptées à des contextes en particulier. Voici quelques exemples de méthodes traditionnelles où \\(x_{i,A}^{\\prime}\\) représente la donnée \\(i\\) normalisée pour l’attribut \\(A\\) et \\(x_{i,A}\\) représente la donnée originale. Normalisation centrée-réduite \\[ x_{i,A}^{\\prime}=\\frac{x_{i,A}-\\bar{x}_A}{\\hat\\sigma_A} \\] où \\(\\bar{x}_A\\) et \\(\\hat\\sigma_A\\) sont la moyenne et la variance observées de l’attribut \\(A\\). Normalisation min-max \\[ x_{i,A}^{\\prime}=\\frac{x_{i,A}-\\min_A}{\\max_A-\\min_A}\\big({\\rm new\\_max}_A-{\\rm new\\_min}_A\\big)+{\\rm new\\_min}_A \\] où \\(\\min_A = \\min_i x_{i,A}\\) et \\(\\max_A = \\max_i x_{i,A}\\) sont calculés sur la distribution de l’attribut \\(A\\), alors que \\({\\rm new\\_max}_A\\) et \\({\\rm new\\_min}_A\\) correspondent aux bornes du nouvel intervalle désiré. Normalisation pas décimation \\[ x_{i,A}^{\\prime}=\\frac{x_{i,A}}{10^j} \\] où \\(j\\) est la plus petite valeur entière où \\(max(|x_{i,A}|)&lt;1\\). 5.3.2 Discrétisation La discrétisation consiste à prendre une donnée numérique et de la transformer en un ensemble de valeurs discrètes qu’on appelle souvent buckets ou bins. L’idée derrière ce genre de transformation est de simplifier la vie du modèle en lui “pré-mâchant” une donnée continue en certains groupes de valeurs plus faciles à apprendre. Il existe encore une fois plusieurs méthodes pour construire ces regroupements de valeurs. L’étape de l’exploration des données permet entres autres de bien comprendre celles-ci et de créer des groupes qui ajoutent une valeur au modèle. La présence d’un expert du domaine peut également être utile dans la création de ce genre de groupes. 5.3.3 Création de nouveaux attributs Un autre type de transformation classique en modélisation consiste à créer de nouveaux attributs en utilisant les attributs originaux. Ces nouveaux attributs peuvent prendre la forme de scores ou tout simplement être une redéfinition d’un attribut en particulier. C’est d’ailleurs le genre de transformation qui demande une certaine compréhension des données et de l’algorithme utilisé pour faire l’apprentissage. Par exemple, si on prend la date de départ d’un trajet, il est difficile d’intégrer directement cette variable dans un modèle prédictif. Toutefois, on peut utiliser cette variable pour créer un attribut qui indique si le moment de départ du trajet est un jour de semaine ou un jour de week-end. On pourrait également créer un attribut qui indique s’il y a eu des précipitations de pluie pendant la journée au lieu d’utiliser la quantité de pluie (mm) directement. Ce type de transformation permet d’intégrer de l’ingénierie dans le jeu de données. Cela peut également permettre de réduire la dimensionnalité lorsqu’un nouvel attribut contient plusieurs autres attributs et que ceux-ci peuvent être retirés du jeu de données. Ultimement, toutes ces transformations de données ont pour objectif de faciliter l’apprentissage et ainsi obtenir un modèle ayant un plus grand pouvoir prédictif. Les méthodes de transformation de données présentées dans cette section ont comme objectif de faciliter l’apprentissage. Cependant, étant donné que les données sont désormais transformées, il faut faire attention à l’interprétation des résultats. En effet, certaines transformations comme la normalisation ou le lissage font en sorte que les données ne sont plus dans leur format original. 5.4 Prétraitements de données en R Dans cette section, nous verrons comment appliquer les concepts théoriques décrits dans la section précédente dans un contexte de production. Dans cette optique, il faudra garder une trace des prétraitements effectués sur le jeu de données d’entraînement pour être en mesure de les effectuer de la même manière lors de l’inférence. Pour se faire, nous proposons de faire 2 processus de prétraitements différents. Le premier servira à faire le prétraitement des données d’entraînement alors que le deuxième servira à faire le prétraitement des données à prédire lors de l’inférence. Les prochaines sections permettront de formaliser les différentes étapes clés à prévoir pour ces deux scénarios différents en tirant profit de certains packages R. 5.4.1 Séparation du jeu de données Avant même d’effectuer les premiers prétraitements, il est primordial de garder en tête que nous cherchons à construire un modèle prédictif qui sera performant en généralisation. Pour se faire, il faut inévitablement garder de côté un jeu de données test que nous utiliserons à la toute fin pour avoir une idée plausible de la performance de notre modèle sur des données que celui-ci n’a jamais encore vues. Afin de ne pas induire de l’information de ce jeu de données dans notre entraînement, nous devrons d’abord effectuer cette séparation et ensuite faire les différents prétraitements sur le jeu de données d’entraînement seulement. À la toute fin, nous pourrons effectuer ces mêmes prétraitements sur le jeu de données test et ainsi simuler l’entrée de nouvelles données. Pour faire la séparation, il courant dans la pratique de prendre un pourcentage arbitraire des données, par exemple 10%, sélectionné aléatoirement pour le jeu de données test. Pour faire cela, il est possible d’utiliser des fonctions de base en R, comme la fonction sample: ind_train &lt;- sample(x = 1:nrow(data_bixi), size = 0.9 * nrow(data_bixi), replace = FALSE) head(sort(ind_train), 10) ## [1] 1 2 3 4 5 6 7 8 9 10 lapply(list(data_bixi[ind_train,]$duration_sec, data_bixi[-ind_train,]$duration_sec), mean) ## [[1]] ## [1] 818.2513 ## ## [[2]] ## [1] 817.3133 Dans certaines situations, il peut être utile de faire une séparation qui permet de s’assurer de la présence de certaines données dans chaque jeu de données. Pour faire ce genre de séparation, il est possible de faire un échantillonnage stratifié sur certaines variables, ce qui assure la cohérence entre les deux jeux de données vis-à-vis ces variables. Cela peut être notamment utile dans un contexte de données débalancées. Pour faciliter la mise en place de ce genre de séparation, il est possible d’utiliser la fonction createDataPartition du package caret: library(caret) ## Loading required package: lattice ind_train &lt;- createDataPartition(y = data_bixi$duration_sec, times = 1, p = 0.9, list = FALSE) head(sort(ind_train), 10) ## [1] 1 2 3 4 5 6 7 9 10 11 lapply(list(data_bixi[ind_train,]$duration_sec, data_bixi[-ind_train,]$duration_sec), mean) ## [[1]] ## [1] 818.1882 ## ## [[2]] ## [1] 817.8807 5.4.2 Processus de prétraitements Une fois que la séparation du jeu de données est faite, on peut désormais effectuer nos prétraitements via 2 processus différents. Un premier processus sera appliqué sur les données d’entraînement et un deuxième processus sera appliqué sur le jeu de données test (ou les données nouvelles à prédire par notre modèle). La figure 5.2 illustre bien ces 2 processus et comment ceux-ci sont interreliés. Figure 5.2: Processus de prétraitements des données Le point clé ici est que certains objets seront calculés et sauvegardés lors du processus de prétraitements effectué sur les données d’entraînement. Ces objets seront par la suite utilisés pour le processus de prétraitements des données de test. Les prochaines sections montrent quelques options disponibles en R pour réaliser certaines étapes de ce prétraitement. 5.4.2.1 Imputations de données manquantes Pour l’imputation de données manquantes, il est nécessaire d’avoir une liste de variables à imputer avec une définition de comment calculer la valeur d’imputation. Le calcul de la valeur d’imputation se fait sur les données d’entraînement seulement. Il est possible de définir les valeurs d’imputations et de sauvegarder ces valeurs dans un fichier JSON en utilisant le package jsonlite: # Définir les valeurs d&#39;imputation valeurs_imputations &lt;- list( start_station_code = median(data_bixi$start_station_code), start_wday = median(data_bixi$start_wday) ) # Sauvegarder le fichier d&#39;imputations dans un JSON write(jsonlite::toJSON(valeurs_imputations, pretty = TRUE), &quot;chemin/du/model/valeurs_imputations.json&quot;) Dans le prétraitement des données lors de l’inférence, on peut désormais importer ces valeurs d’imputation et remplacer les données manquantes: # Importer en mémoire les données d&#39;imputation valeurs_imputations &lt;- jsonlite::fromJSON(&quot;chemin/du/model/valeurs_imputations.json&quot;) # Remplacer les données manquantes par leur valeurs d&#39;imputation for (col in names(valeurs_imputations)) { data_inference[is.na(get(col)), (col) := list[[eval(col)]]] } Les fichiers JSON sont généralement faciles à traiter dans plusieurs logiciels et langages différents. En R, il est également intéressant d’utiliser ce format qui se marie bien avec les objets de type liste. C’est donc un format que nous recommandons d’utiliser. Notez que dans cet exemple, l’imputation est faite en sauvegardant une liste de valeurs d’imputation. Par contre, cela pourrait également être fait via un autre objet, comme un modèle, qui viendrait faire une prédiction en guise d’imputation. 5.4.2.2 Encodage de données catégoriques Pour être en mesure de répliquer l’encodage de données catégoriques, le principal défi est faire l’encodage un-chaud. La fonction dummyVars du package caret permet de réaliser assez facilement ce genre de transformation: objet_un_chaud &lt;- dummyVars(&quot; ~ weekend_flag&quot;, data = data_bixi) saveRDS(objet_un_chaud, &quot;chemin/du/model/objet_un_chaud.rds&quot;) Lors de l’inférence, on peut importer cet objet en mémoire et transformer les variables pour les données à prédire: objet_un_chaud &lt;- readRDS(&quot;chemin/du/model/objet_un_chaud.rds&quot;) data_inference &lt;- predict(objet_un_chaud, newdata = data_inference) Le package caret offre plusieurs fonctionnalités intéressantes dans un contexte de modélisation. La fonction dummyVars est assez intuitive à utiliser et offre certaines options comme la possibilité de nommer les variables indicatrices par un séparateur au choix (sep), comment gérer les nouvelles catégories (na.action), la possibilité d’enlever le niveau de base (fullRank), etc. 5.4.2.3 Valeurs de normalisation Pour ce qui est de la normalisation des attributs, il faut encore une fois garder une trace de ces valeurs pour ainsi normaliser adéquatement les données lors de l’inférence. Un peu comme dans l’imputation, on peut calculer ces valeurs sur le jeu de données d’entraînement. Voici une manière de stocker ces valeurs, en utilisant encore une fois le package jsonlite: variables_a_normaliser &lt;- c(&quot;variable_1&quot;, &quot;variable_2&quot;) moyennes &lt;- apply(data[, (variables_a_normaliser), with = F], 2, mean) ecarts_types &lt;- apply(data[, (variables_a_normaliser), with = F], 2, sd) valeurs_normalisation &lt;- list( moyennes = as.list(moyennes), ecarts_types = as.list(ecarts_types) ) # Sauvegarder les valeurs de normalisation write(jsonlite::toJSON(valeurs_normalisation, pretty = TRUE), &quot;chemin/du/model/valeurs_normalisation.json&quot;) Une fois les valeurs calculées, on peut les importer et les appliquer sur les données à inférer: # Importer en mémoire les valeurs de normalisation valeurs_normalisation &lt;- jsonlite::fromJSON(&quot;chemin/du/model/valeurs_normalisation.json&quot;) # Appliquer la normalisation pour les variables à normaliser data &lt;- lapply(names(valeurs_normalisation$moyennes), function(x) { data[, (x) := (get(x) - valeurs_normalisation$moyennes[[eval(x)]])/valeurs_normalisation$ecarts_types[[eval(x)]]] })[[2]] 5.4.2.4 Création de nouveaux attributs Pour ce qui est de la création de nouveaux attributs et de la discrétisation, ces prétraitements sont généralement fait en appliquant une transformation directe aux données, par exemple, l’application d’une fonction log. Ces transformations peuvent donc être programmées via du code source qui sera appliqué de la même manière pour les données d’entraînement que pour les données de test. Voici un exemple de fonction qui permettrait de créer un nouvel attribut et en discrétiser un autre, et ce, pour les 2 jeux de données: # Fonction qui crée de nouveaux attributs creation_attributs &lt;- function(data){ # Nouveaux attributs data[, `:=`(start_wday = lubridate::wday(start_date, week_start = 1), start_hour = hour(start_date_time))] # Discrétisation data[start_quartier %in% c(&quot;outremont&quot;, &quot;cotedesneigesnotredamedegrace&quot;, &quot;westmount&quot;, &quot;lesudouest&quot;, &quot;verdun&quot;, &quot;lasalle&quot;), start_quartier_group := &quot;sud_ouest&quot;] data } # La fonction s&#39;applique de la même maniere pour les 2 jeux de données data_train &lt;- creation_attributs(data_train) data_test &lt;- creation_attributs(data_test) 5.5 Conclusion En conclusion, le prétraitement de données a comme objectif de partir d’une table de données brutes et de rendre cette table compatible et prédictive pour un algorithme d’apprentissage. Pour se faire, il nécessaire de commencer par nettoyer les données. Ensuite, certaines étapes de réduction et de transformations des données permettront de rendre les données plus prédictives pour le modèle. Finalement, il est important de garder en tête que le prétraitements doivent pouvoir être répliqués sur les données à inférer. C’est pourquoi il est primordial de garder une trace de ces prétraitements, et ainsi rendre possible le prétraitement d’une seule et unique requête à prédire. 5.6 Références References "],
["metho.html", "Chapitre 6 Construction de modèles 6.1 Gestion des données 6.2 Description classique 6.3 Familles de modèles 6.4 Estimation d’un modèle 6.5 Sélection du modèle final (validation) et évaluation 6.6 Exemple 2 : classification avec xgboost (en construction)", " Chapitre 6 Construction de modèles En introduction du document, nous avons fait l’hypothèse qu’il existe une fonction \\(f\\) connectant nos variables explicatives \\(\\mathbf{x}\\) à \\(y\\) de telle sorte que \\[\\begin{equation} y \\approx f(\\mathbf{x}). \\tag{6.1} \\end{equation}\\] L’objectif principal, dans ce chapitre, est d’apprendre (ou plutôt d’approximer) la fonction \\(f\\) à l’aide de la théorie de l’apprentissage statistique. Plusieurs éléments sont tirés des livres An Introduction to Statistical Learning: with Application in R de Gareth James, Daniela Witten, Trevor Hastie et Robert Tibshirani (James et al. 2014); The Elements of Statistical Learning de Trevor Hastie, Robert Tibshirani et Jerome H. Friedman (Friedman, Hastie, and Tibshirani 2001). L’expression apprentissage statstique a été grandement popularisée par ces auteurs, qui donne la définition (traduction libre) L’apprentissage statistique fait référence à un ensemble d’outils pour modéliser et comprendre des jeux de données complexes. C’est une sous-discipline récente de la statistique qui se développe en parallèle avec les avancées en informatique et, plus particulièrement, en apprentissage automatique. (James et al. 2014) Après le nettoyage des données du chapitre 5, nous avons à notre disposition des données prêtes pour utilisation avec le modèle de notre choix. Évidemment, le choix des variables explicatives reste préliminaire : on peut réaliser, après avoir tenté plusieurs modèles, qu’elles ne sont pas assez informatives pour permettre des prédictions satisfaisantes. Il faudra alors considérer d’autres options; transformer nos variables, ou en en collecter de nouvelles. En supposant que le lecteur possède des connaissances de base en statistique, les sujets suivants sont traités : l’identification de modèles adéquats; l’estimation de modèles (fonction de perte, compromis biais-variance); l’évaluation d’un modèle (validation croisée, erreur de généralisation). Finalement, nous pointerons quelques fois vers la librairie (caret)[https://topepo.github.io/caret/index.html], qui contient plusieurs fonctions permettant une modélisation fluide. 6.1 Gestion des données Concepts clefs : entraînement/validation/test 6.1.1 Divisions entraînement/validation/test Pour plusieurs raisons, il est conseillé, avant même le pré-traitement des données de la Section 5, de séparer aléatoirement son jeu de données en deux (ou trois) partie distinctes : les jeux de données d’entraînement, (de validation) et de test. Chacune des trois parties est associées à une étape de la construction du modèle, qu’on effectue dans l’ordre mentionné. Les données d’entrainement serviront à estimer différents modèles; les données de validation à sélectionner un modèle; les données de test à évaluer le modèle final. Une règle du pouce est d’utiliser la moitié des observations pour l’entrainement et le quart pour chacune des deux autres étapes. \\[ {\\Large \\left(\\mathbf{X}|\\mathbf{y}\\right)} \\quad = \\quad \\left(\\begin{array}{ccc|c} x_{11} &amp; \\dots &amp; x_{1d} &amp; y_1\\\\ x_{21} &amp; \\dots &amp; x_{2d} &amp; y_2\\\\ \\vdots &amp; &amp; \\vdots &amp; \\vdots\\\\ x_{n1} &amp; \\dots &amp; x_{nd} &amp; y_n \\end{array}\\right) \\begin{array}{ccc} \\Bigg\\} &amp; \\stackrel{\\approx 1/2}{\\longrightarrow} &amp; (\\mathbf{X}_{\\rm train}|\\mathbf{y}_{\\rm train})\\\\ \\Big\\} &amp; \\stackrel{\\approx 1/4}{\\longrightarrow} &amp; (\\mathbf{X}_{\\rm val}|\\mathbf{y}_{\\rm val})\\\\ \\Big\\} &amp; \\stackrel{\\approx 1/4}{\\longrightarrow} &amp; (\\mathbf{X}_{\\rm test}|\\mathbf{y}_{\\rm test}) \\end{array} \\] Toutefois –particulièrement quand les données manquent– le jeu de validation est “éliminé” et intègré au jeu d’entraînement. L’étape de validation (décrite à la Section 6.5) est alors effectuée en partitionnant le jeu d’entraînement pour créer plusieurs paires \\((X_{\\rm train},X_{\\rm val})\\), au lieu d’une seule comme avec l’approche classique. Selon la situation, on peut effectuer une permutation aléatoire de nos données, pour éviter que notre division du jeu de données ne soit pollué par des effets indésirables de l’ordre de collecte; ou s’assurer de garder un ordre chronologique des jeux de données, pour réellement prédire des données futures avec les jeux de validation et de test. Finalement, caret permet de préserver la proportion des différentes réponses dans les jeux d’entraînement et de test avec createDataPartition. ind_train &lt;- caret::createDataPartition(y_train, p = .8, # split 80/20, classique list = FALSE, times = 1) La fonction caret::createResample permet de faire des échantillons bootstrap, tandis que la fonction caret::createFolds permet de créer des échantillons pour la validation croisée. 6.1.2 Utilité des différents jeux La différence entre les jeux de données se trouve essentiellement dans ce qu’on calcule avec ceux-ci. On associe au jeu d’entraînement l’erreur d’entrainement, qui quantifie la performance de notre modèle sur… les données d’entraînement! À cette étape, on estime les paramètres de notre modèle. Les jeux de validation et de test servent tous deux à estimer l’erreur de généralisation, c’est-à-dire l’erreur faite sur de nouvelles données. Dans la première c’est pour comparer des modèles (trouver les hyper-paramètres), la deuxième pour obtenir une estimation non biaisée de l’erreur de généralisation. Notez qu’on utilise souvent l’erreur de généralisation, l’erreur test et l’erreur de validation interchangeablement. L’erreur d’entraînement quantifie la performance de notre modèle à prédire les données même sur lequel il a été/est entraîné. Dans ce cas, l’erreur obtenue ne sera pas représentative de l’erreur de généralisation. Il faut garder en tête que l’objectif est de prédire de nouvelles valeurs \\(y\\) à l’aide de nouvelles valeurs \\(\\mathbf{x}\\); l’erreur de généralisation est donc au coeur de nos préoccupations. Ainsi, s’il y a normalisation à faire (voir Section 5), on normalise le jeu d’entraînement sans utiliser les jeux de validation et de test. 6.1.3 Exemple bixi Avec la quantité de données à notre disposition pour l’exemple bixi, il est raisonnable de séparer nos données en trois partie. À la section 5, nous avions déjà séparé notre jeu en deux (entr/test), sortons un troisième jeu de nos données d’entraînement. # Nos données # X_reg # Enlevons-en masse, nous ne sommes pas à plaindre # Sinon, des problèmes de mémoires pourraient être irritants. ind_val &lt;- sample(nrow(X_reg), nrow(X_reg)/2) X_val &lt;- X_reg[ind_val,] X_train &lt;- X_reg[-ind_val,] y_val &lt;- y_reg[ind_val] y_train &lt;- y_reg[-ind_val] rm(X_reg) rm(y_reg) # On s&#39;occupera du test en temps et lieu. 6.2 Description classique Concepts clefs : régression vs. classification Commençons d’abord en reformulant l’équation (6.1) en tant qu’égalité stricte. Pour ce faire, on introduit une quantité aléatoire \\(\\varepsilon\\) qui représente la variabilité non captée par notre modèle. Cela donne l’équation \\[\\begin{equation} y = f(\\mathbf{x}) + \\varepsilon. \\tag{6.2} \\end{equation}\\] Pour une variable réponse \\(y\\) continue, il est naturel de faire les deux hypothèses suivantes à propos de \\(\\varepsilon\\): son espérance est nulle, c’est-à-dire \\(\\mathbf{E}(\\varepsilon) = 0\\); et elle est indépendante de \\(\\mathbf{x}\\). Ceci nous permet entre autres d’ignorer \\(\\varepsilon\\) lorsque vient le temps de faire une prédiction. Étant donné \\(\\mathbf{x}\\), on s’attend à ce qu’en moyenne \\(y\\) soit égale à \\(f(\\mathbf{x})\\), i.e. \\(\\mathbf{E}(y) = f(\\mathbf{x})\\). En introduisant \\(\\varepsilon\\), on admet l’existence d’une erreur irréductible : même si nous réussissions à estimer \\(f\\) parfaitement, il faudrait s’attendre à ce que nos prédictions ne soient pas nécéssairement parfaites. Par exemple, faisons comme si nous savions que nos données sont telles que \\(y_i = 2x_i + \\varepsilon_i\\) et générons \\(n=25\\) observations à partir de ce modèle pour visualiser le phénomène. Aucune des prédictions (qui se trouvent sur la droite) ne correspond à la vraie valeur \\(y\\) observée. L’intuition est qu’on admet la présence de facteurs influençant \\(y\\) auxquels nous n’avons pas accès (qui ne sont pas mesurés) ou qui ne sont simplement pas mesurables. L’utilisation d’un modèle \\(f\\) qui ne permet pas de capturer l’essentiel de la relation entre \\(\\mathbf{x}\\) et \\(y\\) peut aussi limiter notre potentiel de réduction de l’erreur. Ce qui est en notre pouvoir (du moins, si on exclut la re-collecte de données) concerne la fonction \\(f\\). Il est donc important de choisir une famille de modèles appropriée pour le problème qui nous intéresse. 6.3 Familles de modèles Le choix d’une famille de modèles est intimement lié à la tâche qu’on souhaite résoudre et aux données à disposition. En se restreignant à une certaine famille, on impose un ensemble de contraintes à la fonction \\(f\\) de l’équation (6.2), ce qui limite le type de relation entre \\(Y\\) et \\(\\mathbf{X}\\) qu’il sera possible d’apprendre ; paradoxalement, c’est aussi ce qui permet l’apprentissage. On divise généralement les problèmes en deux grandes catégories : la régression et la classification. La régression sous-entend une variable réponse continue, e.g. la grandeur d’une individue ; la classification sous-entend une variable réponse catégorique (une classe), e.g. chat ou chien. La différence entre les deux tâches est évidente, mais parfois la ligne peut être mince : par exemple pour classifier des observations/exemples, souvent on modélise la probabilité qu’une observation appartienne à certaine une classe, ce qui revient en quelque sorte à modéliser une variable réponse continue (une fréquence). On assigne ensuite l’observation à la classe la plus probable. 6.3.1 Régression Par exemple, la (populaire) régression linéaire sous-entend une relation linéaire entre la variable réponse et les facteur explicatifs : \\[\\begin{equation} y = \\beta_0 + \\beta_1 x_1 + \\dots + \\beta_d x_d + \\varepsilon \\tag{6.3} \\end{equation}\\] Ici, les paramètres \\(\\mathbf{\\beta} = (\\beta_0,\\dots,\\beta_d)\\) déterminent comment un changement porté aux variables explicatrices influencera la prédiction. Avec nos données bixi, faisons une régression linéaire pour prédire la durée d’un trajet et une classification binaire pour prédire si un utilisateur terminera sa course dans le même arondissement ou non. Nous l’effectuerons avec la librairie glmnet(vignette : glmnet). 6.3.2 Classification Pour une variable réponse catégorique (disons \\(K\\) classes), la régression de (6.3) n’est pas conseillée. On peut toutefois la modifier légèrement pour trouver un modèle de classification très répandu : la régression logistique. Restons dans le cas binaire pour plus de clarté. La clef consiste à considérer non pas notre réponse \\(Y\\), mais \\(y^* = \\mathbb{P}[Y = 1 | \\mathbf{X}]\\), la probabilité que \\(Y = 1\\) conditionellement aux valeurs des variables explicatives \\(\\mathbf{x}\\). Un problème majeur avec la régression en (6.3) est son incapacité à contraindre la réponse entre 0 et 1, l’espace naturel pour une probabilité. Pour intégrer cette contrainte au modèle, on considère les log-cotes (log-odds) avec la fonction logit (logistic unit), ce qui donne \\[ \\mathrm{ln}\\left( \\frac{y^*}{1-y^*} \\right) = \\beta_0 + \\beta_1 x_1 + \\dots + \\beta_d x_d + \\varepsilon. \\] Ce modèle est généralisable pour un problème de classification multi-classes. En fin de chapitre, un modèle de classification est effectué à l’aide de la librairie xgboost (vignette : XGBoost), qui permet de faire du boosting d’arbres de décision. Le concept de boosting y est brièvement expliqué. 6.3.3 Complexité et disponibilité des données La plupart du temps, les modèles plus contraignants sont favorisés lorsque peu d’observations sont disponibles pour prendre avantage d’une structure dans les données qui est connue (ou supposée) à priori. Certains modèles comme les réseaux de neurones profonds sont reconnus pour être efficaces dans des cas ou la relation entre les variables est très complexe, mais requierent généralement une grande quantité de données. Certains modèles plus simples, comme la régression linéaire, sont parfois choisient pour leur interprétabilité. Certains pourraient dire (avec raison) que que les modèles choisis pour la régression et la classification avec nos données bixi sont des overkill pour leur tâche respective, mais leur présentation en vaut la peine. D’autres librairies intéressantes (et nous passons à côté de beaucoup d’autres!) sont mxnet (deep learning, on prêche pour la paroisse!), keras (deep learning), randomForest (random forests) et e1071 (support vecteur machines). 6.3.4 Variables à inclure Même lorsqu’on se limite à une famille de modèles, il reste à déterminer quelles variables seront incluses. L’option simple : toutes les inclure. On verra plus tard que ce n’est pas toujours souhaitable, en particulier si certaines d’entre elles ne sont pas pertinentes. Des procédures existent pour sélectionner les variables incluses ; nous en verrons à la sous-section 6.4.1. On peut aussi vouloir considérer des intéractions entre les variables, c’est-à-dire artificiellement ajouter des termes du style \\(\\beta_{*} x_{i} x_{j}\\), ce qui fait exploser le nombre de modèles possibles. Laissons ces considérations de côté pour le moment et concentrons-nous sur l’estimation d’un modèle pour lequel les variables sont choisies et figées. 6.4 Estimation d’un modèle Concepts clefs : fonction de perte, predict, pénalités ridge et lasso, sur-apprentissage, hyper-paramètre, compromis biais-variance Estimer un modèle consiste à déterminer la valeur “optimale” de ses paramètres. On cherche donc à minimiser l’erreur d’entraîenement, qu’on quantifie à l’aide d’une fonction de perte \\(L(y,\\hat{y}) = L(y,f(x))\\). Celle-ci détermine la pénalité associée à une mauvaise prédiction. Dans certains cas comme la détection de fraude, où une transaction identifiée comme frauduleuse sera vérifiée par un agent, on peut vouloir minimiser le nombre de faux négatifs (les transactions frauduleuses qui nous glissent entre les doigts), quitte à introduire plus de faux positifs (des transactions identifiées frauduleuses qui ne le sont pas réellement). En d’autres termes, le choix de \\(L\\) doit être motivé par nos attentes par rapport au modèle. La fonction de perte la plus populaire est sans contredit l’erreur quadratique, \\(L(y,\\hat{y}) = (\\hat{y} - y)^2\\). Puisque nous avons à notre disposition plusieurs observations (supposées indépendantes), il s’agit de minimiser la somme des erreurs individuelles (une par observation). Par exemple, combinée à la régression linéaire, l’erreur quadratique donne \\[ \\boldsymbol{L}(\\mathbf{y},\\mathbf{\\hat{y}}) = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 = \\sum_{i=1}^n \\Big(y_i - (\\beta_0 + \\beta_1 x_{i1} + \\dots + \\beta_1 x_{id})\\Big)^2. \\] où les variables en gras \\(\\mathbf{y}\\) et \\(\\mathbf{\\hat{y}})\\) sont les vecteurs de réponses et de prédictions respectivement. En équation matricielle, classique : \\[ \\boldsymbol{\\hat\\beta} = \\mathrm{argmin}_{\\mathbf{\\beta}} \\ (\\mathbf{y} - \\mathbf{X}^\\top \\boldsymbol{\\beta})^{\\top}(\\mathbf{y} - \\mathbf{X}^\\top \\boldsymbol{\\beta}) = (\\mathbf{X} \\mathbf{X}^\\top)^{-1} \\mathbf{X} \\mathbf{y}, \\] appelé l’estimateur des moindres carrées. Plusieurs librairies R permettent l’ajustement de modèles linéaires généralisés. La méthode du maximum de vraisemblance, qui coincide avec la méthodes des moindres carrées pour la régression linéaire, est souvent utilisée pour l’estimation des paramètres (voir e.g. (Friedman, Hastie, and Tibshirani 2001) pour plus de détails). Nous utilisons ici glmnet pour modéliser la durée d’un trajet. glm_basic &lt;- glmnet(x = as.matrix(X_train), y = y_train, family = &quot;gaussian&quot;, lambda=0) # On reviendra sur lambda... L’option family sert à choisir une distribution pour l’erreur irréductible \\(\\varepsilon\\). La distribution normale est l’option par défaut. Évidemment, on peut toujours ajouter des intéractions entre nos variables (i.e. ajouter à notre régression des termes \\(\\beta_{ij} x_i x_j\\)). Par exemple pour faire intéragir les zones de départ et moment de la journées : cols_moment &lt;- grep(pattern = &quot;moment&quot;, names(X_train)) # les nums de colones moment cols_sqg &lt;- grep(pattern = &quot;start_quartier_group&quot;, names(X_train)) #les nums de colones cols_pair &lt;- cbind(rep(cols_sqg,length(cols_moment)), rep(cols_moment,length(cols_sqg))) # toutes les paires # on get back les noms de colonnes et on les colle new_names &lt;- apply(matrix(names(X_train)[c(cols_pair)], ncol = 2), 1, paste0, collapse = &quot;:::&quot;) # Nouveau dataset avec intéractions X_new &lt;- copy(X_train) for(r in 1:nrow(cols_pair)){ i &lt;- cols_pair[r,1] j &lt;- cols_pair[r,2] X_new[, (new_names[r]) := X_train[,i,with=F]*X_train[,j,with=F]] } Tenons-nous en au modèle sans intéractions. Pour faire des prédictions avec nos modèles, la fonction predict est toute désignée. Il faut simplement lui fournir le modèle et les données explicatives concernées. Pour une petite idée de la diversité de nos prédictions par rapport aux réponses : y_pred &lt;- predict(object = glm_basic, newx = as.matrix(X_train)) ggplot(data.table(y = c(y_train,y_pred), type=c(rep(&quot;true&quot;,length(y_train)), rep(&quot;pred&quot;,nrow(y_pred)))), aes(x=y, fill=type)) + geom_density(alpha = .5) + theme_minimal() Évidemment, plus de variables explicatives sont nécessaires pour prédire les valeurs extrêmes. Aussi, il semble que les grandes valeurs (en réponse) influencent nos prédictions à la hausse. Un peu trop peut-être? Le modèle fait tout de même des prédictions personnalisées. # permet de voir la valeur des paramètres coef.glmnet(glm_basic) ## 10 x 1 sparse Matrix of class &quot;dgCMatrix&quot; ## s0 ## (Intercept) 1261.784932 ## is_member -466.672321 ## weekend_flag 50.706616 ## start_quartier_group.plateau_mont_royal -170.911583 ## start_quartier_group.sud_ouest 53.463823 ## start_quartier_group.ville_marie -40.096503 ## start_quartier_group.autre 344.321702 ## moment_journee.matin -9.716465 ## moment_journee.nuit -59.815929 ## moment_journee.soir -3.389197 6.4.0.1 Option caret on_y_reviendra &lt;- trainControl(method=&quot;none&quot;) glm_basic &lt;- train(X_train, y_train, method = &quot;glmnet&quot;, trControl = on_y_reviendra, metric = &quot;RMSE&quot;, # Peut-être aimerions-nous &quot;MAE&quot; ici? lambda = 0) 6.4.1 Un mot sur la régularisation Lorsque beaucoup de variables explicatives (ou des fonctions de celles-ci) sont considérées simultanément, il est possible qu’un sous-ensemble d’entre elles ne soit pas pertinent pour la tâche à effectuer. Plus généralement, lorsqu’un modèle est sur-paramétrisé par rapport à la quantité d’observations disponible, les techniques classiques d’estimation doivent être revues pour éviter le sur-apprentissage (overfit). Le danger est que le modèle apprenne (en quelque sorte par coeur) le jeu de données d’entraînement, ce qui diminue son pouvoir de généralisation. Les techniques de régularisation permettent de mitiger ces effets négatifs en modulant l’importance de certaines variables pour la prédiction. Nous l’expliquons ici dans le contexte de la régression linéaire, mais l’idée est valide ou généralisable pour plusieurs modèles. L’approche consiste à ajouter une pénalité (appliquée aux paramètres \\(\\boldsymbol{\\beta}\\)) à la fonction de perte \\(L\\), c’est-à-dire \\[ \\sum_{i=1}^n L(y_i,f(\\mathbf{x}_i | \\boldsymbol\\beta )) + \\lambda P(\\beta), \\qquad \\lambda \\in \\mathbb{R}. \\] Le coefficient \\(\\lambda\\) est un hyper-paramètre controlant le degré de régularisation que nous souhaitons appliquer. La plupart du temps (par choix), la fonction \\(P\\) pénalise davantage les vecteurs \\(\\boldsymbol{\\beta}\\) avec de grandes valeurs. Encore une fois, la norme euclidienne (carrée) qu’on utilise aussi pour la fonction de perte est très populaire. \\[ P(\\boldsymbol{\\hat\\beta}) = ||\\boldsymbol{\\hat\\beta}||_2^2 = \\sum_{j=1}^p \\hat\\beta_j^2. \\] Sa combinaison avec la régression porte le nom de régression ridge. On l’utilise pour atténuer l’impact du bruit (la variance introduite par les variables non-pertinentes). Intuitiviment, si certains coeficients \\(\\beta_j\\) sont artificiellement gonflés, alors on devrait obtenir une meilleure erreur de généralization lorsque ces derniers sont réduits. La librairie glmnet permet la régression ridge avec le paramètre lambda avec e.g. glmnet::glmnet(x = as.matrix(X_train), y = y_train, family = &quot;gaussian&quot;, lambda=1, alpha = 0) # alpha = 0 est necessaire Pour des raisons computationelles (et de convergence), il n’est toutefois pas conseillé de fournir une valeur unique pour lambda à la fonction glmnet, mais plutôt un ensemble de valeurs pour chacunes desquelles l’algorithme ajustera un modèle. Si aucune valeur n’est fournie, la fonction en choisiera automatiquement 100 (ou moins). Une deuxième pénalité très populaire est la somme des valeurs absolues des paramètres (la méthode lasso) : \\[ P(\\boldsymbol{\\hat\\beta}) = \\sum_{j=1}^p |\\hat\\beta_j|. \\] Son grand avantage est qu’elle force, pour une intensité \\(\\lambda\\) assez forte, certains paramètres à zéro exactement (et non pas seulement à être petits). Dans ce cas, il est ensuite plus facile d’identifier les variables explicatives significatives et d’interpréter le modèle. La fonction glmnet applique cette pénalité par défault avec alpha = 1. En fait, alpha permet de pondérer les pénalités ridge et lasso, et donc d’utiliser les deux à la fois (la technique elastic net). Notez que lambda est encore utilisé avec le lasso. D’une certaine façon, la méthode lasso généralise donc les méthodes de sélection de variables classique (step-wise/forward/backward selection). Par exemple, avec la forward selection, on commence avec \\(\\beta_0\\) seulement et on intègre une à une les variables en commençant par les plus significatives (selon un test statistique choisi). \\[\\begin{equation} f_1(\\mathbf{x}) = \\beta0 \\quad \\rightarrow \\quad f_2(\\mathbf{x}) = \\beta0 + \\beta_7 x_7 \\quad \\rightarrow \\dots \\quad f_3(\\mathbf{x}) = \\beta0 + \\beta_7 x_7 + \\beta_2 x_2 \\quad \\rightarrow \\dots \\end{equation}\\] La backward selection est définie similairement, mais en partant du modèle complet et en éliminant des variables non-significatives. Finalement, ces dernières peuvent être combinées en une méthode qui, à chaque étape, peut entrer et/ou sortir des variables du modèle. Comme le nombre de variables incluses dans le modèle n’est pas directement un paramètre du modèle lui-même, on peut le considérer comme un hyper-paramètre, équivalent au \\(\\lambda\\) du lasso. 6.4.2 Les hyper-paramètres et le compromis biais-variance Pour comprendre pourquoi l’inclusion de toutes les variables n’est pas toujours avantageuse, ou plus généralement la pertinence de la régularisation, il faut s’attarder au concept de compromis biais-variance. Dans le cas de la régression linéaire, la fonction \\(f(\\cdot| \\boldsymbol{\\hat\\beta})\\) estimée dépend des données par l’entremise de \\(\\boldsymbol{\\hat\\beta}\\), c’est donc dire qu’avec un autre jeu de données (provenant de la même distribution) on obtiendrait un modèle différent. La variance inhérente au processus d’estimation est une composante importante de l’erreur de généralisation. L’erreur de généralisation au point \\(\\mathbf{x}_0\\) est donnée par \\[\\begin{equation} \\mathbb{E}[(Y - \\hat{f}(\\mathbf{x}_0))^2 | \\mathbf{x}_0] = \\mathbb{V}{\\rm ar}(\\hat{f}(\\mathbf{x}_0)) + {\\rm Biais} [\\hat{f}(\\mathbf{x}_0)]^2 + \\mathbb{V}{\\rm ar}(\\varepsilon). \\end{equation}\\] Le dernier terme est l’erreur irréductible, sur laquelle (par définition) on n’a pas de contrôle. Les deux premiers termes sont le biais (au carré) et la variance de l’estimateur \\(\\hat{f}\\) de \\(f\\) (e.g. l’estimateur \\(f(\\cdot | \\hat\\beta)\\) de \\(f(\\cdot | \\beta)\\)). L’introduction d’une pénalité augmente le biais : certains paramètres se voient réduits injustement et on s’éloigne de leur vraie valeur. Par contre, l’effet sur la variance va dans l’autre sens : les modèles plus pénalisés auront tendance à moins changer lorsqu’entraînés sur de nouvelles données. Pour illustrer l’idée, considérons des données qui proviennet d’un mélange de deux populations normales avec des moyennes différentes. train_dummy &lt;- data.table(y = sample(0:1, 50, replace = T)) train_dummy[, x1 := rnorm(n=50, mean=y, sd=1/2)] train_dummy[, x2 := x1 + rnorm(n=50, mean=y, sd=1/4)] g &lt;- ggplot(train_dummy, aes(x=x1, y=x2, col=factor(y))) + geom_point(size=2) + theme_minimal() + labs(col = &quot;classe&quot;) g On peut utiliser ces données pour prédire la classe associée à un nouveau point \\(x_0\\) en fonction des (disons) 3 points \\(x_j\\) dans data_dummy les plus près de \\(x_0\\). C’est une méthode bien connue qui porte le nom de k-nn (k nearest neighbours, k plus proches voisins), test_dummy &lt;- data.table(y = sample(0:1, 100, replace = T)) test_dummy[, x1 := rnorm(n=100, mean=y, sd=1/2)] test_dummy[, x2 := x1 + rnorm(n=100, mean=y, sd=1/4)] new_y_pred &lt;- class::knn(train = matrix(c(train_dummy$x1,train_dummy$x2),ncol=2), test = matrix(c(test_dummy$x1,test_dummy$x2),ncol=2), cl = train_dummy$y, # vraie classes du data d&#39;ent. k = 3) # nombre de voisins utilisé test_dummy[, pred := new_y_pred] test_dummy[, succes := new_y_pred == y] g + geom_point(data = test_dummy, aes(x=x1, y=x2, colour=factor(y), shape = factor(succes, labels = c(&quot;non&quot;,&quot;oui&quot;))), size=2) + scale_shape_manual(values=c(4,0)) + theme_minimal() + labs(col = &quot;classe&quot;, shape = &quot;bonne prédiction&quot;) Ici, k est l’hyper-paramètre. Comme plusieurs hyper-paramètres, il sert à “lisser” nos prédictions. Plus \\(k\\) est grand, plus on aggrège d’information pour faire notre prédiction. Par exemple lorsque \\(k = n\\), le nombre d’observations dans le jeu d’entraînement, on obtient toujours la même prédiction (la classe avec le plus de représentants, très embettant quand les classes sont de mêmes tailles). Approximons l’erreur de généralisation pour chaque valeur de \\(k\\) au point \\(x_0 = .75\\). Notons que \\(\\mathbb{E}[Y|X_1 = .5, X_2 = 1]\\) est donné par x0 &lt;- c(.75,.75) Sig &lt;- matrix(1/2^2,2,2) # la matrice de variance-covariance pour la classe 1 Sig[2,2] &lt;- Sig[2,2] + 1/4^2 # la variance de x2 Ex0 &lt;- dmvnorm(x0, mean = c(1,1), sigma = Sig)/(dmvnorm(x0, mean = c(0,0), sigma = Sig) + dmvnorm(x0, mean = c(1,1), sigma = Sig)) # la prob conditionnelle que x0 = 1 En générant des données d’entraînement, on peut estimer l’erreur au carré, la variance, et donc l’erreur de généralisation. Voici ce que ça donne en fonction de \\(k\\) (2000 répétitions pour chaque valeur de \\(k\\)). Particularité des hyper-paramètres : Les hyper-paramètres comme \\(\\alpha\\), \\(\\lambda\\) et \\(k\\) ne peuvent être estimés de façon traditionnelle. On détermine souvent leurs valeurs en faisant une recherche en grille (grid search). Ceci veut dire que, dans notre cas, nous tenterions plusieurs combinaisons de alpha et lambda pour trouver la meilleure paire. Fixons alpha=1 et penchons-nous sur lambda. glms &lt;- glmnet(x = as.matrix(X_train), y = y_train, family = &quot;gaussian&quot;) Puisque que notre objectif ultime est de minimiser l’erreur de généralisation, on utilise sur celle-ci pour comparer les différents modèles : c’est l’étape de validation des modèles. 6.5 Sélection du modèle final (validation) et évaluation Concepts clefs : erreur de généralisation, validation croisée, fonction predict La sélection du modèle finale, ayant comme but de minimiser l’erreur, passe généralement par un compromis entre le nombre de paramètres et la qualité de l’ajustement du modèle, ou directement par l’estimation de l’erreur de généralisation. 6.5.1 Validation directe La méthode la plus simple consiste à utiliser nos données de validation (si disponibles!) À moins qu’une autre fonction de perte ne se présente comme naturelle pour le problème en question, on calcule l’erreur de généralisation de la même façon que l’erreur sur le jeu d’entraînement, c’est-à-dire avec la fonction \\(\\boldsymbol{L}\\) ; cette fois-ci en utilisant les données de validation. Notre objet glms contient \\(100\\) modèles différents, pour chacun d’eux, estimons l’erreur de généralisation. Encore une fois, c’est la fonction predict qui nous permet de faire des prédictions à partir d’un modèle et de variables explicatrices. Elle peut être utilisée avec plusieurs types de modèles, à chaque fois avec ses particularités. À titre d’exemple, utilisons ici – au lieu de l’erreur quadratique moyenne (MSE, \\((y - f(x))^2\\)) – l’erreur absolue moyenne (MAE, \\(|y - f(x)|\\)) pour quantifier la performance de nos modèles. pred &lt;- predict.glmnet(glms, newx = as.matrix(X_val)) # avec le jeu de validation erreur &lt;- colMeans(abs(pred - y_val)) ggplot(data.table(lambda = glms$lambda, erreur = c(erreur)), aes(x=lambda, y=erreur)) + geom_line() Sans surprise, la régularisation n’est pas particulièrement avantageuse pour notre problème. Une raison plausible est que nous avons amplement de données pour bien estimer chaque paramètres individuellement. Choisissons donc la régression non-pénalisée comme modèle final. Pour faire une prédiction avec ce modèle en particulier, on spécifie notre choix de lambda avec le paramètre s. predict.glmnet(glms, newx = as.matrix(X_val[1,]), s = 0) Un point laisser de côté jusqu’à présent est qu’on peut choisir comme modèle final non pas celui qui minimise l’erreur, mais le modèle le plus simple qui se trouve à une distance raisonnable (en termes d’erreur) du meilleur modèle. Pour ce faire, on doit avoir une idée de ce qui est raisonnable. Plusieurs librairies en R offrent cette possibilité. glmnet le permet avec la validation croisée, voir la Section 6.5.3. 6.5.2 Critères classiques Lorsqu’aucune donnée de validation n’est disponible, une alternative simple est d’utiliser des critères comme l’AIC (Akaike Information Criterion), le BIC (Bayesian Information Criterion), ou le \\(C_p\\) de Mallows. Chacune de ses méthodes à ses particularités, mais elles s’opèrent similairement. Penchons nous brièvement sur l’AIC par exemple. Pour la régression linéaire, \\[ AIC(f) = 2 k - 2 \\boldsymbol{L}(\\boldsymbol{y},f(\\boldsymbol{\\boldsymbol{x}})) \\] où \\(k\\) est le nombre de paramètres inclut la régression \\(f\\) de (6.3) et \\(\\boldsymbol{L}\\) est la perte quadratique. Notez que ceci est valide justement parce que la perte quadratique, dans ce cas particulier, coincide avec la log-vraisemblance du modèle et que \\(\\boldsymbol{\\hat\\beta}\\) est le vecteur qui minimise la perte. L’utilisation de plus de paramètres permet un meilleur ajustement aux données d’entraînement (une valeur plus faible de \\(\\boldsymbol{L}\\)), mais cette amélioration, pour être acceptée, doit compenser l’augmentation qu’induit le terme \\(2k\\). Ces méthodes, quoique très simples, reposent en général sur certaines hypothèses qui peuvent rendre leur utilisation douteuse. Pour notre application, nous utilisons la validation croisée. 6.5.3 Validation croisée La validation croisée permet d’estimer l’erreur de généralisation à même le jeu de données d’entraînement. Pour ce faire, on se crée artificiellement des pairs \\((\\mathbf{X}_{\\rm train},\\mathbf{X}_{\\rm val})\\) est divisant le jeu d’entraîenement \\(\\mathbf{X}\\) en (disons) \\(K = 10\\) partie de même taille. De là le nom anglophone \\(K\\)-fold cross-validation. \\[ {\\Large \\left(\\mathbf{X}|\\mathbf{y}\\right)} \\quad = \\quad \\left(\\begin{array}{ccc|c} x_{11} &amp; \\dots &amp; x_{1d} &amp; y_1\\\\ x_{21} &amp; \\dots &amp; x_{2d} &amp; y_2\\\\ \\vdots &amp; &amp; \\vdots &amp; \\vdots\\\\ x_{n1} &amp; \\dots &amp; x_{nd} &amp; y_n \\end{array}\\right) \\begin{array}{ccc} \\Big\\} &amp; \\stackrel{\\approx 1/10}{\\longrightarrow} &amp; (\\mathbf{X}^1|\\mathbf{y}^{10})\\\\ \\vdots &amp; &amp; \\vdots\\\\ \\Big\\} &amp; \\stackrel{\\approx 1/10}{\\longrightarrow} &amp; (\\mathbf{X}^{10}|\\mathbf{y}^{10}) \\end{array} \\] Pour chaque valeur de \\(k \\in \\{1,\\dots,10\\}\\), l’idée est d’entraîner nos modèles sur les données \\((\\mathbf{X}^{-k}|\\mathbf{y}^{-k})\\), c’est-à-dire toutes les données sauf \\((\\mathbf{X}^{k}|\\mathbf{y}^{k})\\). Le but est de prédire, avec ces modèles, les réponses \\(\\mathbf{y}_{\\rm train}^{k}\\) laissées de côté pour l’entraînement à partir des variables explicatrices \\(\\mathbf{X}^{k}\\). Comme précédemment, utilisons \\(f(\\mathbf{x})\\) pour référer au modèle théorique et \\(f^{(-k)}(\\mathbf{x})\\) pour les modèles estimés sur \\((\\mathbf{X}^{-k}|\\mathbf{y}^{-k})\\). L’erreur de généralisation de \\(f(\\mathbf{x})\\) est estimée par la moyenne obtenue des erreurs obtenues sur les \\(K\\) folds : \\[\\begin{equation} \\frac{1}{K} \\sum_{k=1}^K \\mathbf{L}(f^{(-k)}(\\mathbf{x}_i^k), \\mathbf{y}^k). \\end{equation}\\] L’élément essentiel de la procédure est qu’en aucun cas les observations “à prédire” ne doivent être utilisées pour l’estimation. C’est l’erreur la plus commune est commise. Les méthodes comme lasso et les régressions step-wise, en combinant la sélection de variables avec l’estimation, utilisent le jeu de données d’entraînement. L’étape de sélection doit donc faire partie de la routine de validation croisée. Ce n’est donc pas seulement les modèles que nous évaluons, mais les procédures d’estimation. Pour notre exemple principal avec la libraire glmnet, cela signifie q’on doit donc appliquer la fonction glmnet à chaque jeu d’entraînement \\((\\mathbf{X}^{-k}|\\mathbf{y}^{-k})\\). Il peut être difficile de choisir les valeurs à tester. Par exemple pour lambda dans notre modèle pour bixi, on peut simplement prendre les valeurs présentes dans glms. Toutefois, il est plus facile d’utiliser la fonction built-in de la librairie pour faire la validation croisée en entier : cv.glmnet. Pour l’exemple, utilisons “seulement” \\(500,000\\) observations et faisons un 5-fold. ind_sub &lt;- sample(nrow(X_train), 500000) X_sub &lt;- X_train[ind_sub,] y_sub &lt;- y_train[ind_sub] glms_cv &lt;- cv.glmnet(x = as.matrix(X_sub), y = y_sub, type.measure = &quot;mae&quot;, # utilisons l&#39;erreur absolue moyenne alpha = 1) plot(glms_cv) Des intervalles de confiances sont fournies, ce qui permet non seulement d’identifier la valeur de lambda qui minimise l’erreur, mais aussi la plus grande valeur pour laquelle l’erreur se trouve à moins d’un écart-type du minimum. Ces valeurs sont indiquées par les traits verticaux et s’obtiennent avec glms_cv$lambda.min ## [1] 0.9442917 glms_cv$lambda.1se ## [1] 8.806496 Si on voulait appliquer la procédure pour d’autres valeurs de alpha (ce qu’on aurait à faire manuellement), il serait important d’utiliser les même folds, c’est-à-dire de garder la même partition du jeu d’entraînement pour chaque valeur de alpha. nfolds &lt;- 5 foldid &lt;- sample(length(y_train), nfolds) glms_cv &lt;- cv.glmnet(x = as.matrix(X_train), y = y_train, type.measure = &quot;mae&quot;, # utilisons l&#39;erreur absolue moyenne lambda = des_valeurs, alpha = une_valeur, foldid = foldid) 6.5.3.1 Option caret La dernière étape, l’ajustement de alpha, implique un peu plus de travail. La librairie caret peut nous faciliter la vie : # 5-fold val_setup &lt;- trainControl(method=&quot;cv&quot;, number=5, returnResamp=&quot;all&quot;) hparam_grid &lt;- expand.grid(alpha = c(0,.5,1), lambda = seq(0.001, 0.1, 0.001)) # Attention, avec trop de données ça peut être long (voir impossible) glm_cv &lt;- train(X_train, y_train, method = &quot;glmnet&quot;, trControl = val_setup, metric = &quot;MAE&quot;, tuneGrid = hparam_grid) 6.5.4 Évaluation finale Puisque l’erreur de généralisation fut estimée pour sélectionner le modèle, il semble inutile de refaire l’exercice avec le jeu de données test. Pour comprendre l’utilité de cette étape finale, considérons la régression linéaire \\[\\begin{equation} y = x_1 + x_2 + x_3 + x_4 + x_5 + \\varepsilon \\tag{6.4} \\end{equation}\\] où \\(x_1,\\dots,x_5,\\varepsilon \\stackrel{\\rm iid}{\\sim} U(0,1)\\). (Les variables sont toutes distribuées uniformément sur l’intervalle \\((0,1)\\), vraiment des données bidons quoi.) Supposons que, pour une raison quelconque, nous puissions seulement utiliser une variable pour faire nos prédictions. Les modèles en compétitions (en supposant en plus que nous sachions que les coefficients sont égaux à \\(1\\) dans (6.4) – aucune estimation requise!) seraient donc, pour \\(k=1,\\dots,5\\), \\[\\begin{equation} f(\\mathbf{x}) = x_k + 2.5 = \\mathbb{E}[y|x_k]. \\end{equation}\\] Évidemment, on doit s’attendre à ce que tous nos modèles soient équivalents. Estimons leur erreur de généralisation (sur des données simulées). set.seed(666) X_test &lt;- matrix(runif(10*6), 10, 6) y_test &lt;- rowSums(X_test) colMeans(( y_test - (X_test[,-6]+2.5) )^2) ## [1] 0.7516437 0.2738359 0.5429150 0.5810867 0.6176490 Selon nos estimations, le modèle utilisant \\(x_2\\) est clairement meilleur que les autres. Refaisons le test. set.seed(667) X_test &lt;- matrix(runif(10*6), 10, 6) y_test &lt;- rowSums(X_test) colMeans(( y_test - (X_test[,-6]+2.5) )^2) ## [1] 0.2626966 0.3012172 0.2360650 0.2104903 0.2803220 Maintenant, le modèle 4 qui semble bien meilleur! Au fond, puisque dans ce cas nous savons que tous les modèles sont équivalent, on peut obtenir une meilleure estimation de l’erreur de généralisation en moyennant celles de chacun des modèles. On obtient mean(colMeans(( y_test - (X_test[,-6]+2.5) )^2)) ## [1] 0.2581582 qui est vraiment au-dessus de nos estimations pour les meilleurs modèles. La morale de ces petits tests est la suivante : si nos modèles sont équivalents, on va nécéssairement sélectionner le modèle qui performe le mieux sur nos données de validation. La supériorité du modèle choisit est illusoire et on risque donc de sous-estimer l’erreur de généralisation. C’est pourquoi il est plus sage de faire une évaluation finale de notre modèle gagnant après l’étape de sélection. Avec nos données bixi, on ne devrait pas voir trop de différence toutefois. L’étape est très simple : # X_test[,glm_pred := predict(object = glms, newx = as.matrix(X_test), s=0)] # X_test[,glm_error := abs(y_test - glm_pred)] # mean(X_test$glm_error)/60 # (en minutes) Ce chiffre nous donne idée de l’erreur moyenne qu’on fera (en minutes) sur la durée d’un trajet lorsqu’on mettra notre modèle en production. Pour une analyse plus détaillée, on peut se pencher sur l’erreur par quartier de départ. # X_test_long &lt;- melt(X_test, measure.vars = grep(&quot;start_quartier&quot;, names(X_test)), # variable.name = &quot;start_quartier&quot;, # value.name = &quot;ind_quartier&quot;) # # X_test_long &lt;- X_test_long[ind_quartier == 1,] # # X_test_long[, mean(glm_error)/60, .(start_quartier)] La plus grande variabilité des durées dans les quartiers “autre” se fait ressentir. Pour les modèles de classification binaires, un des outils les plus utiles est la matrice de confusion, qui nous permet de visualiser nos performances par classe (0 et 1). On l’utilise dans l’exemple avec xgboost qui suit. 6.6 Exemple 2 : classification avec xgboost (en construction) Concepts clefs : boosting, early stopping, matrice de confusion Utilisons la librairie xgboost (eXtreme Gradient Boosting) pour faire un modèle de classification : déterminer si un utilisateur reviendra à la même station. Elle permet d’ajuster un modèle (“boosté”) construit à partir d’arbres de décisions. C’est clairement un overkill pour notre tâche, mais ça donne une idée du potentiel. Avec nos données, il y a peu (pas) de chances qu’on ait assez de signal pour clairement détecter qui reviendra à la station, mais il reste tout de même intéressant de quantitifer la possibilité. 6.6.1 Split train/val Comme nous avons beaucoup de données par rapport à la tâche à effectuer, permettons-nous un (énorme!) jeu de validation. ind_val &lt;- sample(nrow(X_classif),nrow(X_classif)/2) X_val &lt;- X_classif[ind_val,] y_val &lt;- y_classif[ind_val] X_classif &lt;- X_classif[-ind_val,] y_classif &lt;- y_classif[-ind_val] 6.6.2 Le modèle en bref Ce modèle est l’un des plus populaires auprès des participants des concours Kaggle. L’idée (très générale) est de créer des arbres de décisions, qui forment un bassin de weak learners, et d’effectuer un vote pondéré en tant que prédiction. Un weak learners est un algorithme de classification qui performe légèrement mieux que le hasard. Le boosting, dans notre cas, fait référence à la méthode employée pour créer les arbres ; nous y reviendrons brièvement à l’étape d’estimation. On peut percevoir les modèle de boosted trees comme un modèle additif ; une sorte de modèle linéaire généralisée, mais avec des fonctions plus complexes insérées dans la formule. Pour notre cas, \\[\\begin{equation} g(y^*) = f(\\mathbf{x}) = \\sum_{m=1}^M \\beta_m f_m(\\mathbf{x}) \\tag{6.5} \\end{equation}\\] ou les fonctions \\(f_m\\) sont des arbres de décisions qui retournent soit \\(0\\) soit \\(1\\). Nous utiliserons la fonction logit pour \\(g\\). La prédiction finale (l’utilisateur reviendra oui (1) ou non (0)) sera \\(\\mathbb{1}(f(\\mathbf{x}) &gt; .5)\\). En fait, on peut vouloir jouer avec le seuil de décision, disons \\(\\alpha\\) (un hyper-paramètre), et donc considérer \\(\\mathbb{1}(f(\\mathbf{x}) &gt; \\alpha)\\). Les paramètres \\(\\beta_m\\) permettent de donner plus d’importances (un vote qui pèse plus) aux arbres qui sont plus performants. 6.6.3 Estimation Pour certain, l’estimation du modèle peut paraître un peu non-conventionelle. On ajuste d’abord un seul arbre de décision et identifions les observations pour lesquelles nos prédictions sont mauvaises. Lors de la construction du deuxième arbre, on met l’emphase (plus de poids) sur ces observations “problématiques” pour forcer l’arbre à les considérer plus sérieusement. On répète la procédure un nombre déterminé de fois, disons \\(M\\). La procédure est automatisée par la fonction xgboost. Pour ajuster un modèle avec disons \\(M=15\\) (voir (6.5)) : xgb_naive &lt;- xgboost(data = as.matrix(X_classif), label = y_classif, booster = &quot;gbtree&quot;, objective = &quot;binary:logistic&quot;, nrounds = 15, verbose = F) Les fonctions de pertes classiques (mse, mae) peuvent être utilisées en classification, surtout que notre prédiction est (en quelque sorte) une probabilité et est donc “continue”. Toutefois, il est possible d’utiliser des mesures plus “discrètes” comme l’erreur de classification : \\[\\begin{equation} L(y,f(\\mathbf{x})) = 1 - \\mathbb{1}\\{f(\\mathbf{x}) = y\\} \\end{equation}\\] C’est l’erreur utilisé par xgboost avec binary:logistic. Plusieurs autres options existent : eta (entre 0 et 1) qui est le paramètre de learning rate peut être très utile pour éviter le sur-entraînement. Brièvement, des petites valeurs de eta empêche l’algorithme de construire des arbres avec trop de poids (et donc ralenti son apprentissage). En contrepartie, il faudra utiliser une valeur de \\(M\\) plus grande, i.e. intégrer plus d’arbres au modèle. maxdepth (profondeur maximale des arbres) et subsample (utilisation d’un sous-ensemble des données pour créer les arbres) sont deux autres options qui valent la peine d’être considérées. verbose = TRUE permet d’avoir un suivi en continue (des prints) de l’estimation. Abordons plutôt une option générale (aussi disponible avec glmnet), intéressante pour les jeux de données débalancés (nous avons beaucoup plus de y_classif == 0 que de y_classif == 1), ce qui est particulièrement pertinent pour la détection de fraudes. L’option weight nous permet de donner plus d’importance à certaines observations dans la fonction de perte (à ne pas confondre avec ce qui est fait pour construire les arbres de décision, quoique l’idée est en fait très similaire). Concrètement, on définit des poids \\(w_i\\) qu’on introduit comme suit dans la fonction de perte \\[\\begin{equation} \\boldsymbol{L}(\\mathbf{y},\\mathbf{X}) = \\sum_{i=1}^n w_i L(y_i, \\mathbf{x}_i) \\end{equation}\\] où \\(L(y_i, \\mathbf{x}_i)\\) est la perte calculée pour l’observation \\(i\\). Pour donner des poids totaux égaux pour les deux classes : prop_1 &lt;- mean(y_classif == 1) # proportion de 1 poids &lt;- (1-prop_1)*y_classif + prop_1*(1-y_classif) xgb_w &lt;- xgboost(data = as.matrix(X_classif), label = y_classif, booster = &quot;gbtree&quot;, objective = &quot;binary:logistic&quot;, nrounds = 15, verbose = F, weight = poids) 6.6.4 Validation Avec beaucoup de temps, nous pourrions faire une recherche en grille du style : param_grid &lt;- expand.grid(eta = seq(.1,1,.15), nrounds = 10:100, maxdepth = 5:20) soit en utilisant le jeu de données de validation ou la validation croisée. (Note : encore une fois, une fonction existe pour faire la validation croisée, xgb.cv, voir le help.) Il est évident qu’une meilleure grille peut être définie : la plupart des combinaisons présentées ci-haut produiraient des modèles médiocres, en particulier quand eta et nrounds seraient tous les deux petits. Nous contournerons ce problème en gérant nrounds avec du early stopping. Si on possède un jeu de validation, on peut le fournir à xgboost pour garder un oeil sur l’erreur de généralisation pendant l’entraînement. La méthode du early stopping consiste à arrêter l’entraînement quand l’erreur de validation ne s’améliore plus. Nous n’avons donc pas à gérer nrounds, mais il nous faut une perte pour la validation… Au lieu de prendre une des pertes par défault, tentons quelque chose mieux aligné avec notre objectif “détection de fraude”. # Pour utiliser xgb.train, on doit se créer un data de la classe xgb.DMatrix dtrain &lt;- xgb.DMatrix(as.matrix(X_classif), label = y_classif) dval &lt;- xgb.DMatrix(as.matrix(X_val), label = y_val) # Elle fait quoi cette perte? perte_val &lt;- function(y_pred, dtrain){ # On get les réponses dans dtrain y_true &lt;- getinfo(dval, &quot;label&quot;) err &lt;- ( 1 + sum(y_pred*(1-y_true)) ) / ( 1 + sum(y_pred*y_true) ) return(list(metric = &quot;gérabilité&quot;, value = err)) } # Allons-y avec les paramètres par défaut.. sinon, voir xgb.cv !! # ou utiliser caret xgb &lt;- xgb.train(data = dtrain, booster = &quot;gbtree&quot;, objective = &quot;binary:logistic&quot;, nrounds = 100, # On se rendra pas là... early_stopping_rounds = 3, maximize = FALSE, watchlist = list(train = dtrain, test=dval), feval = perte_val) La matrice de confusion nous donne une meilleure idée de ce qui se passe : pred &lt;- predict(xgb, newdata = as.matrix(X_val)) # Encore caret! Notez que le treshold caret::confusionMatrix(as.factor(as.numeric(pred &gt; .1)),as.factor(y_val)) ## Confusion Matrix and Statistics ## ## Reference ## Prediction 0 1 ## 0 1741001 36133 ## 1 407 93 ## ## Accuracy : 0.9794 ## 95% CI : (0.9792, 0.9797) ## No Information Rate : 0.9796 ## P-Value [Acc &gt; NIR] : 0.9523 ## ## Kappa : 0.0045 ## ## Mcnemar&#39;s Test P-Value : &lt;2e-16 ## ## Sensitivity : 0.999766 ## Specificity : 0.002567 ## Pos Pred Value : 0.979668 ## Neg Pred Value : 0.186000 ## Prevalence : 0.979621 ## Detection Rate : 0.979392 ## Detection Prevalence : 0.999719 ## Balanced Accuracy : 0.501167 ## ## &#39;Positive&#39; Class : 0 ## Les valeurs “sensitivity”, “specificity”, etc. sont toutes récupérables à partir de la matrice. Ça fonctionne sur notre test aussi? # pred &lt;- predict(xgb, newdata = as.matrix(X_test)) # caret::confusionMatrix(as.factor(as.numeric(pred &gt; .1)),as.factor(y_test)) Il ne reste qu’à jouer avec le seuil (ici \\(.1\\)). Est-ce qu’on aurait obtenu ces résultats en considérant une séparation entraînement/test respéctant la chronologie? References "],
["deploiement-de-modeles.html", "Chapitre 7 Déploiement de modèles 7.1 OpenCPU 7.2 Plumber 7.3 Détails sur les requêtes HTTP 7.4 Déploiement avec Docker 7.5 Intégration continue et webhook", " Chapitre 7 Déploiement de modèles L’ultime étape pour donner vie à un modèle est de le mettre en production. Les exemples de déploiement de modèles sont omniprésents. Il peut s’agir de cas plus évidents comme l’obtention d’un score de crédit afin de déterminer l’éligibilité pour un prêt ou encore une soumission pour un produit d’assurance basé sur les caractéristiques de l’appliquant et du bien assuré. De manière plus large, ça comprend également des recommendations d’achat faites par un marchand en ligne, les utilitaires un appareil photo permettant la reconnaissance des visages ou un système de conduite autonome. De par la diversité des cas de consommation des modèles, il n’existe pas de solution unique applicable à toutes les situations. Parmi les considérations, notons: nombre de requêtes qui devra être supporté: l’application est-elle destinée à un groupe d’employé ou à une masse de consommateurs localisés partout sur le globe?) besoins en latence: cherche-t-on simplement à assurer une expérience client dynamique ou doit-on exécuter un ordre d’achat dans un contexte d’arbitrage boursier à haute fréquence? les contraintes de la plateforme qui supoprtera les prédictions: par exemple un système de contrôle intelligent qui a des resources processeurs et mémoires limités besoins de stabilité dy système: est-ce qu’une défaillance ou une latence non-déterministe due aux collecteur de déchet pourrait entraîner une collision aérienne? En présence de contraintes importantes sur les précédents facteurs, il peut être préférable d’avoir recours à des langages compilés (C/C++) ou encore plus près du matériel (CUDA) qu’un langage dynamique tel que R. Il n’en reste pas moins que pour un large éventail de contextes, une capacité de développement rapide combinée à des options de déploiement simples font de R une option intéressante pour le déploiement de modèle. À très haut niveau, ce qu’on chercher à accomplir est de permettre à un usager extérieur à notre localisation, sans accès à R, d’accéder aux prédictions de notre modèle: Nous verrons ici deux outils permettant de servir les résultats d’un modèle: OpenCPU Plumber Chacun de ces outils permet de servir un modèle via un REST API, ce qui signifie que les échanges sont réalisés au travers du protocole http. Comme il s’agit d’un protocole omniprésent, il existe de nombreux outils disponibles pour en faciliter l’intégration dans différentes applications. Ça signifie que ces applications peuvent exploiter les capacités de modélisation de R sans qu’elles aient à intégrer R d’aucune façon. L’utilisation d’un modèle via un REST API devient donc agnostic du ou des langages utilisés à l’intérieur de cet API. 7.1 OpenCPU L’unité de travail de OpenCPU repose sur la librairie. Pour pouvoir servir un modèle, on doit donc le structurer de manière à ce qu’il puisse être encapsulé à l’intérieur d’une librairie. Si l’approche peut paraître contraignante, elle incite à l’adoption d’une discipline dans la structure du code et de ses dépendances qui peut être salutaire à sa maintenance. 7.1.1 Débuter avec OpenCPU Le déploiement d’un modèle se fait normalement à partir d’un serveur. Pour se familiariser avec l’outil ainsi que faciliter le développement, il est possible de rouler le serveur OpenCPU localement. Installer la librairie: install.packages(\"opencpu\") La charger: library(opencpu) Démarrer le serveur: opencpu::ocpu_start_server() Une fois le serveur démarré, il est désormais possible d’interagir avec celui-ci via le port local: http://localhost:5656/ocpu/test/ (localhost est ici équivalent à 127.0.0.1). Une requête http peut alors être faite à ce serveur. La programme curl est communément utilisé pour ces requêtes et peut être utilisé à partir du Terminal dans RStudio (ou à partir de tout autre terminal) ou même à l’intérieur de R grâce à des librairies comme curl our httr: $ curl http://127.0.0.1:5656/ocpu/library/stats/R/rnorm/json -d n=2 [-0.4539, 0.1959] Que s’est-il passé? Le serveur a reçu une requête pour la fonction rnorm situé dans stats/R/ avec n=2 comme paramètre. La segment /json a servi à demander à ce que le résultat soit retourné dans le format json. De fait, l’appel à la fonction a retourné un json contenant deux observations simulées d’une Normale(0,1). Il a été possible de faire la précédente requête puisque par défaut, la librairie stats est accessible par OpenCPU, à l’instar de l’ensemble des librairies accessibles au niveau système. Pour servir un modèle prédictif, une librairie permettant de retourner les prédictions de ce modèle devra donc être développée et installée. 7.1.2 Bâtir un squelette de librairie Avant d’illustrer le déploiement d’un modèle, nous allons présenter comment utiliser OpenCPU à partir d’un exemple le plus minimal possible. La première étape est d’avoir le code structuré en une librairie. Une librairie peut être initialisée à l’aide de la commande: usethis::create_package(&quot;~/&lt;nom_de_la_librairie&gt;/&quot;) usethis::create_package(&quot;~/model.ocpu/&quot;) Alternativement, Rstudio offre la fonctionnalité à partir du menu: File -&gt; New Project -&gt; New Directory -&gt; R Package. Une librarie en R n’est qu’un projet qui contient un fichier DESCRIPTION, un fichier NAMESPACE ainsi qu’un dossier R/ à l’intérieur duquel se trouvent les codes R. Les fonctions d’initialisation ci-haut ne sont que des aides facilitant la création de cette structure. Une fois la création du squelette complétée, une fonction de test peut être créée. Par exemple, avec le code suivant dans le fichier ./R/salut.R: #&#39; Salut #&#39; Une fonction qui salue. #&#39; @export salut &lt;- function() { print(&quot;Salut tout le monde!&quot;) } La librairie model.ocpu peut maintenant être bâtie. La documentation sera d’abord générée: devtools::document() L’installation peut ensuite s’exécuter à l’aide du raccourci: ctrl-shift-B. En démarrant une nouvelle session du serveur OpenCPU, il sera désormais possible d’intéragir avec la librairie model.ocpu nouvellement installée. curl http://localhost:5656/ocpu/library/model.ocpu/R/salut/json -d &quot;&quot; 7.1.3 Intégrer un modèle prédictif dans une librairie Une librairie permettant de retourner des prédictions devra supporter les fonctionnalité suivantes: - Lire les informations relativement aux observations pour lesquelles une prédiction doit être retournée - Appliquer les possibles transformations utilisées dans la préparation des données sur lesquelle le modèle a été construit. - Effectuer la prédiction sur ces donnée à partir du modèle sélectionné pour la déploiement. Comment rendre accessible le modèle entraîné à l’intérieur de la librairie? Un modèle peut être représenté comme la combinaison entre des paramètres et un algorithme décrivant comment de transformer les informations d’une observations en une prédiction. Lorsqu’un modèle est entraîné, l’objet résultant contient ces informations, de sorte que la fonction predict appliquée sur ce modèle permet d’obtenir les prédictions désirée. L’approche la plus naturelle sera donc de sauvegarder le modèle désiré à l’intérieur de la librairie et de le rendre accessible aux fonctions de la librairie d’inférence. La méthode recommandée consiste en la création d’un script générant un fichier .Rda qui contient les modèles et autres objets R nécessaires à l’inférence. Ce script sera localisé dans le dossier data-raw. Par exemple, ce script sera le suivant pour rendre disponible le modèle développé à la section précédente: source(&quot;../../../src/collecte/load-merging-data.R&quot;) source(&quot;../../../src/init.R&quot;) init_objects &lt;- init(path_data = &quot;../../../data/&quot;, path_objects = &quot;../../../data/models/&quot;) usethis::use_data(init_objects, internal = T, overwrite = T) Une fonction d’inférence peut maintenant être construite. #&#39; @export bixikwargs &lt;- function(start_date, start_station_code, is_member) { # arranger en un data.table dt_pred &lt;- data.table(start_date, start_station_code, is_member) dt_pred &lt;- merge_data(dt_pred, init_objects$merging_data$data_stations) data_pred &lt;- preprocessing_main(copy(dt_pred), train_mode = FALSE, list_objects = init_objects) data_pred_regression &lt;- data_pred$data_regression data_pred_classif &lt;- data_pred$data_classif duree = predict(init_objects$model_glm, as.matrix(data_pred_regression), s = &quot;lambda.min&quot;) meme_station = predict(init_objects$model_xgb, as.matrix(data_pred_classif)) &gt; 0.5 return(list(duree = duree, meme_station = meme_station)) } curl http://localhost:5656/ocpu/library/model.ocpu/R/bixikwargs/json -d &quot;start_date=&#39;2017-04-15 00:48&#39;&amp;start_station_code=6079&amp;is_member=1&quot; Ajouter des fonctions et dépendances 7.2 Plumber L’approche prise par Plumber repose sur l’ajout d’annotations au code. La technologie sous-jacente est similaire à OpenCPU, l’idée étant de convertir du code R en des services accessibles via le protocole HTTP. 7.2.1 Débuter avec Plumber Installer la librairie: install.packages(\"plumber\") La charger: library(plumber) Une exemple minimaliste est founi dans le dossier src/deploiement/model.plumber/plumber_ini.R. Ce code peut être servi comme un service plumber de la manière suivante: pr &lt;- plumber::plumb(&quot;src/deploiement/plumber/assets/plumber_ini.R&quot;) pr$run(port=8985) Il est alors possible d’accéder au service: curl http://127.0.0.1:8985/message?msg=Salut! 7.2.2 Inférence de modèle prédictif pr &lt;- plumber::plumb(&quot;src/deploiement/plumber/assets/pred_bixi.R&quot;) pr$run(port = 8985) 7.3 Détails sur les requêtes HTTP curl -d msg=Salut! -G http://127.0.0.1:8985/message Passer les paramètres individuellement: curl -X GET --data &#39;{ &quot;start_date&quot;: &quot;2017-04-15 00:48&quot;, &quot;start_station_code&quot;: 6079, &quot;is_member&quot;: 1 }&#39; http://localhost:8985/bixikwargs curl -X GET --data @src/deploiement/data_test_elements.json http://localhost:8985/bixikwargs Passer un fichier json en argument. curl -X GET --data &#39;{&quot;data&quot;:[ { &quot;start_date&quot;: &quot;2017-04-15 00:48&quot;, &quot;start_station_code&quot;: 6079, &quot;is_member&quot;: 1 } ]}&#39; &quot;http://localhost:8985/bixidata&quot; curl -X GET --data @src/deploiement/data_test.json &quot;http://localhost:8985/bixidata&quot; La commande -X est utilisée pour forcer le remplacement des instructions par défaut. La méthode --data ou -d est en effet associés par défaut à une commande POST. Puisqu’on ne cherche ici qu’à retourner une réponse à une instruction, le verbe GET est approprié. 7.4 Déploiement avec Docker Docker est une technologie permettant la containerization d’applications, faciliant leur portabilité sur différentes plateformes. Une image Docker est formée de différentes couches contenant les libraries et configurations requises pour l’exécution de l’application. On peut construire une image à partir d’une racine très générique comme d’une installation générique de Ubuntu, ou encore d’une image intégrant déjà plusieurs fonctionnalités adaptées à notre domaine. La recette pour la construction de ces images est spécifiée dans un fichier texte nommé Dockerfile. docker build ./src/deploiement/plumber/ -t jeremiedb/dot-layer:plumber Lancer une image. docker run --rm -p 8080:8080 jeremiedb/dot-layer:plumber 7.5 Intégration continue et webhook "]
]
