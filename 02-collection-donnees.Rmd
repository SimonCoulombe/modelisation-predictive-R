
# Collecte de données

La collecte de données est le point de départ de la chaîne. Les données étant quelque peu la matière première du produit que nous nous apprêtons à bâtir, plusieurs considérations influenceront leur collecte. 

Pour illustrer le tout, prétendons pour l'instant que le produit qui nous intéresse est en fait une table. La construction d'une table nécessite évidemment de considérer la quantité, la qualité et le coût de la matièere première. Toutefois, plusieurs considérations additionnelles viennent s'ajouter quand on s'intéresse à la provenance, au processus de récolte ou de transformation et à l'entreposage des matériaux.

Les mêmes considérations peuvent s'appliquer dans notre contexte. La section suivante aura donc pour objectif d'aborder ces considérations, tout en soumettant les données de notre atelier à celles-ci.

Idées :
- Toujours relier la théorie au dataset bixi (pourquoi on l'a pris, etc.)
- Balise quelconque pour les overlaps avec la partie pre-processing de Sté

## Liste des données potentielles pour notre atelier

La première étape d'un déploiement de modèle prédictif est de dresser la liste des sources de données potentielles.

Les plus expérimentés auront déjà en tête toutes les considérations couvertes dans ce chapitre au moment de dresser cette liste, ce qui aura nécessairement comme effet de la réduire. Dans notre cas, permettons-nous de lister naïvement un grand nombre de sources. Chaque considération à venir nous permettra de distiller notre sélection.

- Relier la théorie à des datasets supplémentaires (shapefiles, etc.)
- Pourquoi on va utiliser les shapefiles comme exemple de merge (statique, etc.)
- Météo
- Données géographiques (quartiers, pentes, cours d'eau, etc.)
- Données démographiques (densité, revenu, âge, etc.)
- Données individuelles (adresse, résident/visiteur, etc.)
- Historique de trajets par individu
- ...

Le processus de sélection de données de cet ouvrage sera linéaire. Bien entendu, dans un projet réel, il est possible que certains aspects nous échappent au début, et que nous devions réajuster le tir au moment de l'exploration, du prétraitement ou de la modélisation des données, ou même du déploiement du modèle.

[STOP] demander aux participants leurs idées de dataset intéressant

## Considérations {#considerations}

### Quantité

Quantité : tout ce qui touche au volume, au format et à l'entreposage des données

#### Volume
dplyr
data.table

#### Format
JSON, XML, SQL, oracle, etc.
texte, son, image, combinaison

#### Entreposage
feather


### Qualité
Évidemment le pouvoir prédictif, mais aussi la récence, la fréquence, la capacité de merger et le biais

Garbage In Garbage Out
- Possibilité que les sections futures viennent invalider une source de données

#### pouvoir prédictif
#### récence
#### fréquence
#### Merge
facilité/pertinence de la clé, etc. Exemple : historique des déplacements du client ... pas dispo. Données ouvertes Montréal : ok peut-être...
#### Biais
socio-démographique, etc.

### Accessibilité
Dans notre parallèle avec la table, le coût était la troisième considération principale. Dans le cas des données, nous allons plutôt parler d'accessibilité, dont le coût est une des composantes.
#### coût (gratuit?)
#### Lecture de fichiers
readr
readxl
xlsx
#### Connexion à une base de données
sqldf
dbplyr
rio
DBI
odbc
RMySQL, RPostgresSQL, RSQLite
foreign
haven
https://db.rstudio.com/
#### Web Scraping (légalité/api/service/timeouts)
XML
xml2
curl
httr
rvest
downloader
jsonlite
googlesheets
#### Stabilité
- Évolution anticipée du format des données (pas de contrôle sur la source de données? → copier les données freezées. Le modèle a encore de la valeur)
#### Sources
- https://www.kaggle.com/datasets
- https://toolbox.google.com/datasetsearch
- https://www.reddit.com/r/datasets
- https://archive.ics.uci.edu/ml/index.php?fbclid=IwAR09F5grBOTCr1SS4v8gONEYeqk0DqqWpPdt1blmYF9ucZkhsQeM5T0E7ew
- http://donnees.ville.montreal.qc.ca/

## Références
